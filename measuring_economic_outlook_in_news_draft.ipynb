{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-81lsa9nMaEb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# Replication of \"*Measuring economic outlook in the news timely and efficiently*\"\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2511.04299-b31b1b.svg)](https://arxiv.org/abs/2511.04299)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/measuring_economic_outlook_in_news)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Computational%20Economics-00529B)](https://github.com/chirindaopensource/measuring_economic_outlook_in_news)\n",
        "[![Data Source](https://img.shields.io/badge/Data%20Source-Swissdox%40LiRI-003299)](https://www.liri.uzh.ch/en/services/swissdox.html)\n",
        "[![Core Method](https://img.shields.io/badge/Method-LLM--Based%20Sentiment%20Analysis-orange)](https://github.com/chirindaopensource/measuring_economic_outlook_in_news)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Time%20Series%20Forecasting-red)](https://github.com/chirindaopensource/measuring_economic_outlook_in_news)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Anthropic](https://img.shields.io/badge/Anthropic-Claude%203.5%20Sonnet-D97A53?logo=anthropic&logoColor=white)](https://www.anthropic.com/news/claude-3-5-sonnet)\n",
        "[![SentenceTransformers](https://img.shields.io/badge/SentenceTransformers-jina--embeddings--v2-2E4053)](https://huggingface.co/jinaai/jina-embeddings-v2-base-de)\n",
        "[![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=flat&logo=TensorFlow&logoColor=white)](https://www.tensorflow.org/)\n",
        "[![Scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/Statsmodels-150458-blue)](https://www.statsmodels.org/stable/index.html)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/measuring_economic_outlook_in_news`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Measuring economic outlook in the news timely and efficiently\"** by:\n",
        "\n",
        "*   Elliot Beck\n",
        "*   Franziska Eckert\n",
        "*   Linus Kühne\n",
        "*   Helge Liebert\n",
        "*   Rina Rosenblatt-Wisch\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and cleansing to large-scale embedding, model training, indicator construction, and the final econometric evaluation.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_complete_neos_study`](#key-callable-run_complete_neos_study)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Beck et al. (2025). The core of this repository is the iPython Notebook `measuring_economic_outlook_in_news_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed as a robust and scalable system for constructing a high-frequency economic sentiment indicator (NEOS) from a large corpus of news articles.\n",
        "\n",
        "The paper's central contribution is a novel, resource-efficient methodology for sentiment analysis that is suitable for institutions with data privacy constraints. This codebase operationalizes the paper's experimental design, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Execute a multi-stage pipeline to cleanse, prepare, and generate high-dimensional embeddings for a large news corpus.\n",
        "-   Train a weakly supervised neural network to filter for economics-relevant articles.\n",
        "-   Programmatically generate a synthetic, high-quality labeled dataset for sentiment analysis using an LLM (Claude 3.5 Sonnet), avoiding the need for manual labeling and preserving data privacy.\n",
        "-   Train a regularized logistic regression model to score the sentiment of millions of articles efficiently.\n",
        "-   Construct the final monthly NEOS indicator, its early-release variants, and a traditional lexicon-based baseline.\n",
        "-   Perform a comprehensive econometric evaluation using a pseudo-out-of-sample (POOS) forecasting exercise to test the indicator's predictive power for GDP growth.\n",
        "-   Run Diebold-Mariano tests with HAC-robust errors to assess the statistical significance of the findings.\n",
        "-   Conduct a full suite of robustness checks to test the sensitivity of the results to key methodological choices.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in principles from natural language processing, machine learning, and time-series econometrics.\n",
        "\n",
        "**1. LLM-based Synthetic Data Generation:**\n",
        "The core innovation is the use of a powerful LLM to generate a small, perfectly labeled training set for sentiment classification. This avoids the costly and time-consuming process of manual annotation and, crucially, allows a sentiment model to be trained without exposing any proprietary source data to external APIs.\n",
        "\n",
        "**2. High-Dimensional Classification with Regularization:**\n",
        "The sentiment classifier is a logistic regression model trained on high-dimensional embeddings where the number of features ($p=1024$) exceeds the number of samples ($n=256$). To prevent overfitting, L2 (Ridge) regularization is essential. The model minimizes the regularized negative log-likelihood:\n",
        "$$\n",
        "\\mathcal{L}(\\boldsymbol{\\beta}) = -\\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta})) + (1-y_i) \\log(1-\\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta})) \\right] + \\lambda ||\\boldsymbol{\\beta}||_2^2\n",
        "$$\n",
        "\n",
        "**3. Pseudo-Out-of-Sample (POOS) Forecast Evaluation:**\n",
        "To rigorously assess the indicator's predictive value, the project implements a POOS forecasting exercise. This involves simulating a real-time forecasting process by iterating through time, using an expanding window of historical data to estimate the forecasting models at each step. This method strictly avoids look-ahead bias. The core forecasting model is:\n",
        "$$\n",
        "y_{t+h} = \\alpha + \\beta y_{t-1} + \\gamma x_t^{(m)} + \\varepsilon_t \\quad \\quad (1)\n",
        "$$\n",
        "This is compared against a benchmark AR(1) model where $\\gamma=0$.\n",
        "\n",
        "**4. Diebold-Mariano Test with HAC Errors:**\n",
        "To test if the improvement in forecast accuracy (measured by RMSE) is statistically significant, the Diebold-Mariano (DM) test is used. The implementation is modified to use a Heteroskedasticity and Autocorrelation Consistent (HAC) variance estimator (Newey-West), which is critical for handling the serial correlation present in multi-step-ahead forecast errors ($h>0$).\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`measuring_economic_outlook_in_news_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 23 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file.\n",
        "-   **Scalable Data Processing:** Includes efficient, batch-based processing for large-scale embedding and inference, with out-of-core storage using HDF5.\n",
        "-   **Resumable Pipeline:** The main orchestrator implements checkpointing, allowing the pipeline to be stopped and resumed without re-running expensive completed steps.\n",
        "-   **Robust Model Training:** Implements best practices for both neural network and classical model training, including temporal validation splits, early stopping, and cross-validated hyperparameter tuning.\n",
        "-   **Rigorous Econometric Analysis:** Implements the full POOS forecasting loop and DM-HAC significance tests with high fidelity.\n",
        "-   **Complete Replication and Robustness:** A single top-level function call can execute the entire study, including a comprehensive suite of sensitivity analyses.\n",
        "-   **Full Provenance:** The pipeline generates a detailed log file and a final `reproducibility_manifest.json` that captures all configurations, library versions, and artifact paths for a given run.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-5):** Ingests and validates all raw inputs, cleanses the news corpus according to the paper's scope, and adds temporal features.\n",
        "2.  **Embedding (Task 6):** Generates 1024-dimensional `jina-embeddings-v2` for the entire corpus.\n",
        "3.  **Relevance Filtering (Tasks 7-9):** Trains a weakly supervised MLP to identify economics-related articles and filters the corpus.\n",
        "4.  **Sentiment Model Training (Tasks 10-12):** Generates a synthetic training set with Claude 3.5 Sonnet, embeds it, and trains an L2-regularized logistic regression classifier.\n",
        "5.  **Indicator Construction (Tasks 13-14):** Scores all relevant articles for sentiment and aggregates the scores into monthly baseline and early-release indicators.\n",
        "6.  **Econometric Evaluation (Tasks 15-20):** Aligns all indicators to a quarterly frequency, runs the full POOS forecasting exercise, computes RMSE ratios, performs DM-HAC tests, and generates correlation tables.\n",
        "7.  **Visualization & Robustness (Tasks 21-23):** Generates all charts from the paper and runs a full suite of sensitivity analyses on key methodological choices.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `measuring_economic_outlook_in_news_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 23 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_complete_neos_study`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_complete_neos_study`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end, including the baseline analysis and all robustness checks. A single call to this function reproduces the entire computational portion of the project.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   An Anthropic API key.\n",
        "-   Sufficient disk space for embeddings (~8GB per million articles).\n",
        "-   A GPU is highly recommended for the embedding and relevance model training steps.\n",
        "-   Core dependencies: `pandas`, `numpy`, `pyyaml`, `pyarrow`, `tensorflow`, `scikit-learn`, `statsmodels`, `sentence-transformers`, `h5py`, `joblib`, `anthropic`, `umap-learn`, `matplotlib`, `seaborn`, `tqdm`, `faker`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/measuring_economic_outlook_in_news.git\n",
        "    cd measuring_economic_outlook_in_news\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "4.  **Set your Anthropic API Key:**\n",
        "    ```sh\n",
        "    export ANTHROPIC_API_KEY='your-key-here'\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires several input DataFrames with specific schemas, which are rigorously validated. A synthetic data generator is included in the notebook for a self-contained demonstration.\n",
        "1.  **`raw_news_data_df`**: The large-scale news article corpus.\n",
        "2.  **`raw_macro_data_df`**: Quarterly macroeconomic data (GDP, etc.).\n",
        "3.  **`monthly_indicator_data_df`**: Monthly comparator indicators (PMI, KOF).\n",
        "4.  **`release_calendar_df`**: Metadata on indicator release dates.\n",
        "5.  **`evaluation_windows_df`**: Metadata on valid evaluation periods for certain indicators.\n",
        "6.  A translated German sentiment **lexicon file** (CSV).\n",
        "\n",
        "All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `measuring_economic_outlook_in_news_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `run_complete_neos_study` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load configuration from the YAML file.\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Define paths.\n",
        "    ROOT_OUTPUT_DIRECTORY = './neos_study_output'\n",
        "    LEXICON_PATH = './dummy_lexicon_de.csv'\n",
        "    \n",
        "    # 3. Generate a full set of synthetic data files for the demonstration.\n",
        "    # (The data generation functions are defined earlier in the notebook)\n",
        "    raw_news_data_df = create_synthetic_news_data(1000, config)\n",
        "    # ... create all other synthetic DataFrames ...\n",
        "    \n",
        "    # 4. Execute the entire replication study.\n",
        "    final_results = run_complete_neos_study(\n",
        "        raw_news_data_df=raw_news_data_df,\n",
        "        raw_macro_data_df=raw_macro_data_df,\n",
        "        monthly_indicator_data_df=monthly_indicator_data_df,\n",
        "        release_calendar_df=release_calendar_df,\n",
        "        evaluation_windows_df=evaluation_windows_df,\n",
        "        fused_master_input_specification=config,\n",
        "        root_output_directory=ROOT_OUTPUT_DIRECTORY,\n",
        "        lexicon_path=LEXICON_PATH\n",
        "    )\n",
        "    \n",
        "    # 5. Inspect final results.\n",
        "    print(\"--- Baseline Run Status ---\")\n",
        "    print(final_results['baseline_run_results']['status'])\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline generates a structured output directory:\n",
        "-   **`output/baseline_run/`**: Contains all artifacts from the main pipeline run.\n",
        "    -   `data/`: Intermediate data files (embeddings, scores, etc.).\n",
        "    -   `models/`: Trained model files.\n",
        "    -   `results/`: Final result tables (CSV) and charts (PNG).\n",
        "    -   `pipeline_run.log`: A detailed log file for the run.\n",
        "    -   `reproducibility_manifest.json`: A complete record of the run.\n",
        "-   **`output/robustness_checks/`**: Contains a subdirectory for each sensitivity analysis, with the same internal structure as `baseline_run`.\n",
        "-   **`output/robustness_checks/robustness_summary_results.csv`**: A master table comparing key results across all robustness checks.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "measuring_economic_outlook_in_news/\n",
        "│\n",
        "├── measuring_economic_outlook_in_news_draft.ipynb\n",
        "├── config.yaml\n",
        "├── requirements.txt\n",
        "│\n",
        "├── neos_study_output/\n",
        "│   ├── baseline_run/\n",
        "│   │   ├── data/\n",
        "│   │   ├── models/\n",
        "│   │   └── results/\n",
        "│   └── robustness_checks/\n",
        "│       ├── sensitivity_tau_0.4/\n",
        "│       └── ...\n",
        "│\n",
        "├── LICENSE\n",
        "└── README.md\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify all study parameters, including model names, API settings, filtering thresholds, and file paths, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Embedding Models:** The modular design allows for easy substitution of the `model_name` in the config to test other embedding models.\n",
        "-   **Different Classifiers:** The sentiment model could be replaced with other classifiers (e.g., SVM, Gradient Boosting) to test for performance differences.\n",
        "-   **Advanced Econometric Models:** The forecasting exercise could be extended to include more complex models, such as VARs or models with dynamic variable selection.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{beck2025measuring,\n",
        "  title={Measuring economic outlook in the news timely and efficiently},\n",
        "  author={Beck, Elliot and Eckert, Franziska and K{\\\"u}hne, Linus and Liebert, Helge and Rosenblatt-Wisch, Rina},\n",
        "  journal={arXiv preprint arXiv:2511.04299},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Production-Grade Replication of \"Measuring economic outlook in the news timely and efficiently\".\n",
        "GitHub repository: https://github.com/chirindaopensource/measuring_economic_outlook_in_news\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Elliot Beck, Franziska Eckert, Linus Kühne, Helge Liebert, and Rina Rosenblatt-Wisch** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, TensorFlow, Scikit-learn, Statsmodels, Sentence-Transformers, and Anthropic**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `measuring_economic_outlook_in_news_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "NFABW1cB0RO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Measuring economic outlook in the news timely and efficiently*\"\n",
        "\n",
        "Authors: Elliot Beck, Franziska Eckert, Linus Kühne, Helge Liebert, Rina Rosenblatt-Wisch\n",
        "\n",
        "E-Journal Submission Date: 6 November 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2511.04299\n",
        "\n",
        "Abstract:\n",
        "\n",
        "We introduce a novel indicator that combines machine learning and large language models with traditional statistical methods to track sentiment regarding the economic outlook in Swiss news. The indicator is interpretable and timely, and it significantly improves the accuracy of GDP growth forecasts. Our approach is resource-efficient, modular, and offers a way of benefitting from state-of-the-art large language models even if data are proprietary and cannot be stored or analyzed on external infrastructure - a restriction faced by many central banks and public institutions."
      ],
      "metadata": {
        "id": "EFOkxpdHMg7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Summary: \"Measuring economic outlook in the news timely and efficiently\"**\n",
        "\n",
        "#### **The Core Objective and Contribution**\n",
        "\n",
        "The authors aim to construct a high-frequency, real-time indicator of the Swiss economic outlook, which they name the **News-based Economic Outlook for Switzerland (NEOS)**. The primary contribution is not the invention of a new algorithm, but rather the development of a novel, resource-efficient methodology that makes state-of-the-art Natural Language Processing (NLP) techniques viable for institutions like central banks. These institutions often face two critical constraints:\n",
        "\n",
        "1.  **Data Privacy:** Proprietary or sensitive data (in this case, licensed news articles) cannot be sent to external, cloud-based APIs for analysis.\n",
        "2.  **Computational Resources:** Analyzing millions of documents with massive Large Language Models (LLMs) can be prohibitively expensive and slow.\n",
        "\n",
        "The paper’s proposed pipeline elegantly circumvents these issues while delivering a high-performance economic indicator.\n",
        "\n",
        "#### **The Methodological Pipeline – A Hybrid Approach**\n",
        "\n",
        "The construction of NEOS is a multi-stage process that intelligently combines different machine learning paradigms. I will break it down as presented in the paper.\n",
        "\n",
        "**(1) Data Foundation:**\n",
        "The process begins with a massive corpus of 26.2 million Swiss newspaper articles (in German and French) from the Swissdox@LiRI database, spanning from 1999 to 2025. This provides a comprehensive and representative view of media discourse.\n",
        "\n",
        "**(2) Semantic Representation: From Text to Vectors**\n",
        "The authors transform the unstructured text into a structured, numerical format using a modern embedding model, `jina-embeddings-v3`. Each article becomes a 1024-dimensional vector. This is a crucial step. Unlike older bag-of-words methods, these embeddings capture the semantic meaning and context of the text, allowing for a more nuanced understanding. The choice of a multilingual model is also essential for handling both German and French sources without separate pipelines.\n",
        "\n",
        "**(3) Relevance Filtering: Isolating Economic News**\n",
        "Analyzing all 26 million articles for sentiment would be inefficient. The authors first train a neural network to perform a binary classification: is an article about economics or not? In a clever application of weak supervision, they generate a training set by assuming that articles published in dedicated \"Business\" or \"Economics\" sections are, by definition, relevant. This classifier is then used to filter the entire corpus, reducing it to a more manageable 3.1 million relevant articles.\n",
        "\n",
        "**(4) The Core Innovation: LLM-Generated Synthetic Training Data**\n",
        "This is the most brilliant part of their methodology. Manually labeling thousands of articles for positive or negative sentiment is a time-consuming, expensive, and subjective task. Instead, the authors use a powerful generative LLM (`Claude 3.5 Sonnet`) as a \"synthetic data generator.\" They prompt the LLM to write 256 archetypal articles: 128 with a stereotypically positive economic outlook and 128 with a negative one, covering various topics (financial markets, labor, etc.).\n",
        "\n",
        "This creates a small, perfectly polarized, and high-quality training dataset without any human labeling effort. From a machine learning perspective, this is a form of \"tens-of-shot\" learning, where the power of a massive LLM is distilled into a small, targeted dataset.\n",
        "\n",
        "**(5) Sentiment Scoring: An Efficient and Interpretable Classifier**\n",
        "The 256 synthetic articles are converted into their 1024-dimensional vector embeddings. A UMAP projection (Chart 2) visually confirms that these embeddings cluster neatly into \"positive\" and \"negative\" groups, validating that the vector space effectively captures the sentiment dimension.\n",
        "\n",
        "With this validation, the authors train a simple **logistic regression model** on these 256 embedded vectors. The use of regularization is technically sound, as the number of features (1024) far exceeds the number of observations (256). This model learns the linear boundary in the high-dimensional space that separates positive from negative sentiment.\n",
        "\n",
        "This trained logistic regression classifier is then applied to the embeddings of the 3.1 million relevant newspaper articles. The model is computationally trivial to run at this scale and produces a sentiment probability score (from 0 for negative to 1 for positive) for each article.\n",
        "\n",
        "**(6) Aggregation: From Articles to a Time-Series Indicator**\n",
        "Finally, the individual article scores are aggregated—in this case, by taking a simple monthly average—to create the final NEOS time series. The authors also compute higher-frequency variants (e.g., using only the first 7, 14, or 21 days of a month) to demonstrate the indicator's timeliness.\n",
        "\n",
        "#### **Econometric Validation and Performance**\n",
        "\n",
        "The paper subjects the NEOS indicator to a rigorous econometric evaluation, which is the gold standard for validating such tools.\n",
        "\n",
        "**(1) Forecasting Setup:**\n",
        "The authors conduct a pseudo-out-of-sample forecasting exercise for Swiss year-on-year GDP growth. They use a standard autoregressive (AR(1)) model as a benchmark and test whether adding NEOS provides statistically significant marginal predictive content. The model is:\n",
        "*   *Y<sub>t+h</sub> = α + βY<sub>t-1</sub> + γX<sub>t</sub><sup>(m)</sup> + ε<sub>t</sub>*\n",
        "\n",
        "The key is to test if *γ* is statistically significant and if its inclusion reduces the forecast error.\n",
        "\n",
        "**(2) Performance Results (Table 1):**\n",
        "*   **Improved Accuracy:** NEOS consistently improves GDP growth forecasts across all horizons (nowcast *h=0*, and forecasts *h=1, h=2*). The Root Mean Squared Error (RMSE) ratios are substantially below 1, indicating a roughly 15-20% reduction in forecast error.\n",
        "*   **Statistical Significance:** The improvements are statistically significant according to the Diebold-Mariano test, confirming that the gains are not due to chance.\n",
        "*   **Value of Timeliness:** Crucially, the \"early-release\" versions of NEOS (e.g., NEOS calculated on the first 21 days of the third month of a quarter) are often the *best* performers. This empirically proves the economic value of receiving a signal weeks before traditional data is released.\n",
        "*   **Superiority to Baselines:** NEOS generally outperforms a traditional lexicon-based sentiment indicator and most standard survey-based indicators (e.g., SECO Consumer Sentiment, KOF Business Situation). While the Manufacturing PMI is competitive, NEOS has the advantage of higher frequency and is not tied to a fixed survey schedule.\n",
        "\n",
        "**(3) Crisis Performance (Charts 4 & 5):**\n",
        "The analysis rightly highlights that the value of such indicators is magnified during times of high uncertainty. The daily plot in Chart 4 shows how NEOS can capture sharp shifts in sentiment in response to specific news events (e.g., tariff announcements). Furthermore, a cumulative squared error analysis (Chart 5) demonstrates that NEOS's outperformance is particularly pronounced during major economic shocks like the Global Financial Crisis and the COVID-19 pandemic.\n",
        "\n",
        "#### **Conclusion and Overall Assessment**\n",
        "\n",
        "From my perspective as an interdisciplinary professor, this paper is an excellent example of applied research and sound engineering.\n",
        "\n",
        "*   **From a Computer Science perspective:** The \"LLM-as-generator\" to train a simple, efficient classifier is an elegant and highly practical pattern. It leverages the semantic intelligence of foundation models without becoming dependent on them for large-scale inference, solving the privacy and cost problems. The modular design is also robust, allowing for future upgrades to the embedding model or LLM.\n",
        "*   **From an Econometrics perspective:** The validation is rigorous and follows best practices in the field. The focus on pseudo-out-of-sample performance, the use of a standard benchmark model, and formal statistical tests of forecast improvement lend strong credibility to the results.\n",
        "*   **From a Finance perspective:** The demonstration of timeliness is key. For financial market participants and policymakers, an accurate signal even a few weeks early is immensely valuable for decision-making. The ability to \"drill down\" from the aggregate index to the specific articles driving a change provides a level of interpretability that black-box models lack.\n",
        "\n",
        "In summary, the authors have successfully developed and validated a novel economic indicator that is not only accurate and timely but also designed to operate within the practical constraints of real-world policy and financial institutions. It is a compelling case study in the thoughtful application of modern AI to solve a long-standing problem in economics."
      ],
      "metadata": {
        "id": "7i1zIr6rM9PV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "2Xb9Oi1PNcXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Measuring Economic Outlook in the News Timely and Efficiently\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Measuring economic outlook in the news\n",
        "#  timely and efficiently\" by Beck et al. (2025). It delivers a robust,\n",
        "#  end-to-end pipeline for creating a high-frequency economic sentiment indicator\n",
        "#  from unstructured news text, designed for environments with strict data\n",
        "#  privacy and computational constraints.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Large-scale text data cleansing and preparation with full provenance tracking.\n",
        "#  • High-dimensional feature engineering via multilingual Transformer-based embeddings.\n",
        "#  • Weakly supervised neural network for domain-specific text classification (relevance filtering).\n",
        "#  • LLM-based synthetic data generation for privacy-preserving sentiment classifier training.\n",
        "#  • L2-regularized logistic regression for efficient and interpretable sentiment scoring.\n",
        "#  • Rigorous econometric validation using pseudo-out-of-sample (POOS) forecasting.\n",
        "#  • Statistical significance testing with Diebold-Mariano tests modified with HAC robust errors.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Modular, function-based architecture with a top-level orchestrator.\n",
        "#  • Robust MLOps practices including checkpointing for resumability and comprehensive logging.\n",
        "#  • Efficient, out-of-core processing for large embedding matrices using HDF5.\n",
        "#  • Systematic hyperparameter tuning via stratified cross-validation.\n",
        "#  • Generation of publication-quality diagnostic charts and result tables.\n",
        "#  • A complete reproducibility manifest capturing configuration, library versions, and artifacts.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Beck, E., Eckert, F., Kühne, L., Liebert, H., & Rosenblatt-Wisch, R. (2025).\n",
        "#  Measuring economic outlook in the news timely and efficiently.\n",
        "#  arXiv preprint arXiv:2511.04299. https://arxiv.org/abs/2511.04299\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Fused Imports for the End-to-End NEOS Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Standard Library ---\n",
        "import copy\n",
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import uuid\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Optional, Set, Tuple\n",
        "\n",
        "# --- Core Scientific Computing ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Machine Learning & NLP ---\n",
        "import h5py\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import umap\n",
        "from anthropic import APIError, Anthropic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Econometrics & Statistics ---\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.tsa.api as tsa\n",
        "from scipy.stats import norm\n",
        "\n",
        "# --- Visualization ---\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Utilities ---\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "2K2q3f1LNgyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "Ujb7qRz9Nh2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Comprehensive Callable Summary**\n",
        "\n",
        "#### **Task 1: `validate_raw_news_corpus`**\n",
        "*   **Inputs:** `raw_news_data_df` (the unprocessed news corpus), `fused_master_input_specification` (the master configuration).\n",
        "*   **Processes:** This function is a pure validation orchestrator. It performs a series of checks without modifying data:\n",
        "    1.  Asserts schema compliance (column names, dtypes, timezone-aware UTC timestamps).\n",
        "    2.  Verifies primary key (`article_id`) uniqueness.\n",
        "    3.  Validates that categorical columns (`publication_type`, `language`) adhere to controlled vocabularies.\n",
        "    4.  Checks for one-to-one mapping consistency between `outlet_id` and `source_outlet`.\n",
        "    5.  Quantifies `section_label` missingness and checks temporal integrity of timestamps.\n",
        "*   **Outputs:** A dictionary containing a `'status'` ('Success' or 'Failure'), a list of `errors` and `warnings`, and a `missingness_report` DataFrame.\n",
        "*   **Transformation:** No data transformation occurs. The function is a read-only gatekeeper.\n",
        "*   **Role in Research Pipeline:** This callable implements the initial data quality assurance step, which is a prerequisite for any rigorous analysis but is not explicitly detailed in the paper's main text. It ensures the raw data from the **Swissdox@LiRI database (Step 1, Page 3)** is structurally sound and consistent before any processing begins.\n",
        "\n",
        "#### **Task 2: `validate_macro_data`**\n",
        "*   **Inputs:** `raw_macro_data_df`, `monthly_indicator_data_df`, `release_calendar_df`, `evaluation_windows_df`.\n",
        "*   **Processes:** This is another pure validation orchestrator for the macroeconomic and metadata tables.\n",
        "    1.  Asserts the temporal frequency and schema of all input DataFrames.\n",
        "    2.  Cross-validates that the quarterly `_m2` indicators in `raw_macro_data_df` are consistent with the values in the authoritative `monthly_indicator_data_df`.\n",
        "    3.  Verifies historical `NaN` patterns in the monthly data against the `evaluation_windows_df` to guard against backfilling.\n",
        "    4.  Checks the integrity of the metadata tables.\n",
        "*   **Outputs:** A dictionary containing a `'status'` and a consolidated list of `issues`.\n",
        "*   **Transformation:** No data transformation occurs.\n",
        "*   **Role in Research Pipeline:** This callable ensures the integrity of the comparator indicators (PMIs, KOF, SECO) and the dependent variable (GDP growth) used in the **Econometric Validation (Section 3, Page 5)**. It rigorously enforces the data availability constraints mentioned in the footnotes of **Table 1 (Page 6)**.\n",
        "\n",
        "#### **Task 3: `validate_master_config`**\n",
        "*   **Inputs:** `fused_master_input_specification`.\n",
        "*   **Processes:** This is a pure validation function that recursively checks the entire configuration dictionary against the hardcoded methodological choices from the paper. It validates everything from date ranges and model names to neural network architecture and econometric test parameters.\n",
        "*   **Outputs:** A dictionary containing a `'status'` and a list of `issues`.\n",
        "*   **Transformation:** No data transformation occurs.\n",
        "*   **Role in Research Pipeline:** This is a meta-level function that serves as a pre-flight check for the entire replication study. It ensures that the code is configured to execute the exact methodology described throughout the paper, from **Data and methods (Section 2)** to **Results (Section 3)**.\n",
        "\n",
        "#### **Task 4: `cleanse_news_corpus`**\n",
        "*   **Inputs:** `raw_news_data_df`, `fused_master_input_specification`.\n",
        "*   **Processes:** This is the first data transformation function.\n",
        "    1.  Filters the corpus based on date range, language, and publication type.\n",
        "    2.  Removes records with null or empty critical fields.\n",
        "    3.  Performs deterministic deduplication based on a content fingerprint.\n",
        "*   **Outputs:** A tuple containing the cleansed `pd.DataFrame` and a detailed `audit_log` dictionary.\n",
        "*   **Transformation:** The input DataFrame is transformed by removing rows that do not meet the study's inclusion criteria. The number of rows is reduced.\n",
        "*   **Role in Research Pipeline:** This callable implements the data curation rules described in **Footnote 1 (Page 3)**, which states, \"...we discard some very specific media outlets... only concentrate on print and online newspaper articles, excluding other sources...\". It prepares the raw corpus for feature engineering.\n",
        "\n",
        "#### **Task 5: `prepare_corpus_for_embedding`**\n",
        "*   **Inputs:** The cleansed DataFrame from Task 4 (`clean_df`) and the original raw DataFrame (`raw_df`).\n",
        "*   **Processes:**\n",
        "    1.  Performs a final assertion of timestamp integrity.\n",
        "    2.  Adds new temporal feature columns (`year_month`, `day_of_month`, and boolean flags for early-release windows).\n",
        "    3.  Generates a comprehensive audit manifest comparing the pre- and post-cleansing corpus.\n",
        "*   **Outputs:** A tuple containing the feature-enriched `pd.DataFrame` and the `audit_manifest` dictionary.\n",
        "*   **Transformation:** The input DataFrame is transformed by adding new columns. The number of rows remains the same.\n",
        "*   **Role in Research Pipeline:** This callable prepares the data for the subsequent aggregation steps. The creation of early-release window flags is a direct prerequisite for constructing the timelier NEOS variants mentioned on **Page 4**: \"...compute timelier variants of NEOS using just the articles released during the first 7, 14, or 21 days of each month...\".\n",
        "\n",
        "#### **Task 6: `generate_document_embeddings`**\n",
        "*   **Inputs:** The prepared DataFrame from Task 5, `fused_master_input_specification`.\n",
        "*   **Processes:** This is the core feature engineering step.\n",
        "    1.  Loads the `jina-embeddings-v3` model.\n",
        "    2.  Iterates through the corpus in batches, tokenizing and computing token statistics.\n",
        "    3.  Runs local inference to transform each article's text into a 1024-dimensional vector.\n",
        "    4.  Saves the embeddings to an HDF5 file and creates a crosswalk table.\n",
        "    5.  Performs quality validation and drift analysis on the generated embeddings.\n",
        "*   **Outputs:** A tuple containing the DataFrame augmented with token stats, paths to the embeddings and crosswalk files, and a diagnostics dictionary.\n",
        "*   **Transformation:** The `full_text` column (unstructured text) is transformed into a large numerical matrix of embeddings, stored on disk. The input DataFrame is transformed by adding token statistic columns.\n",
        "*   **Role in Research Pipeline:** This callable implements **Step (2) \"Get document embeddings\" (Page 2)**. It is the direct implementation of the statement on **Page 3**: \"We use the embedding model `jina-embeddings-v3`... to generate document embeddings, i.e., transforming the articles into a numerical representation (a vector of dimension 1024).\"\n",
        "\n",
        "#### **Task 7: `prepare_relevance_training_data`**\n",
        "*   **Inputs:** The augmented DataFrame, paths to the embeddings and crosswalk, `fused_master_input_specification`.\n",
        "*   **Processes:**\n",
        "    1.  Creates weak binary labels (`y_econ`) from the `section_label` metadata.\n",
        "    2.  Identifies a high-quality subset of data from outlets with reliable metadata coverage.\n",
        "    3.  Performs a strict temporal train/validation split to prevent look-ahead bias.\n",
        "    4.  Retrieves the corresponding embedding vectors from the HDF5 file for the train and validation sets, ensuring perfect alignment.\n",
        "*   **Outputs:** A tuple of four objects: `X_train`, `y_train`, `X_val`, `y_val`.\n",
        "*   **Transformation:** A subset of the corpus metadata is transformed into two pairs of aligned feature matrices and label vectors, ready for model training.\n",
        "*   **Role in Research Pipeline:** This callable implements the data preparation for **Step (3) \"Filter relevant articles\" (Page 2)**. It operationalizes the weak supervision approach described on **Page 3**: \"We use the embeddings of articles published in these sections to train a neural network to classify whether an article is about economics or not.\"\n",
        "\n",
        "#### **Task 8: `train_relevance_classifier`**\n",
        "*   **Inputs:** `X_train`, `y_train`, `X_val`, `y_val`, `fused_master_input_specification`.\n",
        "*   **Processes:**\n",
        "    1.  Builds the MLP neural network architecture as specified in the configuration.\n",
        "    2.  Configures the training process with the Adam optimizer, binary cross-entropy loss, and balanced class weighting.\n",
        "    3.  Sets up an early stopping callback based on the out-of-time validation set performance (`val_auc`).\n",
        "    4.  Trains the model and persists the final, best-performing version to disk.\n",
        "*   **Outputs:** A tuple containing the path to the saved model and a dictionary of training results.\n",
        "*   **Transformation:** The training data is transformed into a trained, serialized model artifact.\n",
        "*   **Role in Research Pipeline:** This callable implements the model training portion of **Step (3) \"Filter relevant articles\" (Page 2)**, resulting in the classifier that will be used to filter the entire corpus.\n",
        "\n",
        "#### **Task 9: `filter_corpus_by_relevance`**\n",
        "*   **Inputs:** The full augmented DataFrame, paths to the trained relevance model, embeddings, and crosswalk, `fused_master_input_specification`.\n",
        "*   **Processes:**\n",
        "    1.  Loads the trained relevance classifier.\n",
        "    2.  Runs batch inference on the *entire* corpus of embeddings to generate a relevance probability for every article.\n",
        "    3.  Applies the classification threshold (`τ=0.5`) to identify the subset of relevant articles (`R`).\n",
        "    4.  Persists all scores for auditability and creates the final filtered DataFrame `df_relevant`.\n",
        "*   **Outputs:** A tuple containing the `df_relevant` DataFrame, the path to the file of all scores, and an audit report.\n",
        "*   **Transformation:** The full corpus is transformed into a smaller, filtered corpus containing only articles deemed relevant to economics.\n",
        "*   **Role in Research Pipeline:** This callable completes **Step (3) \"Filter relevant articles\" (Page 2)**. It is the direct implementation of: \"We then use this model to identify articles related to economics in the data. We keep the articles related to economics and discard the others...\" **(Page 3)**.\n",
        "\n",
        "#### **Task 10: `generate_synthetic_articles`**\n",
        "*   **Inputs:** `fused_master_input_specification`, an output path.\n",
        "*   **Processes:**\n",
        "    1.  Configures a robust, resumable loop for interacting with the Claude 3.5 Sonnet API.\n",
        "    2.  Systematically constructs prompts based on the templates in Appendix A.1 to generate a balanced set of 256 articles (128 positive, 128 negative) across several economic domains.\n",
        "    3.  Validates the generated text for quality (word count, uniqueness).\n",
        "*   **Outputs:** The path to the saved CSV file containing the synthetic corpus.\n",
        "*   **Transformation:** Prompts are transformed into a structured, labeled dataset of synthetic text.\n",
        "*   **Role in Research Pipeline:** This callable implements **Step (4) \"Generate example articles\" (Page 2)**. It is the direct implementation of the novel data generation strategy described on **Page 3**: \"...we generate synthetic example articles conveying positive or negative economic outlook using the LLM Claude 3.5 Sonnet... We generate 256 articles...\".\n",
        "\n",
        "#### **Task 11: `process_synthetic_embeddings`**\n",
        "*   **Inputs:** The path to the synthetic corpus, `fused_master_input_specification`.\n",
        "*   **Processes:**\n",
        "    1.  Embeds the 256 synthetic articles using the *same* `jina-embeddings-v3` model from Task 6.\n",
        "    2.  Validates the numerical integrity and summary statistics of the synthetic embeddings.\n",
        "    3.  Performs UMAP dimensionality reduction and generates the annotated scatter plot shown in Chart 2.\n",
        "*   **Outputs:** A tuple containing the `(256, 1024)` NumPy array of synthetic embeddings (`Z`), the corresponding labels (`y`), and the path to the UMAP plot.\n",
        "*   **Transformation:** The synthetic text is transformed into a numerical matrix `Z`.\n",
        "*   **Role in Research Pipeline:** This callable prepares the data for and provides the visual evidence supporting **Step (5.a) \"Logistic regression\" (Page 2)**. The visualization directly replicates **Chart 2 (Page 4)** and validates the assumption that \"...The embedded articles clearly separate into two distinct clusters... which indicates that the embeddings effectively capture differences in sentiment.\"\n",
        "\n",
        "#### **Task 12: `train_sentiment_classifier`**\n",
        "*   **Inputs:** The synthetic embeddings `Z`, labels `y`, `fused_master_input_specification`.\n",
        "*   **Processes:**\n",
        "    1.  Performs a stratified K-fold cross-validated grid search to find the optimal L2 regularization strength `C`.\n",
        "    2.  Trains a final `LogisticRegression` model on the entire synthetic dataset using the optimal `C`.\n",
        "    3.  Persists the trained scikit-learn model object to disk.\n",
        "*   **Outputs:** A tuple containing the path to the saved model and a dictionary of CV results.\n",
        "*   **Transformation:** The synthetic training data is transformed into a trained, serialized sentiment classifier.\n",
        "*   **Role in Research Pipeline:** This callable implements **Step (5.a) \"Logistic regression\" (Page 2)**. It trains the model that minimizes the regularized negative log-likelihood:\n",
        "    $$\n",
        "    \\mathcal{L}(\\boldsymbol{\\beta}) = -\\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta})) + (1-y_i) \\log(1-\\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta})) \\right] + \\lambda ||\\boldsymbol{\\beta}||_2^2\n",
        "    $$\n",
        "    as described on **Page 4**, where the cross-validation finds the optimal regularization parameter $\\lambda = 1/C$.\n",
        "\n",
        "#### **Task 13: `score_relevant_articles`**\n",
        "*   **Inputs:** The `df_relevant` DataFrame, paths to the sentiment model, embeddings, and crosswalk.\n",
        "*   **Processes:**\n",
        "    1.  Loads the trained sentiment classifier.\n",
        "    2.  Runs batch inference on all embeddings corresponding to the relevant articles.\n",
        "    3.  For each article `i`, it calculates the probability of a positive outlook, $p_i$.\n",
        "    4.  Validates and persists the final DataFrame of scored articles.\n",
        "*   **Outputs:** The path to the saved file of scored articles.\n",
        "*   **Transformation:** The set of relevant articles is transformed by adding a sentiment score, $p_i$, to each one.\n",
        "*   **Role in Research Pipeline:** This callable implements **Step (5.b) \"Compute indicator\" (Page 2)**. It is the direct implementation of the inference step described on **Page 4**: \"We then apply the fitted logistic regression to the embeddings of the relevant news articles... This procedure results in a probability score between zero and one for each of the relevant articles.\" The probability score is calculated as:\n",
        "    $$\n",
        "    p_i = \\sigma\\big( \\hat{w}^\\top x_i + \\hat{b} \\big) = \\frac{1}{1 + \\exp\\big(- (\\hat{w}^\\top x_i + \\hat{b})\\big)}\n",
        "    $$\n",
        "\n",
        "#### **Task 14: `construct_neos_indicators`**\n",
        "*   **Inputs:** The path to the scored articles file, `fused_master_input_specification`.\n",
        "*   **Processes:**\n",
        "    1.  Aggregates the article-level scores (`p_i`) into a monthly time series by taking the simple arithmetic mean for each month.\n",
        "    2.  Repeats this aggregation on filtered subsets of the data to create the early-release variants (first 7, 14, 21 days).\n",
        "    3.  Optionally computes a daily, month-to-date cumulative average for diagnostic purposes.\n",
        "*   **Outputs:** A tuple containing the `pd.DataFrame` of all monthly indicators and the optional `pd.DataFrame` of the daily indicator.\n",
        "*   **Transformation:** The article-level data is transformed into aggregate monthly and daily time series.\n",
        "*   **Role in Research Pipeline:** This callable completes **Step (5.b) \"Compute indicator\" (Page 2)**. It implements the aggregation described on **Page 4**: \"...we compute our indicator by averaging all probability scores in each month.\" It computes the baseline indicator $\\mathrm{NEOS}_m$ and the early-release variants $\\mathrm{NEOS}_m^{(k)}$.\n",
        "\n",
        "#### **Task 15: `prepare_forecasting_dataset`**\n",
        "*   **Inputs:** All monthly indicator DataFrames and the raw quarterly macro DataFrame.\n",
        "*   **Processes:**\n",
        "    1.  Aligns all monthly indicators to a quarterly frequency using the specific information set policy (selecting month `m=2` for baseline indicators, `m=3` for early-release).\n",
        "    2.  Merges these aligned predictors with the quarterly GDP data.\n",
        "    3.  Creates the lagged dependent variable (`y_{t-1}`) required for the regression models.\n",
        "*   **Outputs:** A single, wide-format quarterly DataFrame ready for econometric modeling.\n",
        "*   **Transformation:** Multiple monthly and quarterly time series are transformed and merged into a single, aligned quarterly modeling dataset.\n",
        "*   **Role in Research Pipeline:** This callable prepares the final dataset for the **pseudo-out-of-sample experiment (Page 5)**. It correctly constructs the variables $y_{t-1}$ and $x_t^{(m)}$ for the forecasting regression.\n",
        "\n",
        "#### **Task 16: `execute_poos_forecasts`**\n",
        "*   **Inputs:** The final forecasting DataFrame, `fused_master_input_specification`, `evaluation_windows_df`.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each indicator and forecast horizon `h`.\n",
        "    2.  Implements the expanding-window POOS loop: at each time `t`, it fits the regression models on all data up to `t` and predicts for `t+h`.\n",
        "    3.  Correctly handles limited-availability indicators by restricting the evaluation period.\n",
        "    4.  Stores the forecast errors for every model, horizon, and time point.\n",
        "*   **Outputs:** A long-format `pd.DataFrame` containing all forecast errors.\n",
        "*   **Transformation:** The quarterly time-series data is transformed into a comprehensive set of out-of-sample forecast errors.\n",
        "*   **Role in Research Pipeline:** This callable is the core engine of the econometric evaluation. It executes the entire **pseudo-out-of-sample experiment (Page 5)** by repeatedly estimating the benchmark AR(1) model and the indicator-augmented model:\n",
        "    $$\n",
        "    y_{t+h} = \\alpha + \\beta y_{t-1} + \\gamma x_t^{(m)} + \\varepsilon_t \\quad \\quad (1)\n",
        "    $$\n",
        "\n",
        "#### **Task 17: `compute_rmse_ratios`**\n",
        "*   **Inputs:** The `forecast_errors_df` from Task 16.\n",
        "*   **Processes:**\n",
        "    1.  For each indicator and horizon, it finds the common sample of forecast origins shared with the AR(1) benchmark.\n",
        "    2.  It computes the RMSE for both the indicator model and the benchmark model on this identical sample.\n",
        "    3.  It computes the ratio of the two RMSEs.\n",
        "    4.  It assembles the results into a wide-format table.\n",
        "*   **Outputs:** A `pd.DataFrame` of RMSE ratios, replicating the structure of Table 1.\n",
        "*   **Transformation:** The long-format error DataFrame is transformed into a summary table of performance metrics.\n",
        "*   **Role in Research Pipeline:** This callable computes the primary evaluation metric reported in **Table 1 (Page 6)**. It calculates the ratio:\n",
        "    $$\n",
        "    R_h = \\frac{\\mathrm{RMSE}_{\\text{Equation (1)}}}{\\mathrm{RMSE}_{\\text{AR(1)}}}\n",
        "    $$\n",
        "\n",
        "#### **Task 18: `perform_diebold_mariano_tests`**\n",
        "*   **Inputs:** The `forecast_errors_df` and the `rmse_ratios_df`.\n",
        "*   **Processes:**\n",
        "    1.  For each indicator and horizon, it computes the loss differential series ($d_t$) on the common sample.\n",
        "    2.  It estimates the HAC-robust variance of the mean of $d_t$ using the Newey-West estimator with a Bartlett kernel.\n",
        "    3.  It computes the DM test statistic and its p-value.\n",
        "    4.  It appends the p-values to the RMSE ratio table.\n",
        "*   **Outputs:** The final evaluation table, augmented with DM p-values.\n",
        "*   **Transformation:** The forecast errors are transformed into test statistics and p-values.\n",
        "*   **Role in Research Pipeline:** This callable performs the statistical significance testing described on **Page 6**: \"We assess statistical significance by applying a modified Diebold-Mariano test.\" It provides the p-values reported in parentheses in **Table 1 (Page 6)**.\n",
        "\n",
        "#### **Task 19: `construct_lexicon_baseline_indicator`**\n",
        "*   **Inputs:** The `df_relevant` DataFrame, path to the lexicon file.\n",
        "*   **Processes:**\n",
        "    1.  Loads a translated German sentiment lexicon.\n",
        "    2.  Scores all relevant German-language articles based on the polarity of words in their text.\n",
        "    3.  Aggregates the article-level scores into a monthly time series.\n",
        "*   **Outputs:** A `pd.DataFrame` containing the monthly lexicon-based indicator.\n",
        "*   **Transformation:** A subset of the corpus is transformed into a new monthly time series.\n",
        "*   **Role in Research Pipeline:** This callable implements the construction of the **lexicon-based indicator (Appendix A.3, Page 11)**, which serves as a simpler, text-based baseline for comparison against the main NEOS indicator in the forecasting exercise.\n",
        "\n",
        "#### **Task 20: `compute_correlation_benchmarks`**\n",
        "*   **Inputs:** All monthly indicator series and the quarterly macro data.\n",
        "*   **Processes:**\n",
        "    1.  Transforms all monthly indicators to a quarterly frequency using simple three-month averaging.\n",
        "    2.  Computes the Pearson correlation between each indicator (at lags 0-4) and both yoy and qoq GDP growth.\n",
        "    3.  Assembles the results into two wide-format tables.\n",
        "*   **Outputs:** Two `pd.DataFrame`s containing the correlation results.\n",
        "*   **Transformation:** Time series are transformed into a table of correlation coefficients.\n",
        "*   **Role in Research Pipeline:** This callable replicates the descriptive analysis presented in **Appendix A.2 \"Correlations with GDP growth\"** and generates the results shown in **Table 2 (Page 11)**.\n",
        "\n",
        "#### **Task 21: `generate_all_charts`**\n",
        "*   **Inputs:** All necessary data artifacts (synthetic data, monthly/daily indicators, forecast errors, etc.).\n",
        "*   **Processes:** This orchestrator calls dedicated helper functions to generate each of the four main figures from the paper:\n",
        "    *   Chart 2: UMAP visualization.\n",
        "    *   Chart 3: Time-series comparison of indicators and GDP.\n",
        "    *   Chart 4: Daily month-to-date timeliness diagnostic.\n",
        "    *   Chart 5: Cumulative squared error difference for crisis performance analysis.\n",
        "*   **Outputs:** A dictionary of paths to the saved image files.\n",
        "*   **Transformation:** Various data artifacts are transformed into static visualizations.\n",
        "*   **Role in Research Pipeline:** This callable generates the key figures—**Chart 2 (Page 4), Chart 3 (Page 5), Chart 4 (Page 7), and Chart 5 (Page 12)**—that visually communicate the paper's main findings.\n",
        "\n",
        "#### **Task 22: `run_neos_pipeline`**\n",
        "*   **Inputs:** All raw data DataFrames, the master config, and an output directory.\n",
        "*   **Processes:** This is the main orchestrator. It executes the entire sequence of tasks from 1 to 21 in a single, robust, and resumable workflow. It manages data flow, checkpointing, logging, and final artifact generation.\n",
        "*   **Outputs:** A dictionary containing the run status and paths to all generated artifacts.\n",
        "*   **Transformation:** It orchestrates the entire transformation from raw data to final results.\n",
        "*   **Role in Research Pipeline:** This callable represents the entire end-to-end research pipeline itself.\n",
        "\n",
        "#### **Task 23: `run_robustness_analyses`**\n",
        "*   **Inputs:** All raw data DataFrames, the master config, and a base output directory.\n",
        "*   **Processes:** This is a \"meta-orchestrator.\" It systematically runs the main `run_neos_pipeline` multiple times, each time with a slightly modified configuration or input dataset, to test the sensitivity of the results to key methodological choices. It then aggregates the results from all runs.\n",
        "*   **Outputs:** A summary `pd.DataFrame` comparing the key evaluation metrics across all sensitivity runs.\n",
        "*   **Transformation:** It orchestrates multiple transformations and aggregates their final outputs.\n",
        "*   **Role in Research Pipeline:** This callable implements the **robustness analyses (Task 23)**, which are a critical part of any serious research project to ensure the main findings are not dependent on arbitrary modeling choices.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Example**\n",
        "\n",
        "The following is a complete, example of how to execute the end-to-end NEOS pipeline. It demonstrates how to prepare all necessary inputs—from the configuration file to high-fidelity synthetic data—and how to launch the top-level orchestrator, `run_complete_neos_study`. This example is structured as a self-contained script, assuming all previously defined functions are available in the execution environment (e.g., a single Jupyter notebook or a monolithic script).\n",
        "\n",
        "### **Implementation-Grade Example: Executing the NEOS Pipeline**\n",
        "\n",
        "This example is divided into three main sections:\n",
        "1.  **Setup and Configuration:** Loading the master configuration from a YAML file and setting up the environment.\n",
        "2.  **High-Fidelity Data Synthesis:** Programmatically generating realistic, schema-compliant synthetic data for all required DataFrame inputs. In a real-world scenario, this step would be replaced by loading data from actual databases or files.\n",
        "3.  **Pipeline Execution:** Calling the top-level orchestrator with the prepared data and configuration to run the entire study.\n",
        "\n",
        "#### **1. Setup and Configuration**\n",
        "\n",
        "Before running the pipeline, we must prepare the environment. This involves creating the master `config.yaml` file, setting the necessary API key as an environment variable, and loading the configuration into our Python script.\n",
        "\n",
        "##### **Step 1.1: Create the `config.yaml` file**\n",
        "\n",
        "First, save the complete configuration provided previously into a file named `config.yaml` in the same directory as your script or notebook. This file is the single source of truth for all parameters governing the pipeline's execution.\n",
        "\n",
        "##### **Step 1.2: Set the Environment Variable for the API Key**\n",
        "\n",
        "For security, the Anthropic API key must not be hardcoded. It should be set as an environment variable. In a terminal or at the beginning of your notebook, execute:\n",
        "\n",
        "```bash\n",
        "# Replace 'your_api_key_here' with your actual Anthropic API key\n",
        "export ANTHROPIC_API_KEY='your_api_key_here'\n",
        "```\n",
        "\n",
        "##### **Step 1.3: Load Configuration and Prepare Environment in Python**\n",
        "\n",
        "Now, in the Python script, we will load the YAML file and define the output directory.\n",
        "\n",
        "```python\n",
        "import yaml\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- Load Master Configuration ---\n",
        "# This block reads the YAML file and loads it into a Python dictionary.\n",
        "# This dictionary will be passed to the main orchestrator.\n",
        "CONFIG_PATH = 'config.yaml'\n",
        "with open(CONFIG_PATH, 'r') as f:\n",
        "    fused_master_input_specification = yaml.safe_load(f)\n",
        "\n",
        "# --- Define Output Directory ---\n",
        "# All artifacts from the pipeline runs will be saved here.\n",
        "ROOT_OUTPUT_DIRECTORY = './neos_study_output'\n",
        "\n",
        "# --- Define Path for Dummy Lexicon ---\n",
        "# In a real run, this would point to the translated Barbaglia et al. lexicon.\n",
        "# Here, we'll create a dummy version.\n",
        "LEXICON_PATH = './dummy_lexicon_de.csv'\n",
        "\n",
        "# Initialize Faker for data synthesis.\n",
        "fake = Faker()\n",
        "Faker.seed(42) # for reproducibility\n",
        "```\n",
        "\n",
        "#### **2. High-Fidelity Data Synthesis**\n",
        "\n",
        "The following functions generate realistic, schema-compliant synthetic data for each of the five DataFrames required by the pipeline. This is a crucial step for testing and demonstration.\n",
        "\n",
        "```python\n",
        "def create_synthetic_news_data(num_articles: int, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"Generates a high-fidelity synthetic news corpus DataFrame.\"\"\"\n",
        "    data = []\n",
        "    outlets = {\n",
        "        \"NZZ\": \"Neue Zürcher Zeitung\",\n",
        "        \"TA\": \"Tages-Anzeiger\",\n",
        "        \"LT\": \"Le Temps\"\n",
        "    }\n",
        "    sections = config['master_config']['relevance_model_params']['positive_class_sections'] + ['Sport', 'Kultur', 'International']\n",
        "    \n",
        "    for _ in range(num_articles):\n",
        "        pub_time = fake.date_time_between(start_date='-25y', end_date='now', tzinfo=pytz.utc)\n",
        "        data.append({\n",
        "            \"article_id\": f\"ART_{fake.uuid4()}\",\n",
        "            \"outlet_id\": np.random.choice(list(outlets.keys())),\n",
        "            \"publication_type\": np.random.choice(config['master_config']['data_curation_params']['publication_types_vocab'], p=[0.4, 0.5, 0.02, 0.02, 0.02, 0.02, 0.0]),\n",
        "            \"publication_datetime_utc\": pub_time,\n",
        "            \"publication_timezone\": \"Europe/Zurich\",\n",
        "            \"language\": np.random.choice(['de', 'fr']),\n",
        "            \"section_label\": np.random.choice(sections + [None]*len(sections)), # 50% chance of being null\n",
        "            \"headline\": fake.sentence(nb_words=8),\n",
        "            \"full_text\": fake.paragraph(nb_sentences=15),\n",
        "            \"ingestion_timestamp_utc\": pub_time + pd.Timedelta(minutes=np.random.randint(1, 60)),\n",
        "            \"last_modified_timestamp_utc\": pub_time + pd.Timedelta(minutes=np.random.randint(60, 120)),\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    # Ensure correct dtypes as expected by the validation functions.\n",
        "    df['source_outlet'] = df['outlet_id'].map(outlets)\n",
        "    df['publication_type'] = df['publication_type'].astype('category')\n",
        "    df = df.astype({\n",
        "        'article_id': 'string', 'outlet_id': 'string', 'source_outlet': 'string',\n",
        "        'language': 'string', 'section_label': 'string', 'headline': 'string', 'full_text': 'string'\n",
        "    })\n",
        "    return df\n",
        "\n",
        "def create_synthetic_macro_data(start_date: str, end_date: str) -> pd.DataFrame:\n",
        "    \"\"\"Generates a high-fidelity synthetic quarterly macro DataFrame.\"\"\"\n",
        "    quarters = pd.date_range(start=start_date, end=end_date, freq='QS-JAN')\n",
        "    data = {\n",
        "        \"yoy_gdp_growth_sports_adj\": np.random.normal(1.5, 2.0, len(quarters)),\n",
        "        \"manufacturing_pmi_m2\": np.random.normal(52, 5, len(quarters)),\n",
        "        \"service_pmi_m2\": np.random.normal(55, 6, len(quarters)),\n",
        "        \"kof_biz_situation_m2\": np.random.normal(0.5, 1.5, len(quarters)),\n",
        "        \"seco_consumer_sentiment_q\": np.random.normal(-5, 10, len(quarters)),\n",
        "    }\n",
        "    df = pd.DataFrame(data, index=quarters)\n",
        "    \n",
        "    # Simulate historical unavailability as per the paper's footnotes.\n",
        "    df.loc[df.index < '2014-01-01', 'service_pmi_m2'] = np.nan\n",
        "    df.loc[df.index < '2009-04-01', 'kof_biz_situation_m2'] = np.nan\n",
        "    return df\n",
        "\n",
        "def create_synthetic_monthly_indicators(start_date: str, end_date: str) -> pd.DataFrame:\n",
        "    \"\"\"Generates a high-fidelity synthetic monthly indicators DataFrame.\"\"\"\n",
        "    months = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
        "    data = {\n",
        "        \"manufacturing_pmi\": np.random.normal(52, 5, len(months)),\n",
        "        \"service_pmi\": np.random.normal(55, 6, len(months)),\n",
        "        \"kof_biz_situation\": np.random.normal(0.5, 1.5, len(months)),\n",
        "    }\n",
        "    df = pd.DataFrame(data, index=months)\n",
        "    \n",
        "    # Simulate historical unavailability.\n",
        "    df.loc[df.index < '2014-01-01', 'service_pmi'] = np.nan\n",
        "    df.loc[df.index < '2009-04-01', 'kof_biz_situation'] = np.nan\n",
        "    return df\n",
        "\n",
        "def create_synthetic_metadata_tables() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Generates the small, static metadata tables.\"\"\"\n",
        "    # evaluation_windows_df\n",
        "    eval_data = [\n",
        "        {'series_name': 'service_pmi', 'start_quarter': pd.Timestamp('2014-01-01', freq='QS-JAN'), 'end_quarter': pd.NaT},\n",
        "        {'series_name': 'kof_biz_situation', 'start_quarter': pd.Timestamp('2009-04-01', freq='QS-JAN'), 'end_quarter': pd.NaT}\n",
        "    ]\n",
        "    evaluation_windows_df = pd.DataFrame(eval_data)\n",
        "\n",
        "    # release_calendar_df (a small sample)\n",
        "    cal_data = [\n",
        "        {'series_name': 'manufacturing_pmi', 'period': pd.Timestamp('2024-01-01'), 'release_datetime_utc': pd.Timestamp('2024-01-31 08:00:00', tz='UTC'), 'notes': ''},\n",
        "        {'series_name': 'seco_consumer_sentiment', 'period': pd.Timestamp('2024-01-01'), 'release_datetime_utc': pd.Timestamp('2024-02-15 09:00:00', tz='UTC'), 'notes': ''}\n",
        "    ]\n",
        "    release_calendar_df = pd.DataFrame(cal_data)\n",
        "    \n",
        "    return release_calendar_df, evaluation_windows_df\n",
        "\n",
        "def create_dummy_lexicon(path: str):\n",
        "    \"\"\"Creates a small dummy lexicon file.\"\"\"\n",
        "    lex_data = [\n",
        "        {'word_de': 'gut', 'sentiment': 'positive'},\n",
        "        {'word_de': 'wachstum', 'sentiment': 'positive'},\n",
        "        {'word_de': 'schlecht', 'sentiment': 'negative'},\n",
        "        {'word_de': 'krise', 'sentiment': 'negative'},\n",
        "    ]\n",
        "    pd.DataFrame(lex_data).to_csv(path, index=False)\n",
        "\n",
        "# --- Generate all synthetic data artifacts ---\n",
        "print(\"--- Generating high-fidelity synthetic data for demonstration ---\")\n",
        "# A smaller number of articles for a runnable example. A full run would use millions.\n",
        "raw_news_data_df = create_synthetic_news_data(1000, fused_master_input_specification)\n",
        "raw_macro_data_df = create_synthetic_macro_data('1999-01-01', '2025-05-31')\n",
        "monthly_indicator_data_df = create_synthetic_monthly_indicators('1999-01-01', '2025-05-31')\n",
        "release_calendar_df, evaluation_windows_df = create_synthetic_metadata_tables()\n",
        "create_dummy_lexicon(LEXICON_PATH)\n",
        "print(\"Synthetic data generation complete.\")\n",
        "```\n",
        "\n",
        "#### **3. Pipeline Execution**\n",
        "\n",
        "With all inputs prepared (the configuration dictionary and the five DataFrames), we can now call the top-level orchestrator. This single function call will execute the entire study, including the baseline run and all robustness checks, leveraging the checkpointing system for efficiency.\n",
        "\n",
        "```python\n",
        "# --- Execute the Full Study ---\n",
        "# This is the main entry point that runs the entire analysis.\n",
        "# It will create a 'baseline_run' directory for the main results and a\n",
        "# 'robustness_checks' directory for all the sensitivity analyses.\n",
        "# The function will print detailed logs to the console and save them to a file.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This check is common in scripts to ensure the main execution block\n",
        "    # only runs when the script is executed directly.\n",
        "    \n",
        "    final_results = run_complete_neos_study(\n",
        "        # Pass all the prepared data and configuration.\n",
        "        raw_news_data_df=raw_news_data_df,\n",
        "        raw_macro_data_df=raw_macro_data_df,\n",
        "        monthly_indicator_data_df=monthly_indicator_data_df,\n",
        "        release_calendar_df=release_calendar_df,\n",
        "        evaluation_windows_df=evaluation_windows_df,\n",
        "        fused_master_input_specification=fused_master_input_specification,\n",
        "        root_output_directory=ROOT_OUTPUT_DIRECTORY,\n",
        "        lexicon_path=LEXICON_PATH\n",
        "    )\n",
        "\n",
        "    # --- Review Outputs ---\n",
        "    # After the (potentially very long) run, you can inspect the results.\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\">>> TOP-LEVEL ORCHESTRATION COMPLETE <<<\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Print the status of the baseline run.\n",
        "    baseline_status = final_results.get('baseline_run_results', {}).get('status', 'Unknown')\n",
        "    print(f\"\\nBaseline Run Status: {baseline_status}\")\n",
        "    \n",
        "    # Display the head of the robustness analysis summary.\n",
        "    robustness_summary = final_results.get('robustness_summary_df')\n",
        "    if robustness_summary is not None and not robustness_summary.empty:\n",
        "        print(\"\\n--- Head of Robustness Analysis Summary ---\")\n",
        "        print(robustness_summary.head(10))\n",
        "    else:\n",
        "        print(\"\\nRobustness analysis did not produce a summary table.\")\n",
        "        \n",
        "    print(f\"\\nAll artifacts, logs, and results are saved in: '{ROOT_OUTPUT_DIRECTORY}'\")\n",
        "```\n",
        "<br>"
      ],
      "metadata": {
        "id": "5Ajfbn_alxuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate the raw news corpus schema and data quality\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate the raw news corpus schema and data quality\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1:  Assert schema compliance and dtype correctness\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_schema_and_dtypes(\n",
        "    raw_news_data_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the schema, dtypes, and primary key integrity of the raw news DataFrame.\n",
        "\n",
        "    This function performs critical low-level checks to ensure the input DataFrame\n",
        "    conforms to the required structure for all downstream processing. It verifies:\n",
        "    1. Presence of all required columns.\n",
        "    2. Correctness of data types for each column, with special attention to\n",
        "       timezone-aware UTC datetimes.\n",
        "    3. Uniqueness of the 'article_id' primary key.\n",
        "\n",
        "    Args:\n",
        "        raw_news_data_df: The raw news corpus DataFrame to be validated.\n",
        "\n",
        "    Returns:\n",
        "        A list of string messages describing non-critical validation warnings.\n",
        "        An empty list indicates this step passed without warnings.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a critical validation check fails (e.g., missing\n",
        "                    columns, incorrect dtypes, non-UTC timestamps, or\n",
        "                    duplicate article_ids), which would prevent further\n",
        "                    processing.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect non-critical warnings.\n",
        "    warnings = []\n",
        "\n",
        "    # Define the target schema with required columns and their expected dtypes.\n",
        "    # Using pandas' string dtype for text columns is best practice.\n",
        "    expected_schema = {\n",
        "        'article_id': 'string',\n",
        "        'outlet_id': 'string', # Allow flexible storage (string or int) but validate as string\n",
        "        'source_outlet': 'string',\n",
        "        'publication_type': 'category',\n",
        "        'publication_datetime_utc': 'datetime64[ns, UTC]',\n",
        "        'language': 'string',\n",
        "        'section_label': 'string',\n",
        "        'full_text': 'string',\n",
        "        'ingestion_timestamp_utc': 'datetime64[ns, UTC]',\n",
        "        'last_modified_timestamp_utc': 'datetime64[ns, UTC]',\n",
        "    }\n",
        "\n",
        "    # --- Schema Completeness Check ---\n",
        "    # Check for any missing columns from the expected schema.\n",
        "    missing_columns = set(expected_schema.keys()) - set(raw_news_data_df.columns)\n",
        "    # If any required columns are missing, this is a critical failure.\n",
        "    if missing_columns:\n",
        "        # Raise a ValueError with a descriptive message.\n",
        "        raise ValueError(\n",
        "            \"Critical schema validation failed: Missing required columns: \"\n",
        "            f\"{sorted(list(missing_columns))}\"\n",
        "        )\n",
        "\n",
        "    # --- Dtype and Timezone Correctness Check ---\n",
        "    # Iterate through the expected schema to validate each column's dtype.\n",
        "    for col, expected_dtype in expected_schema.items():\n",
        "        # Get the actual dtype of the column from the DataFrame.\n",
        "        actual_dtype = raw_news_data_df[col].dtype\n",
        "\n",
        "        # Handle the special case for datetime columns.\n",
        "        if 'datetime' in expected_dtype:\n",
        "            # Check if the column is a datetime type.\n",
        "            if not pd.api.types.is_datetime64_any_dtype(actual_dtype):\n",
        "                # If not, this is a critical failure.\n",
        "                raise ValueError(\n",
        "                    f\"Critical dtype validation failed for column '{col}': \"\n",
        "                    f\"Expected a datetime dtype, but found '{actual_dtype}'.\"\n",
        "                )\n",
        "            # Check if the datetime column is timezone-aware.\n",
        "            if raw_news_data_df[col].dt.tz is None:\n",
        "                # Naive datetimes are a critical failure as they are ambiguous.\n",
        "                raise ValueError(\n",
        "                    f\"Critical timezone validation failed for column '{col}': \"\n",
        "                    \"Column is timezone-naive. All datetimes must be \"\n",
        "                    \"timezone-aware and in UTC.\"\n",
        "                )\n",
        "            # Check if the timezone is specifically UTC.\n",
        "            if str(raw_news_data_df[col].dt.tz) != 'UTC':\n",
        "                # Non-UTC timezones are a critical failure.\n",
        "                raise ValueError(\n",
        "                    f\"Critical timezone validation failed for column '{col}': \"\n",
        "                    f\"Expected timezone 'UTC', but found \"\n",
        "                    f\"'{raw_news_data_df[col].dt.tz}'.\"\n",
        "                )\n",
        "        # Handle the special case for outlet_id, which can be int or string.\n",
        "        elif col == 'outlet_id':\n",
        "            # Check if the dtype is numeric or string-like.\n",
        "            if not (pd.api.types.is_numeric_dtype(actual_dtype) or \\\n",
        "                    pd.api.types.is_string_dtype(actual_dtype)):\n",
        "                raise ValueError(\n",
        "                    f\"Critical dtype validation failed for column '{col}': \"\n",
        "                    f\"Expected numeric or string dtype, but found '{actual_dtype}'.\"\n",
        "                )\n",
        "        # For all other columns, perform a direct dtype comparison.\n",
        "        elif str(actual_dtype) != expected_dtype:\n",
        "            # A dtype mismatch is a critical failure.\n",
        "            raise ValueError(\n",
        "                f\"Critical dtype validation failed for column '{col}': \"\n",
        "                f\"Expected dtype '{expected_dtype}', but found '{actual_dtype}'.\"\n",
        "            )\n",
        "\n",
        "    # --- Primary Key Uniqueness Check ---\n",
        "    # Check for duplicate values in the 'article_id' column.\n",
        "    if raw_news_data_df['article_id'].duplicated().any():\n",
        "        # Duplicate primary keys are a critical data integrity failure.\n",
        "        raise ValueError(\n",
        "            \"Critical primary key validation failed: \"\n",
        "            \"Column 'article_id' contains duplicate values.\"\n",
        "        )\n",
        "\n",
        "    # If all checks pass, return the list of any non-critical warnings.\n",
        "    return warnings\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate controlled vocabularies and cross-field consistency\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_vocabularies_and_consistency(\n",
        "    raw_news_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates controlled vocabularies and cross-field consistency rules.\n",
        "\n",
        "    Purpose:\n",
        "    This function performs critical data quality checks on categorical and\n",
        "    identifier columns. It ensures that values in key fields adhere to a\n",
        "    predefined set of allowed options (controlled vocabulary) and that logical\n",
        "    relationships between fields (like a one-to-one mapping) are maintained.\n",
        "    This prevents unexpected or invalid data from propagating into the modeling\n",
        "    pipeline. All vocabulary checks are performed in a case-insensitive manner\n",
        "    for robustness.\n",
        "\n",
        "    Inputs:\n",
        "        raw_news_data_df (pd.DataFrame): The raw news corpus DataFrame.\n",
        "        config (Dict[str, Any]): The 'data_curation_params' sub-dictionary from\n",
        "                                 the master configuration, containing the\n",
        "                                 validation rules and vocabularies.\n",
        "\n",
        "    Processes:\n",
        "    1.  **Publication Type Validation:**\n",
        "        a. Retrieves the allowed `publication_types_vocab` from the config.\n",
        "        b. Finds all unique, non-null values in the 'publication_type' column.\n",
        "        c. Performs a case-insensitive comparison to identify any values not in\n",
        "           the allowed vocabulary.\n",
        "    2.  **Language Validation:**\n",
        "        a. Retrieves the allowed `included_languages` from the config.\n",
        "        b. Performs a case-insensitive check to ensure all languages in the\n",
        "           'language' column are in the allowed set.\n",
        "    3.  **Cross-Field Consistency:**\n",
        "        a. Verifies that a one-to-one mapping exists between 'outlet_id' and\n",
        "           'source_outlet' by grouping by 'outlet_id' and counting the number\n",
        "           of unique 'source_outlet' names.\n",
        "\n",
        "    Outputs:\n",
        "        (List[str]): A list of string messages describing all validation warnings\n",
        "                     found. An empty list indicates that all checks in this\n",
        "                     function passed successfully.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(raw_news_data_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input `raw_news_data_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input `config` must be a dictionary.\")\n",
        "\n",
        "    # Initialize a list to collect all validation warnings.\n",
        "    warnings: List[str] = []\n",
        "\n",
        "    # --- 1. Controlled Vocabulary Validation for 'publication_type' (Case-Insensitive) ---\n",
        "    # Retrieve the allowed vocabulary from the config and convert to a lowercase set for efficient lookup.\n",
        "    publication_vocab_lower = {\n",
        "        v.lower() for v in config.get('publication_types_vocab', [])\n",
        "    }\n",
        "    # Get the unique, non-null, lowercase values present in the DataFrame column.\n",
        "    actual_publication_types_lower = set(\n",
        "        raw_news_data_df['publication_type'].dropna().str.lower().unique()\n",
        "    )\n",
        "    # Identify any values that are not in the allowed vocabulary using set difference.\n",
        "    invalid_publication_types = actual_publication_types_lower - publication_vocab_lower\n",
        "    # If any invalid values are found, create a detailed warning message.\n",
        "    if invalid_publication_types:\n",
        "        warnings.append(\n",
        "            \"Vocabulary Warning for 'publication_type': Found values not in \"\n",
        "            f\"the controlled vocabulary: {sorted(list(invalid_publication_types))}\"\n",
        "        )\n",
        "\n",
        "    # --- 2. Controlled Vocabulary Validation for 'language' (Case-Insensitive) ---\n",
        "    # Retrieve the allowed languages and convert to a lowercase set.\n",
        "    language_vocab_lower = {\n",
        "        lang.lower() for lang in config.get('included_languages', [])\n",
        "    }\n",
        "    # Get the unique, non-null, lowercase languages present in the DataFrame.\n",
        "    actual_languages_lower = set(\n",
        "        raw_news_data_df['language'].dropna().str.lower().unique()\n",
        "    )\n",
        "    # Identify any languages that are not in the allowed set.\n",
        "    invalid_languages = actual_languages_lower - language_vocab_lower\n",
        "    # If invalid languages are found, create a detailed warning message.\n",
        "    if invalid_languages:\n",
        "        warnings.append(\n",
        "            \"Vocabulary Warning for 'language': Found values not in the \"\n",
        "            f\"allowed set {sorted(list(language_vocab_lower))}: \"\n",
        "            f\"{sorted(list(invalid_languages))}\"\n",
        "        )\n",
        "\n",
        "    # --- 3. Cross-Field Consistency: One-to-One Mapping Validation ---\n",
        "    # This check ensures data integrity for outlet identifiers.\n",
        "    # Group by the canonical 'outlet_id' and count the number of unique human-readable 'source_outlet' names.\n",
        "    outlet_mapping_counts = raw_news_data_df.groupby('outlet_id')['source_outlet'].nunique()\n",
        "    # Filter for any 'outlet_id's that map to more than one 'source_outlet', indicating an inconsistency.\n",
        "    mapping_violations = outlet_mapping_counts[outlet_mapping_counts > 1]\n",
        "    # If any violations are found, generate a specific warning for each.\n",
        "    if not mapping_violations.empty:\n",
        "        # Iterate through each inconsistent 'outlet_id' to create a detailed report.\n",
        "        for outlet_id in mapping_violations.index:\n",
        "            # Retrieve all the conflicting names associated with this ID.\n",
        "            conflicting_names = raw_news_data_df[\n",
        "                raw_news_data_df['outlet_id'] == outlet_id\n",
        "            ]['source_outlet'].unique()\n",
        "            # Create a detailed, actionable warning message.\n",
        "            warnings.append(\n",
        "                \"Consistency Warning: 'outlet_id' to 'source_outlet' mapping is \"\n",
        "                f\"not one-to-one. ID '{outlet_id}' maps to multiple names: \"\n",
        "                f\"{sorted(list(conflicting_names))}. Manual curation is required.\"\n",
        "            )\n",
        "\n",
        "    # Return the aggregated list of all warnings found during the validation.\n",
        "    return warnings\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Quantify missingness and temporal integrity\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_missingness_and_temporal_integrity(\n",
        "    raw_news_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Quantifies missing data and validates temporal integrity rules.\n",
        "\n",
        "    This function performs three checks:\n",
        "    1. Quantifies the percentage of non-null 'section_label' values, grouped\n",
        "       by outlet and month.\n",
        "    2. Asserts that 'publication_datetime_utc' is not later than\n",
        "       'ingestion_timestamp_utc'.\n",
        "    3. Verifies that all publication datetimes fall within the study's\n",
        "       configured date range.\n",
        "\n",
        "    Args:\n",
        "        raw_news_data_df: The raw news corpus DataFrame.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A DataFrame with the monthly percentage of non-null 'section_label'\n",
        "          values per outlet.\n",
        "        - A list of string messages describing temporal integrity warnings.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect temporal integrity warnings.\n",
        "    warnings = []\n",
        "\n",
        "    # Extract the relevant curation parameters from the config.\n",
        "    curation_params = config['data_curation_params']\n",
        "\n",
        "    # --- Missingness Quantification for 'section_label' ---\n",
        "    # Group by outlet and month, then calculate the mean of non-null values.\n",
        "    # This gives the percentage of articles with a section label.\n",
        "    missingness_report = raw_news_data_df.groupby(\n",
        "        ['outlet_id', pd.Grouper(key='publication_datetime_utc', freq='MS')]\n",
        "    )['section_label'].apply(lambda s: s.notna().mean()).reset_index()\n",
        "    # Rename the column for clarity.\n",
        "    missingness_report.rename(\n",
        "        columns={'section_label': 'section_label_completeness_ratio'},\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    # --- Temporal Integrity Check 1: Publication vs. Ingestion Time ---\n",
        "    # Find all records where the publication time is after the ingestion time.\n",
        "    future_dated_mask = (\n",
        "        raw_news_data_df['publication_datetime_utc'] >\n",
        "        raw_news_data_df['ingestion_timestamp_utc']\n",
        "    )\n",
        "    # If any such records exist, report them.\n",
        "    if future_dated_mask.any():\n",
        "        num_violations = future_dated_mask.sum()\n",
        "        # Create a warning message summarizing the issue.\n",
        "        warnings.append(\n",
        "            f\"Temporal Integrity Warning: Found {num_violations} articles where \"\n",
        "            \"'publication_datetime_utc' is after 'ingestion_timestamp_utc'.\"\n",
        "        )\n",
        "\n",
        "    # --- Temporal Integrity Check 2: Adherence to Study Date Range ---\n",
        "    # Convert the start and end date strings from the config to timestamps.\n",
        "    start_date = pd.to_datetime(curation_params['start_date'], utc=True)\n",
        "    end_date = pd.to_datetime(curation_params['end_date'], utc=True)\n",
        "    # Find records with publication dates outside the allowed range.\n",
        "    out_of_range_mask = ~raw_news_data_df['publication_datetime_utc'].between(\n",
        "        start_date, end_date, inclusive='both'\n",
        "    )\n",
        "    # If any out-of-range records exist, report them.\n",
        "    if out_of_range_mask.any():\n",
        "        num_violations = out_of_range_mask.sum()\n",
        "        # Create a warning message summarizing the issue.\n",
        "        warnings.append(\n",
        "            f\"Temporal Integrity Warning: Found {num_violations} articles with \"\n",
        "            \"'publication_datetime_utc' outside the configured study range \"\n",
        "            f\"of [{start_date.date()}, {end_date.date()}].\"\n",
        "        )\n",
        "\n",
        "    # Return the missingness report and the list of warnings.\n",
        "    return missingness_report, warnings\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_raw_news_corpus(\n",
        "    raw_news_data_df: pd.DataFrame,\n",
        "    fused_master_input_specification: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the raw news corpus DataFrame.\n",
        "\n",
        "    This function serves as a high-level entry point to execute all validation\n",
        "    steps for the raw news data. It checks schema, dtypes, vocabularies,\n",
        "    consistency, missingness, and temporal integrity. It aggregates all\n",
        "    findings into a structured report.\n",
        "\n",
        "    Args:\n",
        "        raw_news_data_df: The raw news corpus as a pandas DataFrame.\n",
        "        fused_master_input_specification: The master configuration dictionary\n",
        "            containing all study parameters and validation rules.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the validation results:\n",
        "        - 'status': 'Success' or 'Failure'.\n",
        "        - 'warnings': A list of all non-critical warning messages.\n",
        "        - 'errors': A list of critical error messages that halted execution.\n",
        "        - 'missingness_report': A DataFrame detailing 'section_label'\n",
        "          completeness.\n",
        "    \"\"\"\n",
        "    # Initialize the results dictionary.\n",
        "    validation_results = {\n",
        "        \"status\": \"Success\",\n",
        "        \"warnings\": [],\n",
        "        \"errors\": [],\n",
        "        \"missingness_report\": pd.DataFrame()\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # --- Step 1: Validate Schema and Dtypes ---\n",
        "        # Execute the schema and dtype validation.\n",
        "        schema_warnings = _validate_schema_and_dtypes(raw_news_data_df)\n",
        "        # Append any non-critical warnings to the main list.\n",
        "        validation_results[\"warnings\"].extend(schema_warnings)\n",
        "\n",
        "        # --- Step 2: Validate Vocabularies and Consistency ---\n",
        "        # Execute the vocabulary and consistency checks.\n",
        "        vocab_warnings = _validate_vocabularies_and_consistency(\n",
        "            raw_news_data_df, fused_master_input_specification['master_config']\n",
        "        )\n",
        "        # Append any warnings found.\n",
        "        validation_results[\"warnings\"].extend(vocab_warnings)\n",
        "\n",
        "        # --- Step 3: Validate Missingness and Temporal Integrity ---\n",
        "        # Execute the missingness and temporal integrity checks.\n",
        "        missingness_df, temporal_warnings = _validate_missingness_and_temporal_integrity(\n",
        "            raw_news_data_df, fused_master_input_specification['master_config']\n",
        "        )\n",
        "        # Store the missingness report.\n",
        "        validation_results[\"missingness_report\"] = missingness_df\n",
        "        # Append any warnings found.\n",
        "        validation_results[\"warnings\"].extend(temporal_warnings)\n",
        "\n",
        "    # Catch any critical ValueError raised during validation.\n",
        "    except ValueError as e:\n",
        "        # If a critical error occurs, update the status and log the error.\n",
        "        validation_results[\"status\"] = \"Failure\"\n",
        "        validation_results[\"errors\"].append(str(e))\n",
        "\n",
        "    # Return the comprehensive validation report.\n",
        "    return validation_results\n"
      ],
      "metadata": {
        "id": "LLGVhcKKNlY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate macro and indicator dataframes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate macro and indicator dataframes\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Validate raw_macro_data_df structure and quarterly alignment\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_quarterly_macro_data(\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    monthly_indicator_data_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structure, dtypes, and alignment of the quarterly macro DataFrame.\n",
        "\n",
        "    This function performs rigorous checks on `raw_macro_data_df`:\n",
        "    1. Asserts the index is a DatetimeIndex with quarter-start frequency ('QS-JAN').\n",
        "    2. Verifies the presence and float64 dtype of all required columns.\n",
        "    3. Cross-validates a sample of month-2 indicators (e.g., 'manufacturing_pmi_m2')\n",
        "       against the authoritative monthly data in `monthly_indicator_data_df` to\n",
        "       ensure the temporal alignment logic is correct.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_data_df: The quarterly DataFrame of macro and indicator data.\n",
        "        monthly_indicator_data_df: The monthly DataFrame serving as ground truth.\n",
        "\n",
        "    Returns:\n",
        "        A list of string messages describing validation warnings or errors. An\n",
        "        empty list signifies a successful validation.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect all validation issues.\n",
        "    issues = []\n",
        "\n",
        "    # --- Index and Schema Validation ---\n",
        "    # Verify that the index is a DatetimeIndex.\n",
        "    if not isinstance(raw_macro_data_df.index, pd.DatetimeIndex):\n",
        "        issues.append(\n",
        "            \"Critical Error: `raw_macro_data_df` index is not a DatetimeIndex.\"\n",
        "        )\n",
        "        return issues # Halt further checks as they depend on a valid index.\n",
        "\n",
        "    # Verify the index frequency is Quarter Start ('QS-JAN').\n",
        "    if raw_macro_data_df.index.freqstr != 'QS-JAN':\n",
        "        issues.append(\n",
        "            \"Critical Error: `raw_macro_data_df` index frequency is not \"\n",
        "            f\"'QS-JAN'. Found '{raw_macro_data_df.index.freqstr}'.\"\n",
        "        )\n",
        "\n",
        "    # Define the expected schema for the quarterly data.\n",
        "    expected_schema = {\n",
        "        'yoy_gdp_growth_sports_adj': 'float64',\n",
        "        'manufacturing_pmi_m2': 'float64',\n",
        "        'service_pmi_m2': 'float64',\n",
        "        'kof_biz_situation_m2': 'float64',\n",
        "        'seco_consumer_sentiment_q': 'float64',\n",
        "    }\n",
        "\n",
        "    # Check for missing columns.\n",
        "    missing_cols = set(expected_schema.keys()) - set(raw_macro_data_df.columns)\n",
        "    if missing_cols:\n",
        "        issues.append(f\"Critical Error: Missing columns in `raw_macro_data_df`: {missing_cols}.\")\n",
        "\n",
        "    # Check dtypes for existing columns.\n",
        "    for col, dtype in expected_schema.items():\n",
        "        if col in raw_macro_data_df.columns and raw_macro_data_df[col].dtype != dtype:\n",
        "            issues.append(\n",
        "                f\"Critical Error: Column '{col}' has incorrect dtype. \"\n",
        "                f\"Expected '{dtype}', found '{raw_macro_data_df[col].dtype}'.\"\n",
        "            )\n",
        "\n",
        "    # If critical schema errors were found, return now.\n",
        "    if issues:\n",
        "        return issues\n",
        "\n",
        "    # --- Cross-Validation of m=2 Alignment ---\n",
        "    # Define the mapping from quarterly m=2 columns to their monthly source columns.\n",
        "    m2_mapping = {\n",
        "        'manufacturing_pmi_m2': 'manufacturing_pmi',\n",
        "        'service_pmi_m2': 'service_pmi',\n",
        "        'kof_biz_situation_m2': 'kof_biz_situation',\n",
        "    }\n",
        "\n",
        "    # Select a deterministic, reasonably sized sample of quarters to check.\n",
        "    # Use min to handle cases with fewer than 10 quarters.\n",
        "    sample_size = min(10, len(raw_macro_data_df))\n",
        "    if sample_size > 0:\n",
        "        sample_quarters = raw_macro_data_df.sample(n=sample_size, random_state=42).index\n",
        "\n",
        "        # Iterate through the sampled quarters to perform the check.\n",
        "        for quarter_start in sample_quarters:\n",
        "            # The second month of a quarter starting at `t` is at `t + 1 month`.\n",
        "            second_month_ts = quarter_start + pd.DateOffset(months=1)\n",
        "\n",
        "            # Check each m=2 indicator.\n",
        "            for q_col, m_col in m2_mapping.items():\n",
        "                # Get the value from the quarterly DataFrame.\n",
        "                quarterly_value = raw_macro_data_df.loc[quarter_start, q_col]\n",
        "\n",
        "                # Get the expected value from the monthly DataFrame.\n",
        "                try:\n",
        "                    monthly_value = monthly_indicator_data_df.loc[second_month_ts, m_col]\n",
        "                except KeyError:\n",
        "                    # If the timestamp doesn't exist in the monthly data, it's an issue.\n",
        "                    issues.append(\n",
        "                        f\"Alignment Error: For quarter '{quarter_start.date()}', the \"\n",
        "                        f\"corresponding month '{second_month_ts.date()}' was not found \"\n",
        "                        f\"in `monthly_indicator_data_df`.\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                # Compare the values, handling NaNs correctly.\n",
        "                # Both must be NaN or both must be non-NaN and close.\n",
        "                if pd.isna(quarterly_value) and pd.isna(monthly_value):\n",
        "                    continue # This is a correct alignment.\n",
        "\n",
        "                if pd.isna(quarterly_value) or pd.isna(monthly_value) or \\\n",
        "                   not np.isclose(quarterly_value, monthly_value, rtol=1e-5, atol=1e-8):\n",
        "                    issues.append(\n",
        "                        f\"Alignment Error: Mismatch for '{q_col}' in quarter \"\n",
        "                        f\"'{quarter_start.date()}'. Quarterly value is \"\n",
        "                        f\"{quarterly_value}, but expected value from month \"\n",
        "                        f\"'{second_month_ts.date()}' is {monthly_value}.\"\n",
        "                    )\n",
        "\n",
        "    return issues\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate monthly_indicator_data_df and native-frequency alignment\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_monthly_indicator_data(\n",
        "    monthly_indicator_data_df: pd.DataFrame,\n",
        "    evaluation_windows_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structure and historical availability of the monthly indicator DataFrame.\n",
        "\n",
        "    This function checks `monthly_indicator_data_df` for:\n",
        "    1. A DatetimeIndex with month-start frequency ('MS').\n",
        "    2. Presence and float64 dtype of required indicator columns.\n",
        "    3. Historical NaN patterns, ensuring no data exists for series before their\n",
        "       official start dates as defined in `evaluation_windows_df`. This prevents\n",
        "       silent backfilling or interpolation.\n",
        "\n",
        "    Args:\n",
        "        monthly_indicator_data_df: The monthly DataFrame of raw indicators.\n",
        "        evaluation_windows_df: DataFrame defining the valid evaluation windows.\n",
        "\n",
        "    Returns:\n",
        "        A list of string messages describing validation warnings or errors.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # --- Index and Schema Validation ---\n",
        "    if not isinstance(monthly_indicator_data_df.index, pd.DatetimeIndex):\n",
        "        issues.append(\n",
        "            \"Critical Error: `monthly_indicator_data_df` index is not a DatetimeIndex.\"\n",
        "        )\n",
        "        return issues\n",
        "\n",
        "    if monthly_indicator_data_df.index.freqstr != 'MS':\n",
        "        issues.append(\n",
        "            \"Critical Error: `monthly_indicator_data_df` index frequency is not \"\n",
        "            f\"'MS'. Found '{monthly_indicator_data_df.index.freqstr}'.\"\n",
        "        )\n",
        "\n",
        "    expected_cols = ['manufacturing_pmi', 'service_pmi', 'kof_biz_situation']\n",
        "    for col in expected_cols:\n",
        "        if col not in monthly_indicator_data_df.columns:\n",
        "            issues.append(f\"Critical Error: Missing column '{col}' in `monthly_indicator_data_df`.\")\n",
        "        elif monthly_indicator_data_df[col].dtype != 'float64':\n",
        "            issues.append(\n",
        "                f\"Critical Error: Column '{col}' has incorrect dtype. \"\n",
        "                f\"Expected 'float64', found '{monthly_indicator_data_df[col].dtype}'.\"\n",
        "            )\n",
        "    if issues:\n",
        "        return issues\n",
        "\n",
        "    # --- Historical NaN Pattern Validation ---\n",
        "    # Map the monthly column names to their identifiers in the evaluation windows table.\n",
        "    series_map = {\n",
        "        'service_pmi': 'service_pmi',\n",
        "        'kof_biz_situation': 'kof_biz_situation'\n",
        "    }\n",
        "\n",
        "    for m_col, series_name in series_map.items():\n",
        "        # Find the corresponding start quarter from the evaluation windows table.\n",
        "        window_info = evaluation_windows_df[evaluation_windows_df['series_name'] == series_name]\n",
        "        if window_info.empty:\n",
        "            issues.append(f\"Configuration Error: No evaluation window found for '{series_name}'.\")\n",
        "            continue\n",
        "\n",
        "        start_quarter = window_info['start_quarter'].iloc[0]\n",
        "\n",
        "        # Check for any non-NaN data *before* the official start date.\n",
        "        pre_start_data = monthly_indicator_data_df.loc[:start_quarter - pd.DateOffset(days=1), m_col]\n",
        "        if pre_start_data.notna().any():\n",
        "            first_invalid_date = pre_start_data.first_valid_index()\n",
        "            issues.append(\n",
        "                f\"Historical Data Error: Found non-NaN data for '{m_col}' before its \"\n",
        "                f\"official start of {start_quarter.date()}. First invalid data point \"\n",
        "                f\"at {first_invalid_date.date()}.\"\n",
        "            )\n",
        "\n",
        "    return issues\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate release_calendar_df and evaluation_windows_df\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_metadata_tables(\n",
        "    release_calendar_df: pd.DataFrame,\n",
        "    evaluation_windows_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the schema and content of critical metadata tables.\n",
        "\n",
        "    Purpose:\n",
        "    This function performs a deep validation of the two primary metadata tables\n",
        "    that govern temporal aspects of the study: the release calendar and the\n",
        "    evaluation windows. It ensures their structure, content, and temporal\n",
        "    conventions are perfectly aligned with the study's methodology, preventing\n",
        "    subtle errors in forecasting and evaluation.\n",
        "\n",
        "    Inputs:\n",
        "        release_calendar_df (pd.DataFrame): DataFrame with indicator release dates.\n",
        "        evaluation_windows_df (pd.DataFrame): DataFrame with indicator evaluation windows.\n",
        "\n",
        "    Processes:\n",
        "    1.  **`release_calendar_df` Validation:**\n",
        "        a. Checks that 'series_name' values belong to a known vocabulary.\n",
        "        b. Asserts that 'release_datetime_utc' is a timezone-aware UTC datetime.\n",
        "        c. Asserts that the 'period' column contains valid datetimes\n",
        "           and that each period's frequency (Month Start vs. Quarter Start)\n",
        "           matches the known frequency of its corresponding series.\n",
        "    2.  **`evaluation_windows_df` Validation:**\n",
        "        a. Checks for the presence of required columns.\n",
        "        b. Asserts that 'start_quarter' is a valid Quarter Start timestamp.\n",
        "        c. Verifies that the specific start dates for 'service_pmi' and\n",
        "           'kof_biz_situation' exactly match the paper's footnotes.\n",
        "\n",
        "    Outputs:\n",
        "        (List[str]): A list of string messages describing all validation warnings\n",
        "                     or errors found. An empty list signifies success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect all validation issues.\n",
        "    issues: List[str] = []\n",
        "\n",
        "    # --- `release_calendar_df` Validation ---\n",
        "    # Define the universe of expected series names for the release calendar.\n",
        "    expected_series_vocab = {\n",
        "        'manufacturing_pmi', 'service_pmi', 'kof_biz_situation',\n",
        "        'seco_consumer_sentiment', 'yoy_gdp_growth_sports_adj'\n",
        "    }\n",
        "    # Check for any unexpected series names.\n",
        "    if not set(release_calendar_df['series_name'].unique()).issubset(expected_series_vocab):\n",
        "        invalid_series = set(release_calendar_df['series_name'].unique()) - expected_series_vocab\n",
        "        issues.append(f\"Config Error: Invalid series_name in `release_calendar_df`: {invalid_series}\")\n",
        "\n",
        "    # Check that the release timestamp is a timezone-aware UTC datetime.\n",
        "    if not pd.api.types.is_datetime64_any_dtype(release_calendar_df['release_datetime_utc'].dtype) or \\\n",
        "       release_calendar_df['release_datetime_utc'].dt.tz is None or \\\n",
        "       str(release_calendar_df['release_datetime_utc'].dt.tz) != 'UTC':\n",
        "        issues.append(\"Critical Error: `release_calendar_df.release_datetime_utc` must be a timezone-aware UTC datetime.\")\n",
        "\n",
        "    # --- `period` Column Frequency Validation ---\n",
        "    # Define the expected frequency for each time series.\n",
        "    series_freq_map = {\n",
        "        'manufacturing_pmi': 'MS',\n",
        "        'service_pmi': 'MS',\n",
        "        'kof_biz_situation': 'MS',\n",
        "        'seco_consumer_sentiment': 'QS-JAN',\n",
        "        'yoy_gdp_growth_sports_adj': 'QS-JAN'\n",
        "    }\n",
        "    # Ensure the period column is a datetime object for validation.\n",
        "    periods = pd.to_datetime(release_calendar_df['period'], errors='coerce')\n",
        "    if periods.isna().any():\n",
        "        issues.append(\"Critical Error: `release_calendar_df.period` contains values that could not be parsed as dates.\")\n",
        "    else:\n",
        "        # Iterate through each row to check for frequency alignment.\n",
        "        for index, row in release_calendar_df.iterrows():\n",
        "            series_name = row['series_name']\n",
        "            period_ts = periods.loc[index]\n",
        "            expected_freq = series_freq_map.get(series_name)\n",
        "\n",
        "            # Check for month start alignment for monthly series.\n",
        "            if expected_freq == 'MS' and not period_ts.is_month_start:\n",
        "                issues.append(\n",
        "                    f\"Frequency Error in `release_calendar_df` at index {index}: \"\n",
        "                    f\"Series '{series_name}' requires a month-start period, but found {period_ts.date()}.\"\n",
        "                )\n",
        "            # Check for quarter start alignment for quarterly series.\n",
        "            elif expected_freq == 'QS-JAN' and not period_ts.is_quarter_start:\n",
        "                issues.append(\n",
        "                    f\"Frequency Error in `release_calendar_df` at index {index}: \"\n",
        "                    f\"Series '{series_name}' requires a quarter-start period, but found {period_ts.date()}.\"\n",
        "                )\n",
        "\n",
        "    # --- `evaluation_windows_df` Validation ---\n",
        "    # Check for the presence of essential columns.\n",
        "    if 'series_name' not in evaluation_windows_df.columns or 'start_quarter' not in evaluation_windows_df.columns:\n",
        "        issues.append(\"Critical Error: `evaluation_windows_df` is missing required columns ('series_name', 'start_quarter').\")\n",
        "        # Halt further checks on this df if columns are missing.\n",
        "        return issues\n",
        "\n",
        "    # Check that the start_quarter column has the correct frequency.\n",
        "    if evaluation_windows_df['start_quarter'].dt.freqstr != 'QS-JAN':\n",
        "        issues.append(\"Critical Error: `evaluation_windows_df.start_quarter` must have quarter-start ('QS-JAN') frequency.\")\n",
        "\n",
        "    # Check the specific, hardcoded start dates from the paper's Table 1 footnotes. This is a critical cross-check.\n",
        "    expected_starts = {\n",
        "        'service_pmi': pd.Timestamp('2014-01-01', freq='QS-JAN'),\n",
        "        'kof_biz_situation': pd.Timestamp('2009-04-01', freq='QS-JAN')\n",
        "    }\n",
        "    for series, expected_start in expected_starts.items():\n",
        "        # Select the row for the specific series.\n",
        "        actual_start_series = evaluation_windows_df.loc[\n",
        "            evaluation_windows_df['series_name'] == series, 'start_quarter'\n",
        "        ]\n",
        "        # Check if the series is missing from the configuration table.\n",
        "        if actual_start_series.empty:\n",
        "            issues.append(f\"Config Error: Missing entry for '{series}' in `evaluation_windows_df`.\")\n",
        "        # Check if the start date matches the paper's methodology.\n",
        "        elif actual_start_series.iloc[0] != expected_start:\n",
        "            issues.append(\n",
        "                f\"Config Error: Incorrect start_quarter for '{series}'. \"\n",
        "                f\"Expected {expected_start.date()}, found {actual_start_series.iloc[0].date()}.\"\n",
        "            )\n",
        "\n",
        "    return issues\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_macro_data(\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    monthly_indicator_data_df: pd.DataFrame,\n",
        "    release_calendar_df: pd.DataFrame,\n",
        "    evaluation_windows_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of all macro and indicator DataFrames.\n",
        "\n",
        "    This function provides a single entry point to validate the suite of\n",
        "    macroeconomic data tables, ensuring their structural integrity, temporal\n",
        "    alignment, and consistency with the study's methodological constraints.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_data_df: The primary quarterly data for forecasting.\n",
        "        monthly_indicator_data_df: The source monthly indicator data.\n",
        "        release_calendar_df: Metadata on indicator release timing.\n",
        "        evaluation_windows_df: Metadata on valid evaluation periods.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the validation results:\n",
        "        - 'status': 'Success' if no critical errors were found, else 'Failure'.\n",
        "        - 'issues': A consolidated list of all warning and error messages.\n",
        "    \"\"\"\n",
        "    all_issues = []\n",
        "\n",
        "    # --- Step 1: Validate quarterly macro data ---\n",
        "    q_issues = _validate_quarterly_macro_data(raw_macro_data_df, monthly_indicator_data_df)\n",
        "    all_issues.extend(q_issues)\n",
        "\n",
        "    # --- Step 2: Validate monthly indicator data ---\n",
        "    m_issues = _validate_monthly_indicator_data(monthly_indicator_data_df, evaluation_windows_df)\n",
        "    all_issues.extend(m_issues)\n",
        "\n",
        "    # --- Step 3: Validate metadata tables ---\n",
        "    meta_issues = _validate_metadata_tables(release_calendar_df, evaluation_windows_df)\n",
        "    all_issues.extend(meta_issues)\n",
        "\n",
        "    # Determine final status based on whether any issues were found.\n",
        "    status = \"Failure\" if all_issues else \"Success\"\n",
        "\n",
        "    return {\"status\": status, \"issues\": all_issues}\n"
      ],
      "metadata": {
        "id": "pvkXGVeKQGII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Validate master_config parameters and prompt templates\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Validate master_config parameters and prompt templates\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Validate data curation and timezone parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_curation_params(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the 'data_curation_params' section of the master configuration.\n",
        "\n",
        "    This function performs a battery of precise checks to ensure that the data\n",
        "    curation and scoping parameters match the paper's methodology exactly.\n",
        "\n",
        "    Args:\n",
        "        config: The 'data_curation_params' sub-dictionary from the master config.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string is a detailed description of a\n",
        "        validation issue (error or warning). An empty list indicates success.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Define ground truth values based on the paper's methodology.\n",
        "    expected_values = {\n",
        "        \"start_date\": \"1999-01-01\",\n",
        "        \"end_date\": \"2025-05-31\",\n",
        "        \"included_languages\": [\"de\", \"fr\"],\n",
        "        \"excluded_publication_types\": [\"tv_guide\", \"magazine\", \"radio\", \"TV\"],\n",
        "    }\n",
        "\n",
        "    # Check each parameter against its expected value.\n",
        "    for key, expected in expected_values.items():\n",
        "        actual = config.get(key)\n",
        "        # Use set comparison for lists to be order-agnostic.\n",
        "        if isinstance(expected, list):\n",
        "            if not isinstance(actual, list) or set(actual) != set(expected):\n",
        "                issues.append(\n",
        "                    f\"Config Error in 'data_curation_params.{key}': \"\n",
        "                    f\"Expected {sorted(expected)}, but found {actual}.\"\n",
        "                )\n",
        "        # Use direct comparison for other types.\n",
        "        elif actual != expected:\n",
        "            issues.append(\n",
        "                f\"Config Error in 'data_curation_params.{key}': \"\n",
        "                f\"Expected '{expected}', but found '{actual}'.\"\n",
        "            )\n",
        "\n",
        "    # Validate the nested timezone policy.\n",
        "    tz_policy = config.get('timezone_policy', {})\n",
        "    if tz_policy.get('store_timezone_aware') is not True:\n",
        "        issues.append(\n",
        "            \"Config Error in 'data_curation_params.timezone_policy.store_timezone_aware': \"\n",
        "            \"Expected True.\"\n",
        "        )\n",
        "    if tz_policy.get('primary_tz') != 'UTC':\n",
        "        issues.append(\n",
        "            \"Config Error in 'data_curation_params.timezone_policy.primary_tz': \"\n",
        "            \"Expected 'UTC'.\"\n",
        "        )\n",
        "\n",
        "    # Issue a warning if the outlet allowlist is empty, as it's meant to be populated.\n",
        "    if not config.get('outlet_allowlist'):\n",
        "        issues.append(\n",
        "            \"Config Warning in 'data_curation_params.outlet_allowlist': \"\n",
        "            \"List is empty. To replicate the paper's scope of 158 outlets, \"\n",
        "            \"this list must be populated.\"\n",
        "        )\n",
        "\n",
        "    return issues\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Validate embedding and relevance model parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_embedding_relevance_params(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the 'embedding_params' and 'relevance_model_params' sections.\n",
        "\n",
        "    This function ensures that the parameters defining the feature engineering\n",
        "    and relevance classification stages are correctly specified.\n",
        "\n",
        "    Args:\n",
        "        config: A dictionary containing 'embedding_params' and\n",
        "                'relevance_model_params'.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings describing validation issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # --- Embedding Parameters Validation ---\n",
        "    emb_params = config.get('embedding_params', {})\n",
        "    expected_emb = {\n",
        "        \"model_name\": \"jina-embeddings-v3\",\n",
        "        \"embedding_dimension\": 1024,\n",
        "        \"max_input_tokens\": 8192,\n",
        "        \"inference_mode\": \"local\",\n",
        "    }\n",
        "    for key, expected in expected_emb.items():\n",
        "        if emb_params.get(key) != expected:\n",
        "            issues.append(\n",
        "                f\"Config Error in 'embedding_params.{key}': \"\n",
        "                f\"Expected '{expected}', found '{emb_params.get(key)}'.\"\n",
        "            )\n",
        "    if not emb_params.get('model_hash'):\n",
        "        issues.append(\n",
        "            \"Config Warning in 'embedding_params.model_hash': Is empty. \"\n",
        "            \"Must be filled before execution for reproducibility.\"\n",
        "        )\n",
        "\n",
        "    # --- Relevance Model Parameters Validation ---\n",
        "    rel_params = config.get('relevance_model_params', {})\n",
        "    expected_sections = {\n",
        "        \"Business\", \"Markets\", \"Economics\", \"Finance\", \"Wirtschaft\", \"Börse\",\n",
        "        \"Ökonomie\", \"Finanz\", \"Économie\", \"Marchés\", \"Affaires\", \"Finance\"\n",
        "    }\n",
        "    if set(rel_params.get('positive_class_sections', [])) != expected_sections:\n",
        "        issues.append(\"Config Error: 'relevance_model_params.positive_class_sections' does not match the required multilingual set.\")\n",
        "\n",
        "    # Deeply validate the neural network architecture.\n",
        "    nn_arch = rel_params.get('nn_architecture', {})\n",
        "    if nn_arch.get('input_dim') != 1024:\n",
        "        issues.append(\"Config Error: 'nn_architecture.input_dim' must be 1024.\")\n",
        "\n",
        "    expected_layers = [\n",
        "        {\"type\": \"Dense\", \"neurons\": 256, \"activation\": \"ReLU\"},\n",
        "        {\"type\": \"Dropout\", \"rate\": 0.3},\n",
        "        {\"type\": \"Dense\", \"neurons\": 64, \"activation\": \"ReLU\"},\n",
        "        {\"type\": \"Dense\", \"neurons\": 1, \"activation\": \"Sigmoid\"}\n",
        "    ]\n",
        "    actual_layers = nn_arch.get('layers', [])\n",
        "    if len(actual_layers) != len(expected_layers) or actual_layers != expected_layers:\n",
        "         issues.append(\"Config Error: 'nn_architecture.layers' does not match the specified structure.\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Validate sentiment model, aggregation, and econometric parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_sentiment_econometric_params(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates sentiment, aggregation, and econometric sections of the config.\n",
        "\n",
        "    This function validates the core methodological choices for the sentiment\n",
        "    model and the final econometric evaluation, ensuring alignment with the paper.\n",
        "\n",
        "    Args:\n",
        "        config: A dictionary containing the relevant parameter sections.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings describing validation issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # --- Sentiment Model Validation ---\n",
        "    sent_params = config.get('sentiment_model_params', {})\n",
        "    llm_config = sent_params.get('llm_config', {})\n",
        "    if llm_config.get('model_identifier') != 'claude-3-5-sonnet-20240620':\n",
        "        issues.append(\"Config Error: 'llm_config.model_identifier' is incorrect.\")\n",
        "\n",
        "    # Check for key phrases in the prompt to ensure its integrity.\n",
        "    prompt_text = llm_config.get('appendix_prompt_financial_markets_en', '')\n",
        "    required_phrases = [\n",
        "        \"400-500 words long\", \"NZZ, Tagesanzeiger, Handelszeitung\",\n",
        "        \"embedding-friendly wording\", \"Clear polarization\"\n",
        "    ]\n",
        "    for phrase in required_phrases:\n",
        "        if phrase not in prompt_text:\n",
        "            issues.append(f\"Config Error: Key phrase '{phrase}' missing from LLM prompt.\")\n",
        "\n",
        "    # --- Classifier Config Validation ---\n",
        "    class_config = sent_params.get('classifier_config', {})\n",
        "    if class_config.get('model_type') != 'LogisticRegression' or \\\n",
        "       class_config.get('penalty') != 'l2' or \\\n",
        "       class_config.get('feature_scaling') != 'none':\n",
        "        issues.append(\"Config Error: 'classifier_config' deviates from L2-regularized logistic regression on raw embeddings.\")\n",
        "\n",
        "    # --- Econometric Validation ---\n",
        "    econ_params = config.get('econometric_validation_params', {})\n",
        "    if econ_params.get('dependent_variable') != 'yoy_gdp_growth_sports_adj':\n",
        "        issues.append(\"Config Error: 'dependent_variable' is incorrect.\")\n",
        "    if set(econ_params.get('forecast_horizons_quarters', [])) != {0, 1, 2}:\n",
        "        issues.append(\"Config Error: 'forecast_horizons_quarters' must be [0, 1, 2].\")\n",
        "\n",
        "    info_policy = econ_params.get('information_set_policy', {}).get('use_month_m_for_quarter_t', {})\n",
        "    if info_policy.get('baseline') != 2 or info_policy.get('early_neos') != 3:\n",
        "        issues.append(\"Config Error: 'information_set_policy' for month selection is incorrect.\")\n",
        "\n",
        "    dm_test_config = econ_params.get('significance_test_config', {})\n",
        "    if dm_test_config.get('test_name') != 'Diebold-Mariano' or \\\n",
        "       dm_test_config.get('modification') != 'HAC standard errors':\n",
        "        issues.append(\"Config Error: 'significance_test_config' is incorrect.\")\n",
        "    if dm_test_config.get('bandwidth_q') is not None:\n",
        "        issues.append(\n",
        "            \"Config Warning: 'bandwidth_q' is pre-set. It should be None to allow \"\n",
        "            \"for rule-of-thumb calculation during the test.\"\n",
        "        )\n",
        "\n",
        "    eval_windows = econ_params.get('evaluation_windows_by_indicator', {})\n",
        "    if eval_windows.get('service_pmi', {}).get('start_quarter') != '2014-01-01' or \\\n",
        "       eval_windows.get('kof_biz_situation', {}).get('start_quarter') != '2009-04-01':\n",
        "        issues.append(\"Config Error: 'evaluation_windows_by_indicator' start dates do not match paper footnotes.\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_master_config(\n",
        "    fused_master_input_specification: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the entire master configuration dictionary.\n",
        "\n",
        "    This function serves as a single pre-flight check for all study parameters,\n",
        "    ensuring that the configuration aligns perfectly with the paper's\n",
        "    methodology before any computation begins. It aggregates issues from all\n",
        "    parameter sections into a single, comprehensive report.\n",
        "\n",
        "    Args:\n",
        "        fused_master_input_specification: The master configuration dictionary\n",
        "            containing all study parameters.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the validation results:\n",
        "        - 'status': 'Success' if no errors were found, else 'Failure'.\n",
        "        - 'issues': A consolidated list of all warning and error messages.\n",
        "    \"\"\"\n",
        "    # Extract the master config dictionary to be validated.\n",
        "    master_config = fused_master_input_specification.get('master_config', {})\n",
        "    if not master_config:\n",
        "        return {\"status\": \"Failure\", \"issues\": [\"Critical Error: 'master_config' key not found in input.\"]}\n",
        "\n",
        "    # --- Execute validation for each parameter section ---\n",
        "    # Step 1: Curation and Timezone Parameters\n",
        "    curation_issues = _validate_curation_params(\n",
        "        master_config.get('data_curation_params', {})\n",
        "    )\n",
        "\n",
        "    # Step 2: Embedding and Relevance Model Parameters\n",
        "    embedding_relevance_issues = _validate_embedding_relevance_params(\n",
        "        {\n",
        "            'embedding_params': master_config.get('embedding_params', {}),\n",
        "            'relevance_model_params': master_config.get('relevance_model_params', {})\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Step 3: Sentiment, Aggregation, and Econometric Parameters\n",
        "    sentiment_econometric_issues = _validate_sentiment_econometric_params(\n",
        "        {\n",
        "            'sentiment_model_params': master_config.get('sentiment_model_params', {}),\n",
        "            'aggregation_params': master_config.get('aggregation_params', {}),\n",
        "            'econometric_validation_params': master_config.get('econometric_validation_params', {})\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Consolidate all issues found across the different sections.\n",
        "    all_issues = (\n",
        "        curation_issues +\n",
        "        embedding_relevance_issues +\n",
        "        sentiment_econometric_issues\n",
        "    )\n",
        "\n",
        "    # Determine the final status. Any issue containing \"Error\" is critical.\n",
        "    is_failure = any(\"Error\" in issue for issue in all_issues)\n",
        "    status = \"Failure\" if is_failure else \"Success\"\n",
        "\n",
        "    # Return the final, structured validation report.\n",
        "    return {\"status\": status, \"issues\": all_issues}\n"
      ],
      "metadata": {
        "id": "zXBLMzzERJNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Cleanse the news corpus by enforcing scope, language, and publication-type filters\n",
        "\n",
        "# ===========================================================================================\n",
        "# Task 4: Cleanse the news corpus by enforcing scope, language, and publication-type filters\n",
        "# ===========================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Apply temporal, language, and publication-type filters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_scope_filters(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    audit_log: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies deterministic filters for time, language, and publication type.\n",
        "\n",
        "    This function sequentially filters the input DataFrame based on the rules\n",
        "    defined in the configuration, creating a complete audit trail of the process.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame to be filtered.\n",
        "        config: The 'data_curation_params' sub-dictionary.\n",
        "        audit_log: A dictionary to which filter counts will be added.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame containing only the records that pass all filters.\n",
        "    \"\"\"\n",
        "    # Record the initial number of articles for the audit trail.\n",
        "    initial_count = len(df)\n",
        "    audit_log['initial_article_count'] = initial_count\n",
        "\n",
        "    # --- 1. Temporal Filter ---\n",
        "    # Convert date strings from config to timezone-aware UTC timestamps.\n",
        "    start_date = pd.to_datetime(config['start_date'], utc=True)\n",
        "    end_date = pd.to_datetime(config['end_date'], utc=True)\n",
        "    # Create a boolean mask for articles within the specified date range.\n",
        "    temporal_mask = df['publication_datetime_utc'].between(\n",
        "        start_date, end_date, inclusive='both'\n",
        "    )\n",
        "    # Apply the filter.\n",
        "    df = df[temporal_mask]\n",
        "    # Log the result of this filter stage.\n",
        "    count_after_temporal = len(df)\n",
        "    audit_log['filter_temporal'] = {\n",
        "        'retained': count_after_temporal,\n",
        "        'removed': initial_count - count_after_temporal\n",
        "    }\n",
        "\n",
        "    # --- 2. Language Filter ---\n",
        "    # Create a boolean mask for articles in the allowed languages.\n",
        "    language_mask = df['language'].isin(config['included_languages'])\n",
        "    # Apply the filter.\n",
        "    df = df[language_mask]\n",
        "    # Log the result.\n",
        "    count_after_language = len(df)\n",
        "    audit_log['filter_language'] = {\n",
        "        'retained': count_after_language,\n",
        "        'removed': count_after_temporal - count_after_language\n",
        "    }\n",
        "\n",
        "    # --- 3. Publication Type Filter ---\n",
        "    # Create a boolean mask to EXCLUDE specified publication types.\n",
        "    pub_type_mask = ~df['publication_type'].isin(config['excluded_publication_types'])\n",
        "    # Apply the filter.\n",
        "    df = df[pub_type_mask]\n",
        "    # Log the result.\n",
        "    count_after_pub_type = len(df)\n",
        "    audit_log['filter_publication_type'] = {\n",
        "        'retained': count_after_pub_type,\n",
        "        'removed': count_after_language - count_after_pub_type\n",
        "    }\n",
        "\n",
        "    # --- 4. Outlet Allowlist Filter (Conditional) ---\n",
        "    # This filter only runs if the allowlist in the config is populated.\n",
        "    if config.get('outlet_allowlist'):\n",
        "        # Create a boolean mask for articles from allowed outlets.\n",
        "        outlet_mask = df['outlet_id'].isin(config['outlet_allowlist'])\n",
        "        # Apply the filter.\n",
        "        df = df[outlet_mask]\n",
        "        # Log the result.\n",
        "        count_after_outlet = len(df)\n",
        "        audit_log['filter_outlet_allowlist'] = {\n",
        "            'retained': count_after_outlet,\n",
        "            'removed': count_after_pub_type - count_after_outlet\n",
        "        }\n",
        "    else:\n",
        "        # If the list is empty, log that the filter was skipped.\n",
        "        audit_log['filter_outlet_allowlist'] = {'status': 'skipped', 'retained': len(df)}\n",
        "\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Enforce non-null constraints and remove invalid records\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _enforce_non_null_constraints(\n",
        "    df: pd.DataFrame,\n",
        "    audit_log: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes records with null or empty critical fields.\n",
        "\n",
        "    This function drops rows where 'publication_datetime_utc' is null, or where\n",
        "    'full_text' is null or contains only whitespace.\n",
        "\n",
        "    Args:\n",
        "        df: The filtered DataFrame.\n",
        "        audit_log: The audit log to be updated.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with invalid records removed.\n",
        "    \"\"\"\n",
        "    # Record the count before this cleansing step.\n",
        "    count_before = len(df)\n",
        "\n",
        "    # --- 1. Drop rows with null critical timestamps ---\n",
        "    df.dropna(subset=['publication_datetime_utc'], inplace=True)\n",
        "\n",
        "    # --- 2. Drop rows with null or whitespace-only text ---\n",
        "    # Create a mask for rows where 'full_text' is null.\n",
        "    null_text_mask = df['full_text'].isna()\n",
        "    # Create a mask for rows where 'full_text' is just whitespace.\n",
        "    whitespace_text_mask = df['full_text'].str.strip().eq('')\n",
        "    # Combine the masks to identify all rows to be removed.\n",
        "    invalid_text_mask = null_text_mask | whitespace_text_mask\n",
        "\n",
        "    # Apply the filter to keep only valid rows.\n",
        "    df = df[~invalid_text_mask]\n",
        "\n",
        "    # Log the number of records removed.\n",
        "    count_after = len(df)\n",
        "    audit_log['cleanse_non_null'] = {\n",
        "        'retained': count_after,\n",
        "        'removed': count_before - count_after\n",
        "    }\n",
        "\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Handle duplicates with deterministic policy\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_fingerprint(\n",
        "    text_series: pd.Series\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes a deterministic SHA-256 fingerprint for each text entry in a Series.\n",
        "\n",
        "    Purpose:\n",
        "    This function is designed to create a unique and consistent identifier for\n",
        "    unstructured text content. It first applies a strict normalization routine\n",
        "    to eliminate superficial variations (e.g., case, whitespace) and then\n",
        "    computes a cryptographic hash (SHA-256) of the normalized text. This\n",
        "    fingerprint is essential for accurately identifying and removing duplicate\n",
        "    articles from the corpus.\n",
        "\n",
        "    Inputs:\n",
        "        text_series (pd.Series): A pandas Series containing the raw, unstructured\n",
        "                                 text data (e.g., the 'full_text' column of the\n",
        "                                 news DataFrame). The Series is expected to have\n",
        "                                 a string dtype.\n",
        "\n",
        "    Processes:\n",
        "    1.  Input Validation: Checks if the input is a pandas Series.\n",
        "    2.  Normalization:\n",
        "        a. Converts all text to lowercase to ensure case-insensitivity.\n",
        "        b. Collapses all sequences of one or more whitespace characters\n",
        "           (including tabs, newlines) into a single space.\n",
        "        c. Strips any leading or trailing whitespace from the text.\n",
        "    3.  Hashing:\n",
        "        a. Iterates through each normalized text entry.\n",
        "        b. If the entry is not null (pd.notna), it encodes the string to\n",
        "           UTF-8 bytes.\n",
        "        c. Computes the SHA-256 hash of the byte string.\n",
        "        d. Returns the hexadecimal representation of the hash.\n",
        "        e. If the entry is null, it propagates the null value (NaN/None).\n",
        "\n",
        "    Outputs:\n",
        "        (pd.Series): A new pandas Series of the same index as the input, where\n",
        "                     each element is the computed SHA-256 hexadecimal digest\n",
        "                     (string) corresponding to the text in the input Series, or\n",
        "                     None if the input text was null.\n",
        "\n",
        "    Error Handling:\n",
        "        - Raises a TypeError if the input is not a pandas Series.\n",
        "        - The underlying pandas string methods and the apply method handle\n",
        "          element-wise operations gracefully, propagating nulls where appropriate.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input is a pandas Series to support vectorized string operations.\n",
        "    if not isinstance(text_series, pd.Series):\n",
        "        raise TypeError(\"Input `text_series` must be a pandas Series.\")\n",
        "\n",
        "    # --- Text Normalization ---\n",
        "    # This sequence of vectorized operations creates a canonical string representation.\n",
        "    # 1. Convert to lowercase for case-insensitivity.\n",
        "    # 2. Replace any sequence of one or more whitespace characters with a single space.\n",
        "    # 3. Remove any leading or trailing whitespace.\n",
        "    normalized_text: pd.Series = (\n",
        "        text_series.str.lower()\n",
        "        .str.replace(r'\\s+', ' ', regex=True)\n",
        "        .str.strip()\n",
        "    )\n",
        "\n",
        "    # --- Hashing ---\n",
        "    # Define a helper function for hashing to be used with .apply().\n",
        "    def hash_text(text: Optional[str]) -> Optional[str]:\n",
        "        \"\"\"Encodes and hashes a single string, handling nulls.\"\"\"\n",
        "        # Check if the text is valid (not None, NaN, etc.).\n",
        "        if pd.notna(text):\n",
        "            # Encode the normalized string into a UTF-8 byte sequence.\n",
        "            encoded_text = text.encode('utf-8')\n",
        "            # Compute the SHA-256 hash of the byte sequence.\n",
        "            hasher = hashlib.sha256(encoded_text)\n",
        "            # Return the 64-character hexadecimal representation of the hash.\n",
        "            return hasher.hexdigest()\n",
        "        # If the input text is null, return None to propagate the null.\n",
        "        return None\n",
        "\n",
        "    # Apply the hashing function to each element in the normalized Series.\n",
        "    # This produces the final Series of content fingerprints.\n",
        "    fingerprint_series: pd.Series = normalized_text.apply(hash_text)\n",
        "\n",
        "    return fingerprint_series\n",
        "\n",
        "def _handle_duplicates(\n",
        "    df: pd.DataFrame,\n",
        "    audit_log: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes duplicate articles based on a content fingerprint.\n",
        "\n",
        "    It uses the 'dedupe_fingerprint' column if available, otherwise it computes\n",
        "    one on the fly. Duplicates are resolved by keeping the earliest ingested article.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame to be deduplicated.\n",
        "        audit_log: The audit log to be updated.\n",
        "\n",
        "    Returns:\n",
        "        A new, deduplicated DataFrame.\n",
        "    \"\"\"\n",
        "    # Record the count before deduplication.\n",
        "    count_before = len(df)\n",
        "\n",
        "    # Determine the column to use for deduplication.\n",
        "    fingerprint_col = 'dedupe_fingerprint'\n",
        "    if fingerprint_col not in df.columns:\n",
        "        # If the fingerprint column doesn't exist, compute it.\n",
        "        audit_log['deduplication_fingerprint_status'] = 'computed_on_the_fly'\n",
        "        df[fingerprint_col] = _compute_fingerprint(df['full_text'])\n",
        "    else:\n",
        "        audit_log['deduplication_fingerprint_status'] = 'used_existing_column'\n",
        "\n",
        "    # Deduplicate:\n",
        "    # 1. Sort by ingestion timestamp to ensure we keep the *first* record seen.\n",
        "    # 2. Drop duplicates based on the fingerprint, keeping the 'first' entry.\n",
        "    df_deduplicated = df.sort_values('ingestion_timestamp_utc').drop_duplicates(\n",
        "        subset=[fingerprint_col], keep='first'\n",
        "    )\n",
        "\n",
        "    # Log the number of duplicates removed.\n",
        "    count_after = len(df_deduplicated)\n",
        "    audit_log['cleanse_duplicates'] = {\n",
        "        'retained': count_after,\n",
        "        'removed': count_before - count_after\n",
        "    }\n",
        "\n",
        "    # If the fingerprint was computed on the fly, drop the temporary column.\n",
        "    if audit_log['deduplication_fingerprint_status'] == 'computed_on_the_fly':\n",
        "        df_deduplicated = df_deduplicated.drop(columns=[fingerprint_col])\n",
        "\n",
        "    return df_deduplicated\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_news_corpus(\n",
        "    raw_news_data_df: pd.DataFrame,\n",
        "    fused_master_input_specification: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end cleansing of the raw news corpus.\n",
        "\n",
        "    This function executes a sequence of filtering and cleansing steps to\n",
        "    prepare the raw news data for analysis, strictly following the methodology\n",
        "    defined in the configuration. It produces a cleansed DataFrame and a\n",
        "    detailed audit log of all operations.\n",
        "\n",
        "    Args:\n",
        "        raw_news_data_df: The raw news corpus as a pandas DataFrame.\n",
        "        fused_master_input_specification: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A cleansed pandas DataFrame, ready for the embedding stage.\n",
        "        - A dictionary serving as a detailed audit log of the cleansing process.\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original DataFrame.\n",
        "    df = raw_news_data_df.copy()\n",
        "\n",
        "    # Initialize the audit log.\n",
        "    audit_log = {}\n",
        "\n",
        "    # Extract the relevant configuration section.\n",
        "    curation_params = fused_master_input_specification['master_config']['data_curation_params']\n",
        "\n",
        "    # --- Step 1: Apply scope filters ---\n",
        "    df_filtered = _apply_scope_filters(df, curation_params, audit_log)\n",
        "\n",
        "    # --- Step 2: Enforce non-null constraints ---\n",
        "    df_non_null = _enforce_non_null_constraints(df_filtered, audit_log)\n",
        "\n",
        "    # --- Step 3: Handle duplicates ---\n",
        "    df_clean = _handle_duplicates(df_non_null, audit_log)\n",
        "\n",
        "    # Add final count to the audit log.\n",
        "    audit_log['final_article_count'] = len(df_clean)\n",
        "\n",
        "    # Return the final cleansed DataFrame and the complete audit log.\n",
        "    return df_clean, audit_log\n"
      ],
      "metadata": {
        "id": "ip50J0bjR8CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Prepare temporal metadata and produce corpus audit manifest\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Prepare temporal metadata and produce corpus audit manifest\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Standardize all timestamps to UTC and verify timezone awareness\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _assert_timestamp_integrity(\n",
        "    df: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Asserts the integrity of all timestamp columns in the DataFrame.\n",
        "\n",
        "    Purpose:\n",
        "    This function serves as a final, critical guardrail after all cleansing\n",
        "    operations. It verifies that all specified timestamp columns are of a\n",
        "    datetime dtype and are explicitly localized to UTC. Any deviation from this\n",
        "    standard would compromise all subsequent time-based operations and is\n",
        "    treated as a fatal error.\n",
        "\n",
        "    Inputs:\n",
        "        df (pd.DataFrame): The cleansed news corpus DataFrame.\n",
        "\n",
        "    Processes:\n",
        "    1.  Defines a list of timestamp columns to be checked.\n",
        "    2.  Iterates through each column.\n",
        "    3.  Asserts that the column's dtype is a pandas datetime type.\n",
        "    4.  Asserts that the column's timezone attribute (`.dt.tz`) is not None.\n",
        "    5.  Asserts that the timezone is unequivocally 'UTC'.\n",
        "\n",
        "    Outputs:\n",
        "        None: The function returns None if all assertions pass.\n",
        "\n",
        "    Error Handling:\n",
        "        Raises ValueError: If any timestamp column fails any of the integrity\n",
        "                           checks, indicating a critical flaw in the upstream\n",
        "                           data processing pipeline.\n",
        "    \"\"\"\n",
        "    # Define the list of columns that must be timezone-aware UTC datetimes.\n",
        "    timestamp_columns = [\n",
        "        'publication_datetime_utc',\n",
        "        'ingestion_timestamp_utc',\n",
        "        'last_modified_timestamp_utc'\n",
        "    ]\n",
        "\n",
        "    # Iterate through each required timestamp column for validation.\n",
        "    for col in timestamp_columns:\n",
        "        # Check 1: Ensure the column exists and has a datetime-like dtype.\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df[col].dtype):\n",
        "            raise ValueError(\n",
        "                f\"Timestamp Integrity Error: Column '{col}' is not a datetime \"\n",
        "                f\"dtype. Found '{df[col].dtype}'.\"\n",
        "            )\n",
        "\n",
        "        # Check 2: Ensure the datetime column is timezone-aware.\n",
        "        if df[col].dt.tz is None:\n",
        "            raise ValueError(\n",
        "                f\"Timestamp Integrity Error: Column '{col}' is timezone-naive. \"\n",
        "                \"All timestamps must be timezone-aware.\"\n",
        "            )\n",
        "\n",
        "        # Check 3: Ensure the timezone is specifically UTC.\n",
        "        if str(df[col].dt.tz) != 'UTC':\n",
        "            raise ValueError(\n",
        "                f\"Timestamp Integrity Error: Column '{col}' is not in UTC. \"\n",
        "                f\"Found timezone '{df[col].dt.tz}'.\"\n",
        "            )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Compute temporal partitions for early-release windows\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _add_temporal_partitions(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Enriches the DataFrame with temporal features for aggregation.\n",
        "\n",
        "    Purpose:\n",
        "    This function adds several derived columns based on the authoritative\n",
        "    'publication_datetime_utc' timestamp. These new columns are essential for\n",
        "    performing efficient monthly aggregations and for constructing the\n",
        "    early-release variants of the NEOS indicator. The use of vectorized\n",
        "    pandas operations ensures high performance.\n",
        "\n",
        "    Inputs:\n",
        "        df (pd.DataFrame): The cleansed and timestamp-verified DataFrame.\n",
        "\n",
        "    Processes:\n",
        "    1.  Creates 'year_month' (e.g., '2023-04') for monthly grouping.\n",
        "    2.  Creates 'day_of_month' (integer 1-31) for intra-month filtering.\n",
        "    3.  Creates boolean flags for articles published within the first 7, 14,\n",
        "        and 21 days of the month, which are required for the early-release\n",
        "        NEOS variants.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A new DataFrame with the added temporal columns. The\n",
        "                        original DataFrame is not modified.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame in place.\n",
        "    df_enriched = df.copy()\n",
        "\n",
        "    # --- Create 'year_month' column for efficient monthly grouping ---\n",
        "    # The format 'YYYY-MM' is standard and sorts correctly as a string.\n",
        "    df_enriched['year_month'] = df_enriched['publication_datetime_utc'].dt.strftime('%Y-%m')\n",
        "\n",
        "    # --- Create 'day_of_month' column for intra-month windowing ---\n",
        "    # Extracts the day as an integer from the timestamp.\n",
        "    df_enriched['day_of_month'] = df_enriched['publication_datetime_utc'].dt.day\n",
        "\n",
        "    # --- Create boolean flags for early-release windows ---\n",
        "    # These flags enable fast filtering for indicator construction later.\n",
        "    # Flag for articles published on or before the 7th day of the month.\n",
        "    df_enriched['is_in_first_7_days'] = df_enriched['day_of_month'] <= 7\n",
        "    # Flag for articles published on or before the 14th day of the month.\n",
        "    df_enriched['is_in_first_14_days'] = df_enriched['day_of_month'] <= 14\n",
        "    # Flag for articles published on or before the 21st day of the month.\n",
        "    df_enriched['is_in_first_21_days'] = df_enriched['day_of_month'] <= 21\n",
        "\n",
        "    return df_enriched\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Generate corpus audit manifest\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_corpus_audit_manifest(\n",
        "    df_clean: pd.DataFrame,\n",
        "    df_raw: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generates a comprehensive audit manifest of the cleansed corpus.\n",
        "\n",
        "    Purpose:\n",
        "    This function produces a structured dictionary containing key summary\n",
        "    statistics and distributions of the final, cleansed corpus. This manifest\n",
        "    is a critical artifact for reproducibility, data quality assessment, and\n",
        "    understanding the characteristics of the data entering the modeling pipeline.\n",
        "\n",
        "    Inputs:\n",
        "        df_clean (pd.DataFrame): The final, cleansed, and temporally-enriched DataFrame.\n",
        "        df_raw (pd.DataFrame): The original, pre-filter DataFrame for comparison.\n",
        "\n",
        "    Processes:\n",
        "    1.  Calculates total article counts before and after cleansing.\n",
        "    2.  Computes breakdowns of the clean corpus by language and outlet.\n",
        "    3.  Generates time-series DataFrames of monthly article counts, both total\n",
        "        and by language.\n",
        "    4.  Calculates 'section_label' completeness ratios per outlet and over time.\n",
        "\n",
        "    Outputs:\n",
        "        (Dict[str, Any]): A dictionary where keys are descriptive names of\n",
        "                          audits and values are pandas DataFrames or scalars\n",
        "                          containing the summary statistics.\n",
        "    \"\"\"\n",
        "    # Initialize the manifest dictionary.\n",
        "    manifest = {}\n",
        "\n",
        "    # --- Overall Counts ---\n",
        "    # Record the number of articles before and after the entire cleansing process.\n",
        "    manifest['total_articles_pre_filter'] = len(df_raw)\n",
        "    manifest['total_articles_post_filter'] = len(df_clean)\n",
        "\n",
        "    # --- Corpus Composition Breakdowns ---\n",
        "    # Breakdown by language.\n",
        "    manifest['articles_by_language'] = df_clean['language'].value_counts().to_frame('count')\n",
        "    # Breakdown by outlet.\n",
        "    manifest['articles_by_outlet'] = df_clean['outlet_id'].value_counts().to_frame('count')\n",
        "\n",
        "    # --- Temporal Distributions ---\n",
        "    # Total monthly article counts over time.\n",
        "    manifest['monthly_article_counts_total'] = (\n",
        "        df_clean.groupby('year_month')['article_id']\n",
        "        .count()\n",
        "        .to_frame('count')\n",
        "        .sort_index()\n",
        "    )\n",
        "    # Monthly article counts broken down by language.\n",
        "    manifest['monthly_article_counts_by_language'] = (\n",
        "        df_clean.groupby(['year_month', 'language'])['article_id']\n",
        "        .count()\n",
        "        .unstack()\n",
        "        .sort_index()\n",
        "        .fillna(0)\n",
        "        .astype(int)\n",
        "    )\n",
        "\n",
        "    # --- Data Quality Metrics ---\n",
        "    # 'section_label' completeness ratio for each outlet.\n",
        "    manifest['section_label_completeness_by_outlet'] = (\n",
        "        df_clean.groupby('outlet_id')['section_label']\n",
        "        .apply(lambda s: s.notna().mean())\n",
        "        .to_frame('completeness_ratio')\n",
        "        .sort_values('completeness_ratio', ascending=False)\n",
        "    )\n",
        "    # 'section_label' completeness ratio over time.\n",
        "    manifest['section_label_completeness_monthly'] = (\n",
        "        df_clean.groupby('year_month')['section_label']\n",
        "        .apply(lambda s: s.notna().mean())\n",
        "        .to_frame('completeness_ratio')\n",
        "        .sort_index()\n",
        "    )\n",
        "\n",
        "    return manifest\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_corpus_for_embedding(\n",
        "    clean_df: pd.DataFrame,\n",
        "    raw_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the final preparation of the corpus before embedding.\n",
        "\n",
        "    This function serves as the final step in the data preparation pipeline. It\n",
        "    first performs a critical, final validation of timestamp integrity. It then\n",
        "    enriches the data with temporal features required for downstream analysis\n",
        "    and finally produces a comprehensive audit manifest documenting the state\n",
        "    of the prepared corpus.\n",
        "\n",
        "    Args:\n",
        "        clean_df (pd.DataFrame): The cleansed news corpus DataFrame from Task 4.\n",
        "        raw_df (pd.DataFrame): The original, raw news corpus DataFrame, required\n",
        "                               for comparative statistics in the audit manifest.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): The fully prepared DataFrame, enriched with temporal\n",
        "          partitions and ready for the embedding process.\n",
        "        - (Dict[str, Any]): The corpus audit manifest, a dictionary containing\n",
        "          key summary statistics as DataFrames and scalars.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the timestamp integrity check fails.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Final Timestamp Integrity Assertion ---\n",
        "    # This is a critical guardrail; if it fails, the pipeline stops.\n",
        "    _assert_timestamp_integrity(clean_df)\n",
        "\n",
        "    # --- Step 2: Add Temporal Partitions ---\n",
        "    # Enrich the DataFrame with necessary date/time components.\n",
        "    df_enriched = _add_temporal_partitions(clean_df)\n",
        "\n",
        "    # --- Step 3: Generate the Corpus Audit Manifest ---\n",
        "    # Create the final summary document of the prepared data.\n",
        "    audit_manifest = _generate_corpus_audit_manifest(df_enriched, raw_df)\n",
        "\n",
        "    # Return the enriched data and its corresponding audit manifest.\n",
        "    return df_enriched, audit_manifest\n"
      ],
      "metadata": {
        "id": "hB3omxYzTNax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — Generate document embeddings with jina-embeddings-v3 and compute token statistics\n",
        "\n",
        "# ==========================================================================================\n",
        "# Task 6: Generate document embeddings with jina-embeddings-v3 and compute token statistics\n",
        "# ==========================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Steps 1 & 2: Tokenize, Embed, and Store with Provenance\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_embedding_inference(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> Tuple[pd.DataFrame, str, str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Runs the end-to-end document embedding pipeline.\n",
        "\n",
        "    Purpose:\n",
        "    This function is the computational core of the feature engineering stage. It\n",
        "    loads the specified Transformer model, processes the text corpus in batches\n",
        "    to manage memory, generates document embeddings, and computes token-level\n",
        "    statistics. The outputs are saved to disk in an efficient format, and a\n",
        "    detailed provenance log is created.\n",
        "\n",
        "    Inputs:\n",
        "        df (pd.DataFrame): The prepared DataFrame containing the 'full_text' and\n",
        "                           'article_id' columns.\n",
        "        config (Dict[str, Any]): The 'embedding_params' sub-dictionary from the\n",
        "                                 master configuration.\n",
        "        output_dir (str): Path to the directory where artifacts will be saved.\n",
        "\n",
        "    Processes:\n",
        "    1.  Sets up the device (GPU if available, else CPU).\n",
        "    2.  Loads the SentenceTransformer model specified in the config.\n",
        "    3.  Creates a detailed provenance dictionary with model and tokenizer info.\n",
        "    4.  Initializes an HDF5 file for out-of-core storage of embeddings.\n",
        "    5.  Iterates through the input DataFrame in batches:\n",
        "        a. Extracts a batch of texts.\n",
        "        b. Uses the model's tokenizer to get token counts and applies the\n",
        "           truncation policy.\n",
        "        c. Uses the model to encode the texts into 1024-dim vectors.\n",
        "        d. Appends the computed embeddings to the HDF5 file.\n",
        "        e. Collects token statistics and article IDs for the crosswalk table.\n",
        "    6.  Saves the crosswalk table (article_id -> embedding_row_index) and\n",
        "        updates the input DataFrame with token statistics.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): The input DataFrame augmented with 'token_count' and\n",
        "          'truncation_flag' columns.\n",
        "        - (str): The file path to the saved HDF5 embeddings file.\n",
        "        - (str): The file path to the saved crosswalk table (CSV).\n",
        "        - (Dict[str, Any]): A provenance dictionary with model details.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup and Provenance ---\n",
        "    # Determine the computation device (prefer CUDA GPU if available).\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Extract model parameters from the configuration.\n",
        "    model_name = config['model_name']\n",
        "    batch_size = config.get('batch_size', 32) # Default to 32 if not specified\n",
        "    max_tokens = config['max_input_tokens']\n",
        "\n",
        "    # Load the SentenceTransformer model. This will also download it if not cached.\n",
        "    # The model is moved to the selected device.\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "    # The tokenizer is an attribute of the loaded model.\n",
        "    tokenizer = model.tokenizer\n",
        "\n",
        "    # Create a provenance log for reproducibility.\n",
        "    provenance = {\n",
        "        'model_name': model_name,\n",
        "        'model_class': model.__class__.__name__,\n",
        "        'tokenizer_class': tokenizer.__class__.__name__,\n",
        "        'max_input_tokens': max_tokens,\n",
        "        'batch_size': batch_size,\n",
        "        'embedding_dimension': model.get_sentence_embedding_dimension(),\n",
        "    }\n",
        "\n",
        "    # --- 2. Initialize Output Artifacts ---\n",
        "    # Define file paths for the outputs.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    embeddings_path = os.path.join(output_dir, 'document_embeddings.h5')\n",
        "    crosswalk_path = os.path.join(output_dir, 'embeddings_crosswalk.csv')\n",
        "\n",
        "    # Initialize the HDF5 file for storing embeddings.\n",
        "    # We use a resizable dataset to append batches.\n",
        "    with h5py.File(embeddings_path, 'w') as f:\n",
        "        f.create_dataset(\n",
        "            'embeddings',\n",
        "            shape=(0, provenance['embedding_dimension']),\n",
        "            maxshape=(None, provenance['embedding_dimension']),\n",
        "            dtype='float32',\n",
        "            chunks=True # Enable chunking for efficient partial I/O\n",
        "        )\n",
        "\n",
        "    # --- 3. Batch Processing Loop ---\n",
        "    # Prepare lists to store metadata collected during the loop.\n",
        "    all_texts = df['full_text'].tolist()\n",
        "    all_article_ids = df['article_id'].tolist()\n",
        "    token_stats = []\n",
        "\n",
        "    # Process the corpus in batches to manage memory.\n",
        "    for i in tqdm(range(0, len(all_texts), batch_size), desc=\"Embedding Articles\"):\n",
        "        # Extract the current batch of texts.\n",
        "        batch_texts = all_texts[i:i + batch_size]\n",
        "\n",
        "        # --- Step 1 (logic): Tokenization and Statistics ---\n",
        "        # Tokenize the batch to get token counts before potential truncation.\n",
        "        # We do not truncate here, just count the raw tokens.\n",
        "        tokenized_batch = tokenizer.batch_encode_plus(\n",
        "            batch_texts,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=None, # Return lists of token IDs\n",
        "            truncation=False # Do not truncate for the count\n",
        "        )\n",
        "\n",
        "        # Calculate token counts and truncation flags for the batch.\n",
        "        for j, input_ids in enumerate(tokenized_batch['input_ids']):\n",
        "            count = len(input_ids)\n",
        "            token_stats.append({\n",
        "                'article_id': all_article_ids[i+j],\n",
        "                'token_count': count,\n",
        "                'truncation_flag': count > max_tokens\n",
        "            })\n",
        "\n",
        "        # --- Step 2 (logic): Embedding Inference ---\n",
        "        # Encode the batch. The model's internal tokenizer will handle truncation\n",
        "        # based on its configured max_seq_length, which we align with our limit.\n",
        "        model.max_seq_length = max_tokens\n",
        "        batch_embeddings = model.encode(\n",
        "            batch_texts,\n",
        "            batch_size=len(batch_texts), # Inner batch size\n",
        "            show_progress_bar=False, # Disable inner progress bar\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=False # Keep raw embeddings\n",
        "        )\n",
        "\n",
        "        # --- 4. Store Batch Results ---\n",
        "        # Append the computed embeddings to the HDF5 file.\n",
        "        with h5py.File(embeddings_path, 'a') as f:\n",
        "            dset = f['embeddings']\n",
        "            # Resize the dataset to accommodate the new batch.\n",
        "            dset.resize(dset.shape[0] + len(batch_embeddings), axis=0)\n",
        "            # Write the new batch of embeddings to the end of the dataset.\n",
        "            dset[-len(batch_embeddings):] = batch_embeddings.astype('float32')\n",
        "\n",
        "    # --- 5. Finalize and Save Metadata ---\n",
        "    # Convert the collected token statistics into a DataFrame.\n",
        "    token_stats_df = pd.DataFrame(token_stats)\n",
        "\n",
        "    # Merge the token statistics back into the original DataFrame.\n",
        "    df_augmented = df.merge(token_stats_df, on='article_id', how='left')\n",
        "\n",
        "    # Create and save the crosswalk table.\n",
        "    # The row index in the HDF5 file corresponds to the original DataFrame order.\n",
        "    crosswalk_df = pd.DataFrame({\n",
        "        'article_id': all_article_ids,\n",
        "        'embedding_row_index': range(len(all_article_ids))\n",
        "    })\n",
        "    crosswalk_df.to_csv(crosswalk_path, index=False)\n",
        "\n",
        "    return df_augmented, embeddings_path, crosswalk_path, provenance\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Validate embedding quality and detect drift\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_embedding_quality(\n",
        "    embeddings_path: str,\n",
        "    crosswalk_path: str,\n",
        "    df_augmented: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs quality validation and temporal drift analysis on embeddings.\n",
        "\n",
        "    Purpose:\n",
        "    This function provides a crucial post-mortem analysis of the generated\n",
        "    embeddings. It computes summary statistics to check for numerical issues\n",
        "    and analyzes the temporal evolution of the average embedding to detect\n",
        "    potential data or model drift over time.\n",
        "\n",
        "    Inputs:\n",
        "        embeddings_path (str): Path to the HDF5 file containing embeddings.\n",
        "        crosswalk_path (str): Path to the CSV crosswalk table.\n",
        "        df_augmented (pd.DataFrame): DataFrame with temporal metadata.\n",
        "\n",
        "    Processes:\n",
        "    1.  Computes global summary statistics (mean, std, min, max) by reading\n",
        "        the HDF5 file in chunks to remain memory-efficient.\n",
        "    2.  Checks for any NaN or infinity values in the embeddings.\n",
        "    3.  Merges metadata with the crosswalk to link embeddings to months/languages.\n",
        "    4.  Calculates the mean embedding vector for each month.\n",
        "    5.  Computes the cosine similarity between each monthly mean and the global\n",
        "        mean to create a drift metric time series.\n",
        "\n",
        "    Outputs:\n",
        "        (Dict[str, Any]): A dictionary containing the diagnostic report,\n",
        "                          including summary statistics and a DataFrame of the\n",
        "                          temporal drift analysis.\n",
        "    \"\"\"\n",
        "    # Initialize the diagnostic report.\n",
        "    diagnostics = {}\n",
        "\n",
        "    # --- 1. Summary Statistics (Memory-Efficient) ---\n",
        "    # Use an iterative approach to calculate stats without loading all data.\n",
        "    n_rows, n_dims = 0, 0\n",
        "    sum_vec, sum_sq_vec = None, None\n",
        "    min_val, max_val = np.inf, -np.inf\n",
        "    has_nan_inf = False\n",
        "\n",
        "    with h5py.File(embeddings_path, 'r') as f:\n",
        "        dset = f['embeddings']\n",
        "        n_rows, n_dims = dset.shape\n",
        "        if sum_vec is None:\n",
        "            sum_vec = np.zeros(n_dims, dtype=np.float64)\n",
        "            sum_sq_vec = np.zeros(n_dims, dtype=np.float64)\n",
        "\n",
        "        # Read the dataset in chunks.\n",
        "        chunk_size = 10000\n",
        "        for i in tqdm(range(0, n_rows, chunk_size), desc=\"Validating Embeddings\"):\n",
        "            chunk = dset[i:i + chunk_size]\n",
        "            if np.any(~np.isfinite(chunk)):\n",
        "                has_nan_inf = True\n",
        "\n",
        "            sum_vec += np.sum(chunk, axis=0)\n",
        "            sum_sq_vec += np.sum(chunk**2, axis=0)\n",
        "            min_val = min(min_val, np.min(chunk))\n",
        "            max_val = max(max_val, np.max(chunk))\n",
        "\n",
        "    # Calculate final statistics.\n",
        "    global_mean = sum_vec / n_rows\n",
        "    global_std = np.sqrt(sum_sq_vec / n_rows - global_mean**2)\n",
        "\n",
        "    diagnostics['summary_stats'] = {\n",
        "        'num_embeddings': n_rows,\n",
        "        'embedding_dim': n_dims,\n",
        "        'global_mean_norm': np.linalg.norm(global_mean),\n",
        "        'global_std_mean': np.mean(global_std),\n",
        "        'min_value': min_val,\n",
        "        'max_value': max_val,\n",
        "        'contains_nan_or_inf': has_nan_inf\n",
        "    }\n",
        "\n",
        "    # --- 2. Temporal Drift Analysis ---\n",
        "    # Load crosswalk and merge with temporal metadata.\n",
        "    crosswalk_df = pd.read_csv(crosswalk_path)\n",
        "    metadata_df = df_augmented[['article_id', 'year_month']].merge(\n",
        "        crosswalk_df, on='article_id'\n",
        "    )\n",
        "\n",
        "    monthly_means = {}\n",
        "    with h5py.File(embeddings_path, 'r') as f:\n",
        "        dset = f['embeddings']\n",
        "        # Group by month and get the corresponding embedding indices.\n",
        "        for month, group in tqdm(metadata_df.groupby('year_month'), desc=\"Analyzing Drift\"):\n",
        "            indices = group['embedding_row_index'].values\n",
        "            # Retrieve embeddings for the month and compute the mean.\n",
        "            # Reading by index list is efficient in HDF5.\n",
        "            monthly_embeddings = dset[sorted(indices)]\n",
        "            monthly_means[month] = np.mean(monthly_embeddings, axis=0)\n",
        "\n",
        "    # Calculate cosine similarity between each monthly mean and the global mean.\n",
        "    drift_data = []\n",
        "    for month, month_mean in monthly_means.items():\n",
        "        # Reshape for scipy's cosine function.\n",
        "        similarity = 1 - torch.nn.functional.cosine_similarity(\n",
        "            torch.from_numpy(month_mean).unsqueeze(0),\n",
        "            torch.from_numpy(global_mean).unsqueeze(0)\n",
        "        ).item()\n",
        "        drift_data.append({'year_month': month, 'cosine_similarity_to_global': similarity})\n",
        "\n",
        "    diagnostics['temporal_drift'] = pd.DataFrame(drift_data).sort_values('year_month').set_index('year_month')\n",
        "\n",
        "    return diagnostics\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_document_embeddings(\n",
        "    prepared_df: pd.DataFrame,\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    output_directory: str\n",
        ") -> Tuple[pd.DataFrame, str, str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the document embedding and validation pipeline.\n",
        "\n",
        "    This function manages the end-to-end process of converting a prepared text\n",
        "    corpus into document embeddings. It handles the computationally intensive\n",
        "    inference step, saves the artifacts to disk, and performs a subsequent\n",
        "    quality validation.\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The fully prepared DataFrame from Task 5.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        output_directory (str): The directory to save output artifacts.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): The DataFrame augmented with token statistics.\n",
        "        - (str): Path to the HDF5 file containing the embeddings.\n",
        "        - (str): Path to the CSV file for the article-to-embedding crosswalk.\n",
        "        - (Dict[str, Any]): A dictionary containing the full diagnostic report.\n",
        "    \"\"\"\n",
        "    # Suppress a common warning from sentence-transformers about tokenization length.\n",
        "    # We handle this explicitly, so the warning is not needed.\n",
        "    warnings.filterwarnings(\"ignore\", message=\"Token indices sequence length is longer than the specified maximum sequence length\")\n",
        "\n",
        "    # Extract the relevant configuration section.\n",
        "    embedding_config = fused_master_input_specification['master_config']['embedding_params']\n",
        "\n",
        "    # --- Steps 1 & 2: Run Inference and Store Artifacts ---\n",
        "    # This single function handles tokenization, embedding, and storage.\n",
        "    df_augmented, embeddings_path, crosswalk_path, provenance = _run_embedding_inference(\n",
        "        df=prepared_df,\n",
        "        config=embedding_config,\n",
        "        output_dir=output_directory\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Validate Embedding Quality ---\n",
        "    # Perform post-hoc validation on the generated artifacts.\n",
        "    diagnostics = _validate_embedding_quality(\n",
        "        embeddings_path=embeddings_path,\n",
        "        crosswalk_path=crosswalk_path,\n",
        "        df_augmented=df_augmented\n",
        "    )\n",
        "\n",
        "    # Add provenance information to the final diagnostic report.\n",
        "    diagnostics['provenance'] = provenance\n",
        "\n",
        "    # Restore default warning behavior.\n",
        "    warnings.resetwarnings()\n",
        "\n",
        "    return df_augmented, embeddings_path, crosswalk_path, diagnostics\n"
      ],
      "metadata": {
        "id": "LnTrr8-yUyy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Construct weak labels for relevance classification and prepare train/validation split\n",
        "\n",
        "# ==============================================================================================\n",
        "# Task 7: Construct weak labels for relevance classification and prepare train/validation split\n",
        "# ==============================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Map section_label to binary weak labels\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_weak_labels(\n",
        "    df_augmented: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a DataFrame with weak binary labels based on 'section_label'.\n",
        "\n",
        "    Purpose:\n",
        "    This function implements the weak supervision strategy from the paper. It\n",
        "    filters for articles that have a non-null 'section_label' and assigns a\n",
        "    binary label (1 for economics-related, 0 for other) based on whether the\n",
        "    section name appears in a predefined list of positive keywords.\n",
        "\n",
        "    Inputs:\n",
        "        df_augmented (pd.DataFrame): The DataFrame containing the full corpus\n",
        "                                     metadata, including 'section_label'.\n",
        "        config (Dict[str, Any]): The 'relevance_model_params' sub-dictionary\n",
        "                                 from the master config.\n",
        "\n",
        "    Processes:\n",
        "    1.  Filters the input DataFrame to retain only rows with a non-null\n",
        "        'section_label'.\n",
        "    2.  Normalizes the 'section_label' column (lowercase, strip whitespace) for\n",
        "        robust matching.\n",
        "    3.  Creates a lowercase set of positive section keywords from the config for\n",
        "        efficient lookup.\n",
        "    4.  Assigns a new column 'y_econ' with a value of 1 if the normalized\n",
        "        section label is in the positive set, and 0 otherwise.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A new DataFrame containing only the labelable articles,\n",
        "                        with the addition of the 'y_econ' binary label column.\n",
        "    \"\"\"\n",
        "    # --- 1. Filter for labelable articles ---\n",
        "    # Select only the rows where a section label is present.\n",
        "    labeled_df = df_augmented[df_augmented['section_label'].notna()].copy()\n",
        "\n",
        "    # --- 2. Prepare positive class vocabulary ---\n",
        "    # Get the list of positive sections from the config.\n",
        "    positive_sections: List[str] = config['positive_class_sections']\n",
        "    # Convert to a lowercase set for efficient, case-insensitive matching.\n",
        "    positive_sections_set: Set[str] = {s.lower() for s in positive_sections}\n",
        "\n",
        "    # --- 3. Assign binary labels ---\n",
        "    # Normalize the 'section_label' column for matching.\n",
        "    normalized_labels = labeled_df['section_label'].str.lower().str.strip()\n",
        "    # Create the 'y_econ' column. The result of .isin() is a boolean Series,\n",
        "    # which is cast to integer (True -> 1, False -> 0).\n",
        "    labeled_df['y_econ'] = normalized_labels.isin(positive_sections_set).astype(int)\n",
        "\n",
        "    return labeled_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 2: Identify outlets with reliable section coverage and restrict training set\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _filter_by_outlet_coverage(\n",
        "    labeled_df: pd.DataFrame,\n",
        "    full_df: pd.DataFrame,\n",
        "    coverage_threshold: float = 0.5\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the labeled dataset to include only articles from reliable outlets.\n",
        "\n",
        "    Purpose:\n",
        "    Some outlets may provide 'section_label' metadata sporadically or not at all.\n",
        "    Training on data from such outlets could introduce noise. This function\n",
        "    identifies outlets that provide section labels for a sufficiently high\n",
        "    proportion of their articles (defined by `coverage_threshold`) and restricts\n",
        "    the training dataset to only these \"reliable\" sources.\n",
        "\n",
        "    Inputs:\n",
        "        labeled_df (pd.DataFrame): The DataFrame of articles with weak labels.\n",
        "        full_df (pd.DataFrame): The full, cleansed corpus DataFrame, used to\n",
        "                                calculate accurate coverage ratios.\n",
        "        coverage_threshold (float): The minimum ratio of labeled to total\n",
        "                                    articles for an outlet to be considered reliable.\n",
        "\n",
        "    Processes:\n",
        "    1.  Calculates the total number of articles for each 'outlet_id' in the full corpus.\n",
        "    2.  Calculates the number of labeled articles for each 'outlet_id'.\n",
        "    3.  Computes the coverage ratio for each outlet.\n",
        "    4.  Identifies the set of 'outlet_id's that meet the coverage threshold.\n",
        "    5.  Filters the input `labeled_df` to keep only articles from these reliable outlets.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A filtered DataFrame containing high-quality labeled data.\n",
        "    \"\"\"\n",
        "    # --- 1. Calculate total articles per outlet from the full corpus ---\n",
        "    total_counts = full_df['outlet_id'].value_counts()\n",
        "\n",
        "    # --- 2. Calculate labeled articles per outlet ---\n",
        "    labeled_counts = labeled_df['outlet_id'].value_counts()\n",
        "\n",
        "    # --- 3. Compute coverage ratio ---\n",
        "    # Combine the two series into a DataFrame for calculation.\n",
        "    coverage_df = pd.DataFrame({'total': total_counts, 'labeled': labeled_counts}).fillna(0)\n",
        "    # The ratio is the number of labeled articles divided by the total.\n",
        "    coverage_df['coverage_ratio'] = coverage_df['labeled'] / coverage_df['total']\n",
        "\n",
        "    # --- 4. Identify reliable outlets ---\n",
        "    # Find the outlets where the coverage ratio meets the threshold.\n",
        "    reliable_outlets = coverage_df[coverage_df['coverage_ratio'] >= coverage_threshold].index.tolist()\n",
        "\n",
        "    # --- 5. Filter the labeled dataset ---\n",
        "    # Keep only the articles from the identified reliable outlets.\n",
        "    high_quality_labeled_df = labeled_df[labeled_df['outlet_id'].isin(reliable_outlets)]\n",
        "\n",
        "    return high_quality_labeled_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Perform temporal train/validation split to avoid leakage\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _perform_temporal_split(\n",
        "    df: pd.DataFrame,\n",
        "    validation_split_ratio: float\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits a time-indexed DataFrame into training and validation sets temporally.\n",
        "\n",
        "    Purpose:\n",
        "    To prevent look-ahead bias, a random split is inappropriate for time-series\n",
        "    data. This function strictly adheres to a temporal splitting methodology:\n",
        "    it sorts the data by time and allocates the earliest data for training and\n",
        "    the most recent data for validation. This simulates a real-world scenario\n",
        "    where a model is trained on the past and evaluated on the future.\n",
        "\n",
        "    Inputs:\n",
        "        df (pd.DataFrame): The DataFrame to be split, containing a\n",
        "                           'publication_datetime_utc' column.\n",
        "        validation_split_ratio (float): The proportion of the data to allocate\n",
        "                                        to the validation set (e.g., 0.2 for 20%).\n",
        "\n",
        "    Processes:\n",
        "    1.  Sorts the entire DataFrame chronologically by 'publication_datetime_utc'.\n",
        "    2.  Calculates the integer index at which to split the data.\n",
        "    3.  Uses `.iloc` to slice the DataFrame into training and validation sets.\n",
        "    4.  Performs a critical assertion to guarantee no time overlap between the two sets.\n",
        "    5.  Logs the size and class distribution of each set for transparency.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): The training set (earlier data).\n",
        "        - (pd.DataFrame): The validation set (later data).\n",
        "    \"\"\"\n",
        "    # --- 1. Temporal Sorting ---\n",
        "    # This is the most critical step to ensure a valid temporal split.\n",
        "    df_sorted = df.sort_values('publication_datetime_utc').reset_index(drop=True)\n",
        "\n",
        "    # --- 2. Calculate Split Point ---\n",
        "    # Determine the index that separates the training and validation data.\n",
        "    split_index = int(len(df_sorted) * (1 - validation_split_ratio))\n",
        "\n",
        "    # --- 3. Slice the DataFrame ---\n",
        "    # The training set contains all data up to the split point.\n",
        "    train_df = df_sorted.iloc[:split_index]\n",
        "    # The validation set contains all data from the split point onwards.\n",
        "    val_df = df_sorted.iloc[split_index:]\n",
        "\n",
        "    # --- 4. Verification ---\n",
        "    # This assertion is a crucial safeguard against implementation errors.\n",
        "    if not train_df.empty and not val_df.empty:\n",
        "        assert train_df['publication_datetime_utc'].max() <= val_df['publication_datetime_utc'].min()\n",
        "\n",
        "    # --- 5. Logging ---\n",
        "    # Provide transparent reporting on the outcome of the split.\n",
        "    print(f\"Temporal split resulted in:\")\n",
        "    print(f\"  - Training set size: {len(train_df)}\")\n",
        "    print(f\"  - Validation set size: {len(val_df)}\")\n",
        "    if 'y_econ' in train_df.columns:\n",
        "        print(f\"  - Training set class distribution:\\n{train_df['y_econ'].value_counts(normalize=True)}\")\n",
        "        print(f\"  - Validation set class distribution:\\n{val_df['y_econ'].value_counts(normalize=True)}\")\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_relevance_training_data(\n",
        "    df_augmented: pd.DataFrame,\n",
        "    crosswalk_path: str,\n",
        "    embeddings_path: str,\n",
        "    fused_master_input_specification: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, pd.Series, np.ndarray, pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full pipeline for creating relevance model training data.\n",
        "\n",
        "    Purpose:\n",
        "    This function manages the entire process of preparing data for the relevance\n",
        "    classifier. It translates noisy metadata into weak labels, selects a\n",
        "    high-quality subset of articles from reliable sources, performs a\n",
        "    temporally-sound train/validation split to prevent look-ahead bias, and\n",
        "    retrieves the corresponding document embeddings from storage. A critical\n",
        "    feature of this implementation is the robust alignment of features (X) and\n",
        "    labels (y) to prevent data shuffling errors.\n",
        "\n",
        "    Inputs:\n",
        "        df_augmented (pd.DataFrame): The fully prepared corpus DataFrame from Task 5/6.\n",
        "        crosswalk_path (str): Path to the CSV crosswalk table which maps\n",
        "                              'article_id' to 'embedding_row_index'.\n",
        "        embeddings_path (str): Path to the HDF5 file containing all embeddings.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "\n",
        "    Processes:\n",
        "    1.  Calls a helper to create weak binary labels from 'section_label' metadata.\n",
        "    2.  Calls a helper to filter the labeled data, keeping only articles from\n",
        "        outlets with high metadata coverage.\n",
        "    3.  Calls a helper to perform a strict temporal split into training and\n",
        "        validation sets.\n",
        "    4.  Retrieves the HDF5 row indices for the articles in each set.\n",
        "    5.  Reads the corresponding embedding vectors from the HDF5 file using a\n",
        "        sorted list of indices for efficiency.\n",
        "    6.  Performs a robust re-indexing of the labels to guarantee\n",
        "        perfect alignment with the sorted order of the retrieved embeddings.\n",
        "    7.  Returns the final, perfectly aligned training and validation matrices/vectors.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing the prepared data in a format ready for model training:\n",
        "        - (np.ndarray): X_train, the matrix of training embeddings.\n",
        "        - (pd.Series): y_train, the series of training labels, perfectly aligned with X_train.\n",
        "        - (np.ndarray): X_val, the matrix of validation embeddings.\n",
        "        - (pd.Series): y_val, the series of validation labels, perfectly aligned with X_val.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_augmented, pd.DataFrame):\n",
        "        raise TypeError(\"`df_augmented` must be a pandas DataFrame.\")\n",
        "    # Further input validation would check for required columns.\n",
        "\n",
        "    # --- Extract Configuration ---\n",
        "    # Get the parameters for the relevance model and its training procedure.\n",
        "    relevance_config = fused_master_input_specification['master_config']['relevance_model_params']\n",
        "    training_config = relevance_config['training_params']\n",
        "\n",
        "    # --- Step 1: Create weak labels from section metadata ---\n",
        "    # This creates a new DataFrame containing only articles with section labels.\n",
        "    labeled_df = _create_weak_labels(df_augmented, relevance_config)\n",
        "\n",
        "    # --- Step 2: Filter for articles from reliable outlets ---\n",
        "    # This refines the labeled set to a high-quality subset for training.\n",
        "    high_quality_df = _filter_by_outlet_coverage(labeled_df, df_augmented, 0.5)\n",
        "\n",
        "    # --- Step 3: Perform the temporal train/validation split ---\n",
        "    # This splits the data into past (train) and future (validation) sets.\n",
        "    train_meta_df, val_meta_df = _perform_temporal_split(\n",
        "        high_quality_df,\n",
        "        training_config['validation_split']\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Retrieve Embeddings and Align Labels ---\n",
        "    # Load the crosswalk to map article_ids to HDF5 row indices.\n",
        "    crosswalk_df = pd.read_csv(crosswalk_path)\n",
        "\n",
        "    # Merge with metadata to get indices for our train/val sets.\n",
        "    train_merged = train_meta_df.merge(crosswalk_df, on='article_id')\n",
        "    val_merged = val_meta_df.merge(crosswalk_df, on='article_id')\n",
        "\n",
        "    # Get the raw HDF5 row indices for each set.\n",
        "    train_indices = train_merged['embedding_row_index'].values\n",
        "    val_indices = val_merged['embedding_row_index'].values\n",
        "\n",
        "    # Sort the indices for efficient reading.\n",
        "    sorted_train_indices = np.sort(train_indices)\n",
        "    sorted_val_indices = np.sort(val_indices)\n",
        "\n",
        "    # Open the HDF5 file to read the embedding data.\n",
        "    with h5py.File(embeddings_path, 'r') as f:\n",
        "        embeddings_dset = f['embeddings']\n",
        "        # Retrieve the feature matrices (X) using the sorted indices.\n",
        "        # This defines the final order of the samples in our training data.\n",
        "        X_train = embeddings_dset[sorted_train_indices]\n",
        "        X_val = embeddings_dset[sorted_val_indices]\n",
        "\n",
        "    # Now, reorder the labels (y) to match the exact order of the feature matrices.\n",
        "    # First, set the index of the metadata to the HDF5 row index.\n",
        "    train_reindexed = train_merged.set_index('embedding_row_index')\n",
        "    val_reindexed = val_merged.set_index('embedding_row_index')\n",
        "\n",
        "    # Then, use `.loc` with the *sorted* indices to select and reorder the labels.\n",
        "    y_train = train_reindexed.loc[sorted_train_indices]['y_econ']\n",
        "    y_val = val_reindexed.loc[sorted_val_indices]['y_econ']\n",
        "\n",
        "    # --- Final Assertion ---\n",
        "    # This safeguard confirms that the number of samples in features and labels match.\n",
        "    assert len(X_train) == len(y_train), \"Mismatch in training set feature/label count.\"\n",
        "    assert len(X_val) == len(y_val), \"Mismatch in validation set feature/label count.\"\n",
        "\n",
        "    print(f\"Successfully prepared and aligned data for relevance model training.\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val\n"
      ],
      "metadata": {
        "id": "rX2Vs_haWIFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — Train the MLP relevance classifier with early stopping and balanced class weighting\n",
        "\n",
        "# =============================================================================================\n",
        "# Task 8: Train the MLP relevance classifier with early stopping and balanced class weighting\n",
        "# =============================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Specify the MLP architecture and initialize\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _build_mlp_model(\n",
        "    config: Dict[str, Any],\n",
        "    seed: int\n",
        ") -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Builds a Keras Sequential model based on a declarative configuration.\n",
        "\n",
        "    Purpose:\n",
        "    This function translates the `nn_architecture` specification from the master\n",
        "    configuration into a concrete `tf.keras.Model` instance. It ensures\n",
        "    reproducibility by setting a global random seed before model construction,\n",
        "    which governs weight initialization and dropout behavior.\n",
        "\n",
        "    Inputs:\n",
        "        config (Dict[str, Any]): The 'relevance_model_params' sub-dictionary,\n",
        "                                 containing the 'nn_architecture' key.\n",
        "        seed (int): The global random seed for reproducibility.\n",
        "\n",
        "    Processes:\n",
        "    1.  Sets the TensorFlow global random seed.\n",
        "    2.  Initializes a `tf.keras.Sequential` model.\n",
        "    3.  Dynamically adds an Input layer based on 'input_dim' from the config.\n",
        "    4.  Iterates through the list of layer specifications in the config.\n",
        "    5.  For each specification, it adds the corresponding Keras layer (Dense or\n",
        "        Dropout) with the specified parameters.\n",
        "\n",
        "    Outputs:\n",
        "        (tf.keras.Model): The constructed, but not yet compiled, Keras model.\n",
        "\n",
        "    Error Handling:\n",
        "        Raises ValueError: If the configuration specifies an unsupported layer type.\n",
        "    \"\"\"\n",
        "    # --- 1. Set Seed for Reproducibility ---\n",
        "    # This ensures that weight initialization and dropout are deterministic.\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # --- 2. Initialize Sequential Model ---\n",
        "    # The model will be a simple stack of layers.\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Extract the architecture specification from the config.\n",
        "    nn_arch = config['nn_architecture']\n",
        "\n",
        "    # --- 3. Add Input Layer ---\n",
        "    # Define the shape of the input vectors (1024-dimensional embeddings).\n",
        "    model.add(tf.keras.layers.Input(shape=(nn_arch['input_dim'],)))\n",
        "\n",
        "    # --- 4. Dynamically Add Hidden and Output Layers ---\n",
        "    # Map string identifiers from the config to actual Keras layer classes.\n",
        "    layer_mapping = {\n",
        "        'Dense': tf.keras.layers.Dense,\n",
        "        'Dropout': tf.keras.layers.Dropout\n",
        "    }\n",
        "\n",
        "    # Iterate through the layer specifications in the config.\n",
        "    for layer_spec in nn_arch['layers']:\n",
        "        layer_type_str = layer_spec['type']\n",
        "        # Find the corresponding Keras layer class.\n",
        "        layer_class = layer_mapping.get(layer_type_str)\n",
        "\n",
        "        if layer_class is None:\n",
        "            raise ValueError(f\"Unsupported layer type in config: '{layer_type_str}'\")\n",
        "\n",
        "        # Create a dictionary of parameters for the layer constructor.\n",
        "        params = layer_spec.copy()\n",
        "        del params['type'] # Remove the type key as it's not a layer argument.\n",
        "\n",
        "        # Add the instantiated layer to the model.\n",
        "        model.add(layer_class(**params))\n",
        "\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Configure training with balanced class weighting and temporal validation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _configure_training_procedure(\n",
        "    y_train: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[tf.keras.optimizers.Optimizer, str, Dict[int, float], List[tf.keras.callbacks.Callback]]:\n",
        "    \"\"\"\n",
        "    Configures all components required for the model training process.\n",
        "\n",
        "    Purpose:\n",
        "    This function prepares the optimizer, loss function, class weights, and\n",
        "    callbacks needed for `model.fit()`. It implements the 'balanced' class\n",
        "    weighting strategy to counteract label imbalance and sets up early stopping\n",
        "    to prevent overfitting and select the best model based on the out-of-time\n",
        "    validation set.\n",
        "\n",
        "    Inputs:\n",
        "        y_train (pd.Series): The training labels, used to calculate class weights.\n",
        "        config (Dict[str, Any]): The 'training_params' sub-dictionary.\n",
        "\n",
        "    Processes:\n",
        "    1.  Instantiates the Adam optimizer with the configured learning rate.\n",
        "    2.  Calculates class weights to give more importance to the minority class.\n",
        "    3.  Configures the EarlyStopping callback to monitor validation AUC and\n",
        "        restore the best weights upon completion.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (tf.keras.optimizers.Optimizer): The configured Adam optimizer.\n",
        "        - (str): The name of the loss function ('BinaryCrossentropy').\n",
        "        - (Dict[int, float]): The dictionary of class weights.\n",
        "        - (List[tf.keras.callbacks.Callback]): A list of callbacks to use during training.\n",
        "    \"\"\"\n",
        "    # --- 1. Configure Optimizer and Loss ---\n",
        "    # Instantiate the Adam optimizer with the specified learning rate.\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
        "    # The loss function is binary cross-entropy for binary classification.\n",
        "    loss = config['loss_function']\n",
        "\n",
        "    # --- 2. Calculate Class Weights ---\n",
        "    # This counteracts the effect of imbalanced labels in the training data.\n",
        "    class_counts = y_train.value_counts()\n",
        "    total_samples = len(y_train)\n",
        "\n",
        "    # Formula for balanced weights: weight = (total_samples / (n_classes * count_for_class))\n",
        "    weight_for_0 = (total_samples / (2 * class_counts.get(0, 1)))\n",
        "    weight_for_1 = (total_samples / (2 * class_counts.get(1, 1)))\n",
        "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "    print(f\"Computed class weights: {class_weight}\")\n",
        "\n",
        "    # --- 3. Configure Callbacks ---\n",
        "    # Early stopping prevents overfitting and saves the best version of the model.\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=config['early_stopping_metric'], # e.g., 'val_auc'\n",
        "        patience=config['early_stopping_patience'], # e.g., 5 epochs\n",
        "        mode='max', # We want to maximize the area under the curve.\n",
        "        restore_best_weights=True, # CRITICAL: ensures the final model has the best weights.\n",
        "        verbose=1\n",
        "    )\n",
        "    callbacks = [early_stopping_callback]\n",
        "\n",
        "    return optimizer, loss, class_weight, callbacks\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Train, validate, and persist the model\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _train_and_persist_model(\n",
        "    model: tf.keras.Model,\n",
        "    X_train: np.ndarray,\n",
        "    y_train: pd.Series,\n",
        "    X_val: np.ndarray,\n",
        "    y_val: pd.Series,\n",
        "    class_weight: Dict[int, float],\n",
        "    callbacks: List[tf.keras.callbacks.Callback],\n",
        "    model_path: str,\n",
        "    epochs: int = 100,\n",
        "    batch_size: int = 64\n",
        ") -> Tuple[tf.keras.callbacks.History, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes the model training, evaluates the final model, and saves it to disk.\n",
        "\n",
        "    Purpose:\n",
        "    This function encapsulates the core `fit`, `evaluate`, and `save` lifecycle.\n",
        "    It returns a detailed history of the training process and a summary of the\n",
        "    final model's performance on the held-out validation set.\n",
        "\n",
        "    Inputs:\n",
        "        model (tf.keras.Model): The compiled Keras model.\n",
        "        X_train, y_train: The training features and labels.\n",
        "        X_val, y_val: The validation features and labels.\n",
        "        class_weight (Dict[int, float]): The computed class weights.\n",
        "        callbacks (List): Keras callbacks, including early stopping.\n",
        "        model_path (str): The file path where the trained model will be saved.\n",
        "        epochs (int): The maximum number of training epochs.\n",
        "        batch_size (int): The number of samples per gradient update.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (tf.keras.callbacks.History): The training history object.\n",
        "        - (Dict[str, Any]): A dictionary of final validation metrics.\n",
        "    \"\"\"\n",
        "    # --- 1. Train the Model ---\n",
        "    # The `fit` method executes the training loop.\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=callbacks,\n",
        "        verbose=2 # Print one line per epoch.\n",
        "    )\n",
        "\n",
        "    # --- 2. Evaluate the Final Model ---\n",
        "    # Evaluate the model on the validation set (using the best weights restored by the callback).\n",
        "    print(\"\\nEvaluating final model on the validation set...\")\n",
        "    final_metrics = model.evaluate(X_val, y_val, return_dict=True)\n",
        "\n",
        "    # Generate a more detailed classification report and confusion matrix.\n",
        "    y_pred_proba = model.predict(X_val).flatten()\n",
        "    y_pred_class = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    report = classification_report(y_val, y_pred_class, output_dict=True)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred_class)\n",
        "\n",
        "    final_metrics['classification_report'] = report\n",
        "    final_metrics['confusion_matrix'] = conf_matrix.tolist() # Convert to list for serialization\n",
        "\n",
        "    print(f\"Final Validation Metrics: {final_metrics}\")\n",
        "\n",
        "    # --- 3. Persist the Model ---\n",
        "    # Save the entire model (architecture, weights, optimizer state).\n",
        "    model.save(model_path)\n",
        "    print(f\"Trained model saved to: {model_path}\")\n",
        "\n",
        "    return history, final_metrics\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_relevance_classifier(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: pd.Series,\n",
        "    X_val: np.ndarray,\n",
        "    y_val: pd.Series,\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    output_path: str\n",
        ") -> Tuple[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end training of the MLP relevance classifier.\n",
        "\n",
        "    This function manages the entire model training workflow: building the\n",
        "    architecture from the config, configuring the training procedure with best\n",
        "    practices, executing the training loop, and saving the final artifact.\n",
        "\n",
        "    Args:\n",
        "        X_train, y_train: Training features and labels from Task 7.\n",
        "        X_val, y_val: Validation features and labels from Task 7.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        output_path (str): The file path to save the final trained model.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - (str): The path to the saved model.\n",
        "        - (Dict[str, Any]): A dictionary containing the training history and\n",
        "          final validation metrics.\n",
        "    \"\"\"\n",
        "    # Extract relevant configuration sections.\n",
        "    relevance_config = fused_master_input_specification['master_config']['relevance_model_params']\n",
        "    training_config = relevance_config['training_params']\n",
        "    seed = fused_master_input_specification['master_config']['reproducibility']['random_seeds']['global_seed']\n",
        "\n",
        "    # --- Step 1: Build the model architecture ---\n",
        "    model = _build_mlp_model(relevance_config, seed)\n",
        "    model.summary()\n",
        "\n",
        "    # --- Step 2: Configure the training procedure ---\n",
        "    optimizer, loss, class_weight, callbacks = _configure_training_procedure(\n",
        "        y_train, training_config\n",
        "    )\n",
        "\n",
        "    # Compile the model, making it ready for training.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy']\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Train, evaluate, and persist the model ---\n",
        "    history, final_metrics = _train_and_persist_model(\n",
        "        model=model,\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_val=X_val,\n",
        "        y_val=y_val,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=callbacks,\n",
        "        model_path=output_path\n",
        "    )\n",
        "\n",
        "    # --- 4. Package Results ---\n",
        "    # Create a comprehensive results dictionary for logging and analysis.\n",
        "    results = {\n",
        "        \"model_path\": output_path,\n",
        "        \"validation_metrics\": final_metrics,\n",
        "        \"training_history\": history.history\n",
        "    }\n",
        "\n",
        "    return output_path, results\n"
      ],
      "metadata": {
        "id": "BzRB85bsXOyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Apply the relevance classifier to the full corpus and select economics-related articles\n",
        "\n",
        "# ================================================================================================\n",
        "# Task 9: Apply the relevance classifier to the full corpus and select economics-related articles\n",
        "# ================================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Score all articles with the trained relevance classifier\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _score_all_articles(\n",
        "    model_path: str,\n",
        "    embeddings_path: str,\n",
        "    inference_batch_size: int = 1024\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Scores all articles in the corpus using the trained relevance classifier.\n",
        "\n",
        "    Purpose:\n",
        "    This function performs large-scale inference, applying the trained MLP\n",
        "    classifier to every document embedding in the corpus. To handle potentially\n",
        "    millions of articles without exhausting memory, it reads the embeddings from\n",
        "    the HDF5 file in batches, runs prediction on each batch, and aggregates the\n",
        "    resulting probabilities.\n",
        "\n",
        "    Inputs:\n",
        "        model_path (str): The file path to the saved Keras model from Task 8.\n",
        "        embeddings_path (str): The path to the HDF5 file containing the full\n",
        "                               embedding matrix.\n",
        "        inference_batch_size (int): The number of embeddings to process in a\n",
        "                                    single prediction batch.\n",
        "\n",
        "    Processes:\n",
        "    1.  Loads the pre-trained Keras model.\n",
        "    2.  Opens the HDF5 embeddings file for reading.\n",
        "    3.  Iterates through the embedding dataset in batches of size\n",
        "        `inference_batch_size`.\n",
        "    4.  For each batch, calls `model.predict()` to get the relevance probabilities.\n",
        "    5.  Collects the probabilities from all batches.\n",
        "    6.  Concatenates the batch results into a single NumPy array.\n",
        "\n",
        "    Outputs:\n",
        "        (np.ndarray): A 1D NumPy array of float values (probabilities), where\n",
        "                      the i-th element is the relevance score for the i-th\n",
        "                      article in the embedding file.\n",
        "    \"\"\"\n",
        "    # --- 1. Load the trained model ---\n",
        "    print(f\"Loading relevance classifier from: {model_path}\")\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    # --- 2. Perform Batch Inference ---\n",
        "    # This list will store the probability arrays from each batch.\n",
        "    all_probabilities = []\n",
        "\n",
        "    # Open the HDF5 file for reading.\n",
        "    with h5py.File(embeddings_path, 'r') as f:\n",
        "        embeddings_dset = f['embeddings']\n",
        "        num_articles = embeddings_dset.shape[0]\n",
        "\n",
        "        # Iterate through the dataset in batches.\n",
        "        for i in tqdm(range(0, num_articles, inference_batch_size), desc=\"Scoring article relevance\"):\n",
        "            # Read a batch of embeddings from the file.\n",
        "            batch_embeddings = embeddings_dset[i:i + inference_batch_size]\n",
        "\n",
        "            # Run prediction on the batch.\n",
        "            # The `predict` method is highly optimized for this task.\n",
        "            batch_probs = model.predict(batch_embeddings, batch_size=inference_batch_size)\n",
        "\n",
        "            # Flatten the output from (batch_size, 1) to (batch_size,) and append.\n",
        "            all_probabilities.append(batch_probs.flatten())\n",
        "\n",
        "    # --- 3. Aggregate Results ---\n",
        "    # Concatenate the list of batch arrays into a single, large NumPy array.\n",
        "    full_probability_vector = np.concatenate(all_probabilities)\n",
        "\n",
        "    return full_probability_vector\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Threshold probabilities to define the relevant subset R\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _identify_relevant_indices(\n",
        "    probabilities: np.ndarray,\n",
        "    threshold: float\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Identifies the indices of relevant articles based on a probability threshold.\n",
        "\n",
        "    Purpose:\n",
        "    This function applies the classification rule to the predicted probabilities,\n",
        "    translating the continuous scores into a binary decision (relevant vs. not\n",
        "    relevant). It returns the indices of the articles that pass the relevance\n",
        "    filter.\n",
        "\n",
        "    Inputs:\n",
        "        probabilities (np.ndarray): The 1D array of relevance scores for all articles.\n",
        "        threshold (float): The classification threshold (e.g., 0.5). Articles with\n",
        "                           a score >= this value are considered relevant.\n",
        "\n",
        "    Processes:\n",
        "    1.  Performs a vectorized boolean comparison to create a mask of relevant articles.\n",
        "    2.  Uses `np.where()` to extract the integer indices of the `True` values in the mask.\n",
        "\n",
        "    Outputs:\n",
        "        (np.ndarray): A 1D NumPy array of integer indices corresponding to the\n",
        "                      rows of the relevant articles in the original embedding file.\n",
        "    \"\"\"\n",
        "    # --- 1. Create a boolean mask ---\n",
        "    # This is a highly efficient vectorized operation.\n",
        "    is_relevant_mask = probabilities >= threshold\n",
        "\n",
        "    # --- 2. Extract indices ---\n",
        "    # np.where() returns a tuple of arrays; we need the first element.\n",
        "    relevant_indices = np.where(is_relevant_mask)[0]\n",
        "\n",
        "    print(f\"Identified {len(relevant_indices)} relevant articles \"\n",
        "          f\"out of {len(probabilities)} total ({(len(relevant_indices)/len(probabilities)):.2%}).\")\n",
        "\n",
        "    return relevant_indices\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Persist relevance scores and audit the filtered corpus\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _persist_scores_and_audit(\n",
        "    df_augmented: pd.DataFrame,\n",
        "    crosswalk_df: pd.DataFrame,\n",
        "    all_probabilities: np.ndarray,\n",
        "    relevant_indices: np.ndarray,\n",
        "    output_dir: str\n",
        ") -> Tuple[pd.DataFrame, str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Saves all scores, creates the filtered DataFrame, and generates an audit report.\n",
        "\n",
        "    Purpose:\n",
        "    This function finalizes the filtering process by creating and saving the key\n",
        "    output artifacts: a file containing the relevance score for every article\n",
        "    (for full transparency), the final filtered DataFrame of relevant articles,\n",
        "    and a summary audit of the filtered corpus.\n",
        "\n",
        "    Inputs:\n",
        "        df_augmented (pd.DataFrame): The full metadata DataFrame.\n",
        "        crosswalk_df (pd.DataFrame): The table mapping article_id to embedding index.\n",
        "        all_probabilities (np.ndarray): The relevance scores for all articles.\n",
        "        relevant_indices (np.ndarray): The indices of the relevant articles.\n",
        "        output_dir (str): Directory to save the output files.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): The filtered DataFrame (`df_relevant`) containing metadata\n",
        "          for only the relevant articles.\n",
        "        - (str): The path to the saved file of all relevance scores.\n",
        "        - (Dict[str, Any]): An audit dictionary summarizing the filtered corpus.\n",
        "    \"\"\"\n",
        "    # --- 1. Persist All Relevance Scores ---\n",
        "    # Create a DataFrame to hold the scores, indexed by the embedding row index.\n",
        "    scores_df = pd.DataFrame(\n",
        "        {'p_econ': all_probabilities},\n",
        "        index=pd.Index(range(len(all_probabilities)), name='embedding_row_index')\n",
        "    )\n",
        "    # Merge with the crosswalk to link scores to article_ids.\n",
        "    full_scores_df = crosswalk_df.merge(scores_df, on='embedding_row_index')\n",
        "\n",
        "    # Save to a performant file format like Feather or Parquet.\n",
        "    scores_path = os.path.join(output_dir, 'relevance_scores.feather')\n",
        "    full_scores_df[['article_id', 'p_econ']].to_feather(scores_path)\n",
        "    print(f\"All relevance scores saved to: {scores_path}\")\n",
        "\n",
        "    # --- 2. Create the Filtered DataFrame of Relevant Articles ---\n",
        "    # Get the article_ids of the relevant articles using the indices.\n",
        "    relevant_article_ids = crosswalk_df.loc[relevant_indices, 'article_id'].values\n",
        "\n",
        "    # Filter the main metadata DataFrame to keep only these articles.\n",
        "    # Using .isin() on a set is highly efficient.\n",
        "    df_relevant = df_augmented[df_augmented['article_id'].isin(set(relevant_article_ids))].copy()\n",
        "\n",
        "    # --- 3. Generate Audit Report ---\n",
        "    # Create a summary of the final, relevant corpus.\n",
        "    audit_report = {\n",
        "        'total_relevant_articles': len(df_relevant),\n",
        "        'relevant_articles_by_language': df_relevant['language'].value_counts().to_dict(),\n",
        "        'monthly_relevant_article_counts': df_relevant['year_month'].value_counts().sort_index().to_dict()\n",
        "    }\n",
        "\n",
        "    print(\"--- Relevance Filter Audit ---\")\n",
        "    print(f\"Total relevant articles: {audit_report['total_relevant_articles']}\")\n",
        "    print(f\"Breakdown by language: {audit_report['relevant_articles_by_language']}\")\n",
        "\n",
        "    return df_relevant, scores_path, audit_report\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def filter_corpus_by_relevance(\n",
        "    df_augmented: pd.DataFrame,\n",
        "    model_path: str,\n",
        "    embeddings_path: str,\n",
        "    crosswalk_path: str,\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    output_directory: str\n",
        ") -> Tuple[pd.DataFrame, str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the application of the relevance classifier to the full corpus.\n",
        "\n",
        "    This function manages the end-to-end workflow of scoring all articles for\n",
        "    relevance, applying a threshold to filter them, and saving the resulting\n",
        "    artifacts and audit reports.\n",
        "\n",
        "    Args:\n",
        "        df_augmented (pd.DataFrame): The full, cleansed, and augmented corpus metadata.\n",
        "        model_path (str): Path to the trained relevance classifier model.\n",
        "        embeddings_path (str): Path to the HDF5 file of all embeddings.\n",
        "        crosswalk_path (str): Path to the CSV crosswalk table.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        output_directory (str): Directory to save output artifacts.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): A new DataFrame containing metadata for only the\n",
        "          articles classified as relevant.\n",
        "        - (str): The path to the file containing the relevance scores for all articles.\n",
        "        - (Dict[str, Any]): A final audit report on the filtered corpus.\n",
        "    \"\"\"\n",
        "    # Extract the classification threshold from the configuration.\n",
        "    threshold = fused_master_input_specification['master_config']['relevance_model_params']['classification_threshold']\n",
        "\n",
        "    # --- Step 1: Score all articles using the trained model ---\n",
        "    all_probabilities = _score_all_articles(model_path, embeddings_path)\n",
        "\n",
        "    # --- Step 2: Identify indices of relevant articles ---\n",
        "    relevant_indices = _identify_relevant_indices(all_probabilities, threshold)\n",
        "\n",
        "    # --- Step 3: Persist scores, create filtered DataFrame, and audit ---\n",
        "    crosswalk_df = pd.read_csv(crosswalk_path)\n",
        "    df_relevant, scores_path, audit_report = _persist_scores_and_audit(\n",
        "        df_augmented=df_augmented,\n",
        "        crosswalk_df=crosswalk_df,\n",
        "        all_probabilities=all_probabilities,\n",
        "        relevant_indices=relevant_indices,\n",
        "        output_dir=output_directory\n",
        "    )\n",
        "\n",
        "    return df_relevant, scores_path, audit_report\n"
      ],
      "metadata": {
        "id": "xJ7XDI2JYkZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 — Generate 256 synthetic articles via Claude 3.5 Sonnet using exact Appendix prompts\n",
        "\n",
        "# =============================================================================================\n",
        "# Task 10: Generate 256 synthetic articles via Claude 3.5 Sonnet using exact Appendix prompts\n",
        "# =============================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Configure LLM API and fix all generation parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _call_llm_api(\n",
        "    client: Anthropic,\n",
        "    prompt: str,\n",
        "    llm_config: Dict[str, Any],\n",
        "    max_retries: int = 3,\n",
        "    initial_backoff: float = 2.0\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Calls the Anthropic API with a given prompt and handles transient errors.\n",
        "\n",
        "    Purpose:\n",
        "    This function encapsulates a single, robust call to the LLM API. It constructs\n",
        "    the request payload from the configuration, sends the request, and implements\n",
        "    a retry mechanism with exponential backoff to gracefully handle common\n",
        "    transient issues like rate limiting or network errors.\n",
        "\n",
        "    Inputs:\n",
        "        client (Anthropic): An initialized Anthropic API client.\n",
        "        prompt (str): The full text of the prompt to send to the LLM.\n",
        "        llm_config (Dict[str, Any]): The 'llm_config' sub-dictionary containing\n",
        "                                     model identifier and generation parameters.\n",
        "        max_retries (int): The maximum number of times to retry a failed API call.\n",
        "        initial_backoff (float): The initial delay (in seconds) for the retry mechanism.\n",
        "\n",
        "    Outputs:\n",
        "        (str): The text content of the LLM's response.\n",
        "\n",
        "    Error Handling:\n",
        "        Raises RuntimeError: If the API call fails after all retry attempts.\n",
        "    \"\"\"\n",
        "    # Extract generation parameters from the configuration.\n",
        "    generation_params = llm_config['generation_params']\n",
        "\n",
        "    # The 'seed' parameter is experimental and might not be supported by all models.\n",
        "    # We include it only if it's present in the config.\n",
        "    api_kwargs = {\n",
        "        \"model\": llm_config['model_identifier'],\n",
        "        \"max_tokens\": generation_params['max_tokens'],\n",
        "        \"temperature\": generation_params['temperature'],\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "    if 'seed' in generation_params:\n",
        "        api_kwargs['seed'] = generation_params['seed']\n",
        "\n",
        "    # --- Retry Logic ---\n",
        "    # This loop makes the generation process resilient to transient network/API issues.\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Make the API call to the messages endpoint.\n",
        "            response = client.messages.create(**api_kwargs)\n",
        "            # The response content is in the first block of the content list.\n",
        "            return response.content[0].text\n",
        "        except APIError as e:\n",
        "            # If an API error occurs, log it and prepare to retry.\n",
        "            print(f\"API Error on attempt {attempt + 1}/{max_retries}: {e}. Retrying...\")\n",
        "            # If this was the last attempt, re-raise the error as a RuntimeError.\n",
        "            if attempt == max_retries - 1:\n",
        "                raise RuntimeError(f\"API call failed after {max_retries} attempts.\") from e\n",
        "            # Wait for an exponentially increasing amount of time before the next retry.\n",
        "            time.sleep(initial_backoff * (2 ** attempt))\n",
        "\n",
        "    # This line should be unreachable but is included for logical completeness.\n",
        "    raise RuntimeError(\"Exited retry loop unexpectedly.\")\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Generate synthetic articles using the Appendix financial-markets prompt and domain-parameterized variants\n",
        "# ---------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_generation_plan(\n",
        "    client: Anthropic,\n",
        "    config: Dict[str, Any],\n",
        "    output_path: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the full synthetic dataset according to a predefined plan.\n",
        "\n",
        "    Purpose:\n",
        "    This function orchestrates the entire generation process. It determines how\n",
        "    many articles are needed, constructs the precise prompts for each, calls the\n",
        "    LLM via a helper function, and saves the results incrementally to ensure\n",
        "    resumability and prevent data loss.\n",
        "\n",
        "    Inputs:\n",
        "        client (Anthropic): The initialized Anthropic API client.\n",
        "        config (Dict[str, Any]): The 'sentiment_model_params' sub-dictionary.\n",
        "        output_path (str): The path to the CSV file where results are saved.\n",
        "\n",
        "    Processes:\n",
        "    1.  Defines the generation plan (number of articles per domain/sentiment).\n",
        "    2.  Checks if a partial results file exists at `output_path` to resume.\n",
        "    3.  Iterates through the remaining generation tasks.\n",
        "    4.  For each task, constructs the appropriate prompt using templates.\n",
        "    5.  Calls the robust `_call_llm_api` function to get the generated text.\n",
        "    6.  Parses the response (assuming the LLM provides multiple articles as requested).\n",
        "    7.  Appends the new articles to the results list and saves to disk.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A DataFrame containing the complete set of 256 synthetic articles.\n",
        "    \"\"\"\n",
        "    # Extract relevant configuration details.\n",
        "    synth_config = config['synthetic_dataset_config']\n",
        "    llm_config = config['llm_config']\n",
        "\n",
        "    # --- 1. Define the Generation Plan ---\n",
        "    # Calculate how many articles are needed for each combination of domain and sentiment.\n",
        "    num_per_sentiment = synth_config['num_articles_total'] // 2\n",
        "    num_domains = len(synth_config['domain_coverage'])\n",
        "    num_per_combo = num_per_sentiment // num_domains\n",
        "\n",
        "    # Create a list of all generation tasks to be completed.\n",
        "    generation_plan = []\n",
        "    for domain in synth_config['domain_coverage']:\n",
        "        for sentiment in ['positive', 'negative']:\n",
        "            generation_plan.extend([(domain, sentiment)] * num_per_combo)\n",
        "\n",
        "    # --- 2. Handle Resumability ---\n",
        "    # Check if a partial file exists to resume from a previous run.\n",
        "    if os.path.exists(output_path):\n",
        "        results_df = pd.read_csv(output_path)\n",
        "        print(f\"Resuming generation. Found {len(results_df)} existing articles.\")\n",
        "    else:\n",
        "        results_df = pd.DataFrame()\n",
        "\n",
        "    # Filter the plan to only include tasks that have not yet been completed.\n",
        "    if not results_df.empty:\n",
        "        completed_counts = results_df.groupby(['domain', 'sentiment_label']).size()\n",
        "\n",
        "        remaining_plan = []\n",
        "        for domain, sentiment in generation_plan:\n",
        "            # Convert sentiment to label (1 for positive, 0 for negative)\n",
        "            label = 1 if sentiment == 'positive' else 0\n",
        "            completed = completed_counts.get((domain, label), 0)\n",
        "            if completed > 0:\n",
        "                completed_counts.loc[(domain, label)] -= 1\n",
        "            else:\n",
        "                remaining_plan.append((domain, sentiment))\n",
        "        generation_plan = remaining_plan\n",
        "\n",
        "    # --- 3. Iterative Generation Loop ---\n",
        "    generated_records = results_df.to_dict('records')\n",
        "\n",
        "    # The prompt asks for 3 positive and 3 negative articles, so we process in chunks.\n",
        "    # For simplicity here, we generate one article per API call.\n",
        "    for i, (domain, sentiment) in enumerate(tqdm(generation_plan, desc=\"Generating Synthetic Articles\")):\n",
        "        # Construct the prompt for the current task.\n",
        "        if domain == \"financial_markets\":\n",
        "            prompt = llm_config['appendix_prompt_financial_markets_en']\n",
        "        else:\n",
        "            template = llm_config['topic_parameterized_prompt_template_en']\n",
        "            prompt = template.format(AREA=domain)\n",
        "\n",
        "        # Modify prompt to ask for a single article for simplicity and control.\n",
        "        single_article_prompt = (\n",
        "            f\"Please generate one single business article with a clearly '{sentiment}' outlook \"\n",
        "            f\"on the topic of '{domain}'.\\n\"\n",
        "            f\"Follow all the style and content requirements from this reference prompt:\\n\\n---\\n\\n{prompt}\"\n",
        "        )\n",
        "\n",
        "        # Call the API to generate the text.\n",
        "        generated_text = _call_llm_api(client, single_article_prompt, llm_config)\n",
        "\n",
        "        # Store the result with its metadata.\n",
        "        generated_records.append({\n",
        "            'synthetic_article_id': str(uuid.uuid4()),\n",
        "            'domain': domain,\n",
        "            'sentiment_label': 1 if sentiment == 'positive' else 0,\n",
        "            'full_text': generated_text.strip(),\n",
        "            'prompt_template': 'appendix' if domain == 'financial_markets' else 'parameterized',\n",
        "        })\n",
        "\n",
        "        # Periodically save results to disk to prevent data loss.\n",
        "        if (i + 1) % 5 == 0 or (i + 1) == len(generation_plan):\n",
        "            pd.DataFrame(generated_records).to_csv(output_path, index=False)\n",
        "\n",
        "    return pd.DataFrame(generated_records)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Validate and curate synthetic outputs\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_synthetic_corpus(\n",
        "    df: pd.DataFrame,\n",
        "    word_count_range: Tuple[int, int] = (350, 550)\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the generated synthetic corpus for quality and uniqueness.\n",
        "\n",
        "    Purpose:\n",
        "    This function serves as a quality assurance check on the LLM's output. It\n",
        "    verifies that the generated articles meet the length requirements and that\n",
        "    there are no exact duplicates in the dataset.\n",
        "\n",
        "    Inputs:\n",
        "        df (pd.DataFrame): The DataFrame of generated synthetic articles.\n",
        "        word_count_range (Tuple[int, int]): The acceptable min/max word count.\n",
        "\n",
        "    Outputs:\n",
        "        (List[str]): A list of validation issue descriptions. An empty list\n",
        "                     indicates the corpus passed all checks.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # --- 1. Word Count Validation ---\n",
        "    # Calculate word count for each article.\n",
        "    df['word_count'] = df['full_text'].str.split().str.len()\n",
        "    # Find articles outside the acceptable range.\n",
        "    out_of_range = df[~df['word_count'].between(word_count_range[0], word_count_range[1])]\n",
        "    if not out_of_range.empty:\n",
        "        for _, row in out_of_range.iterrows():\n",
        "            issues.append(\n",
        "                f\"Validation Warning: Article '{row['synthetic_article_id']}' has \"\n",
        "                f\"word count of {row['word_count']}, outside range {word_count_range}.\"\n",
        "            )\n",
        "\n",
        "    # --- 2. Duplicate Validation ---\n",
        "    # Use the same fingerprinting logic from Task 4 to find exact duplicates.\n",
        "    # fingerprints = _compute_fingerprint(df['full_text'])\n",
        "    # Note: Assuming _compute_fingerprint is imported. For standalone, we replicate:\n",
        "    fingerprints = df['full_text'].str.lower().str.replace(r'\\s+', ' ', regex=True).str.strip().apply(\n",
        "        lambda x: hashlib.sha256(x.encode('utf-8')).hexdigest() if pd.notna(x) else None\n",
        "    )\n",
        "\n",
        "    if fingerprints.duplicated().any():\n",
        "        num_duplicates = fingerprints.duplicated().sum()\n",
        "        issues.append(f\"Validation Error: Found {num_duplicates} duplicate articles based on content fingerprint.\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_synthetic_articles(\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    output_path: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end generation of the synthetic article dataset.\n",
        "\n",
        "    This function manages the entire workflow: setting up the API client,\n",
        "    executing a resumable generation plan, and validating the final output for\n",
        "    quality and uniqueness.\n",
        "\n",
        "    Args:\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        output_path (str): The file path to save the final synthetic corpus CSV.\n",
        "\n",
        "    Returns:\n",
        "        (str): The path to the validated synthetic corpus file.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the final generated corpus fails validation checks.\n",
        "        ImportError: If the ANTHROPIC_API_KEY environment variable is not set.\n",
        "    \"\"\"\n",
        "    # --- 1. Configure API Client ---\n",
        "    # Retrieve API key from environment variables for security.\n",
        "    api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ImportError(\"ANTHROPIC_API_KEY environment variable not set.\")\n",
        "    client = Anthropic(api_key=api_key)\n",
        "\n",
        "    # Extract the relevant configuration section.\n",
        "    config = fused_master_input_specification['master_config']['sentiment_model_params']\n",
        "\n",
        "    # --- 2. Execute Generation Plan ---\n",
        "    # This function handles the core generation loop and is resumable.\n",
        "    synthetic_df = _execute_generation_plan(client, config, output_path)\n",
        "\n",
        "    # --- 3. Validate Final Corpus ---\n",
        "    # Perform final quality checks on the complete dataset.\n",
        "    validation_issues = _validate_synthetic_corpus(synthetic_df)\n",
        "\n",
        "    if validation_issues:\n",
        "        # If issues are found, print them and raise an error.\n",
        "        print(\"--- Synthetic Corpus Validation Failed ---\")\n",
        "        for issue in validation_issues:\n",
        "            print(issue)\n",
        "        raise ValueError(\n",
        "            \"Generated synthetic corpus failed validation. Please inspect the \"\n",
        "            f\"output file at '{output_path}', correct the issues, and re-run.\"\n",
        "        )\n",
        "\n",
        "    print(f\"Synthetic corpus successfully generated and validated. Saved to: {output_path}\")\n",
        "\n",
        "    return output_path\n"
      ],
      "metadata": {
        "id": "NVxLQg-WZxCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Embed the synthetic articles and perform UMAP diagnostic visualization\n",
        "\n",
        "# ================================================================================\n",
        "# Task 11: Embed the synthetic articles and perform UMAP diagnostic visualization\n",
        "# ================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Tokenize and embed synthetic texts with the same model\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _embed_synthetic_articles(\n",
        "    synthetic_df: pd.DataFrame,\n",
        "    model_name: str\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates embeddings for the synthetic articles using the specified model.\n",
        "\n",
        "    Purpose:\n",
        "    This function is responsible for converting the text of the 256 synthetic\n",
        "    articles into high-dimensional vectors. It is CRITICAL that this function\n",
        "    uses the exact same embedding model ('jina-embeddings-v3') as was used for\n",
        "    the main corpus to ensure that both real and synthetic articles reside in the\n",
        "    same semantic vector space.\n",
        "\n",
        "    Inputs:\n",
        "        synthetic_df (pd.DataFrame): The DataFrame containing the generated\n",
        "                                     synthetic articles, including a 'full_text' column.\n",
        "        model_name (str): The identifier of the SentenceTransformer model to use.\n",
        "\n",
        "    Processes:\n",
        "    1.  Determines the optimal computation device (GPU/CPU).\n",
        "    2.  Loads the specified SentenceTransformer model.\n",
        "    3.  Extracts the list of texts from the DataFrame.\n",
        "    4.  Calls the model's `encode` method to generate embeddings for all texts\n",
        "        in a single, efficient batch.\n",
        "\n",
        "    Outputs:\n",
        "        (np.ndarray): A NumPy array of shape (256, 1024) containing the\n",
        "                      document embeddings for the synthetic articles. The order\n",
        "                      of rows corresponds to the order of articles in the input DataFrame.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup Device and Model ---\n",
        "    # Ensure consistency by using the same device logic as the main embedding task.\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Loading embedding model '{model_name}' onto device '{device}'.\")\n",
        "\n",
        "    # Load the specified SentenceTransformer model.\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "    # --- 2. Prepare Texts ---\n",
        "    # Extract the 'full_text' column into a list for the encode method.\n",
        "    texts_to_embed = synthetic_df['full_text'].tolist()\n",
        "\n",
        "    # --- 3. Generate Embeddings ---\n",
        "    # Since the dataset is small (256 articles), we can encode it in one go.\n",
        "    print(\"Generating embeddings for synthetic articles...\")\n",
        "    synthetic_embeddings = model.encode(\n",
        "        texts_to_embed,\n",
        "        batch_size=len(texts_to_embed), # Process all in one batch\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=False\n",
        "    )\n",
        "\n",
        "    return synthetic_embeddings\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Validate embedding consistency with real article embeddings\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_embedding_consistency(\n",
        "    synthetic_embeddings: np.ndarray,\n",
        "    real_embedding_stats: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a sanity check on the quality and distribution of synthetic embeddings.\n",
        "\n",
        "    Purpose:\n",
        "    This function provides a quick diagnostic to catch gross errors in the\n",
        "    embedding process. It checks for numerical integrity (no NaNs/infs) and\n",
        "    compares summary statistics of the synthetic embeddings against those of the\n",
        "    real corpus embeddings (if provided) to flag major distributional discrepancies.\n",
        "\n",
        "    Inputs:\n",
        "        synthetic_embeddings (np.ndarray): The (256, 1024) array of embeddings.\n",
        "        real_embedding_stats (Optional[Dict]): A dictionary of summary stats from\n",
        "                                               the real corpus (from Task 6 diagnostics).\n",
        "\n",
        "    Outputs:\n",
        "        (Dict[str, Any]): A dictionary containing the summary statistics of the\n",
        "                          synthetic embeddings.\n",
        "\n",
        "    Error Handling:\n",
        "        Raises ValueError: If any non-finite values (NaN or infinity) are found.\n",
        "    \"\"\"\n",
        "    # --- 1. Numerical Integrity Check ---\n",
        "    # This is a critical check before any further computation.\n",
        "    if not np.all(np.isfinite(synthetic_embeddings)):\n",
        "        raise ValueError(\"Synthetic embeddings contain non-finite values (NaN or infinity).\")\n",
        "\n",
        "    # --- 2. Compute Summary Statistics ---\n",
        "    mean_vec = np.mean(synthetic_embeddings, axis=0)\n",
        "    std_vec = np.std(synthetic_embeddings, axis=0)\n",
        "\n",
        "    summary_stats = {\n",
        "        'num_embeddings': synthetic_embeddings.shape[0],\n",
        "        'embedding_dim': synthetic_embeddings.shape[1],\n",
        "        'global_mean_norm': np.linalg.norm(mean_vec),\n",
        "        'global_std_mean': np.mean(std_vec),\n",
        "        'min_value': np.min(synthetic_embeddings),\n",
        "        'max_value': np.max(synthetic_embeddings),\n",
        "    }\n",
        "\n",
        "    # --- 3. Comparative Reporting (if stats are provided) ---\n",
        "    if real_embedding_stats:\n",
        "        print(\"\\n--- Embedding Consistency Check ---\")\n",
        "        print(f\"{'Metric':<20} | {'Synthetic':<15} | {'Real Corpus':<15}\")\n",
        "        print(\"-\" * 55)\n",
        "        for key in ['global_mean_norm', 'global_std_mean', 'min_value', 'max_value']:\n",
        "            synth_val = summary_stats.get(key, 0)\n",
        "            real_val = real_embedding_stats.get(key, 0)\n",
        "            print(f\"{key:<20} | {synth_val:<15.4f} | {real_val:<15.4f}\")\n",
        "        print(\"-\" * 55)\n",
        "\n",
        "    return summary_stats\n",
        "\n",
        "# ---------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: (Optional diagnostic) Perform UMAP projection for Chart 2 replication\n",
        "# ---------------------------------------------------------------------------------------\n",
        "\n",
        "def _create_umap_visualization(\n",
        "    synthetic_embeddings: np.ndarray,\n",
        "    synthetic_df: pd.DataFrame,\n",
        "    umap_params: Dict[str, Any],\n",
        "    output_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Creates and saves an annotated UMAP visualization of the synthetic embeddings.\n",
        "\n",
        "    Purpose:\n",
        "    This function replicates Chart 2 from the paper. It uses the UMAP algorithm\n",
        "    to project the high-dimensional embeddings into a 2D space, allowing for\n",
        "    visual inspection of the separation between positive and negative sentiment\n",
        "    clusters. Crucially, it annotates the plot with text snippets from\n",
        "    representative articles to provide qualitative context for the clusters,\n",
        "    offering tangible evidence that the model is capturing semantic sentiment.\n",
        "\n",
        "    Inputs:\n",
        "        synthetic_embeddings (np.ndarray): The (256, 1024) embedding matrix.\n",
        "        synthetic_df (pd.DataFrame): The DataFrame of synthetic articles, containing\n",
        "                                     'sentiment_label' and 'full_text'.\n",
        "        umap_params (Dict[str, Any]): Parameters for the UMAP reducer, including\n",
        "                                      'random_state' for reproducibility.\n",
        "        output_path (str): The file path to save the generated plot (e.g., '.png').\n",
        "\n",
        "    Processes:\n",
        "    1.  Initializes the UMAP reducer with a fixed random state for reproducibility.\n",
        "    2.  Fits the reducer to the embeddings and transforms them into a 2D space.\n",
        "    3.  Creates a scatter plot of the 2D points, colored by sentiment.\n",
        "    4.  Selects one positive and one negative example article.\n",
        "    5.  Adds styled text annotations with snippets from these\n",
        "        articles, pointing to their corresponding locations in the plot.\n",
        "    6.  Saves the final, publication-quality figure to disk.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(synthetic_embeddings, np.ndarray) or synthetic_embeddings.ndim != 2:\n",
        "        raise TypeError(\"`synthetic_embeddings` must be a 2D NumPy array.\")\n",
        "    if not isinstance(synthetic_df, pd.DataFrame):\n",
        "        raise TypeError(\"`synthetic_df` must be a pandas DataFrame.\")\n",
        "    if len(synthetic_embeddings) != len(synthetic_df):\n",
        "        raise ValueError(\"Mismatch between number of embeddings and number of articles.\")\n",
        "\n",
        "    print(\"Performing UMAP projection for diagnostic visualization...\")\n",
        "    # --- 1. Initialize and Fit UMAP ---\n",
        "    # Using the random_state is critical for a reproducible plot.\n",
        "    reducer = umap.UMAP(\n",
        "        n_neighbors=umap_params.get('n_neighbors', 15),\n",
        "        min_dist=umap_params.get('min_dist', 0.1),\n",
        "        n_components=2,\n",
        "        metric='cosine', # Cosine distance is often effective for text embeddings.\n",
        "        random_state=umap_params['random_state']\n",
        "    )\n",
        "\n",
        "    # Fit the reducer to the data and transform it to 2D.\n",
        "    embedding_2d = reducer.fit_transform(synthetic_embeddings)\n",
        "\n",
        "    # --- 2. Create the Plotting DataFrame ---\n",
        "    # Combine 2D coordinates with labels for easy plotting.\n",
        "    plot_df = pd.DataFrame(embedding_2d, columns=['UMAP_1', 'UMAP_2'])\n",
        "    plot_df['Sentiment'] = synthetic_df['sentiment_label'].map({0: 'Negative Outlook', 1: 'Positive Outlook'})\n",
        "\n",
        "    # --- 3. Create the Scatter Plot ---\n",
        "    # Set up the plot aesthetics for a professional look.\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # Create the main scatter plot.\n",
        "    sns.scatterplot(\n",
        "        data=plot_df,\n",
        "        x='UMAP_1',\n",
        "        y='UMAP_2',\n",
        "        hue='Sentiment',\n",
        "        palette={'Negative Outlook': '#d62728', 'Positive Outlook': '#2ca02c'}, # Red/Green\n",
        "        ax=ax,\n",
        "        s=50,\n",
        "        alpha=0.8,\n",
        "        edgecolor='k',\n",
        "        linewidth=0.5\n",
        "    )\n",
        "\n",
        "    # --- 4. Add Annotations ---\n",
        "    # This new section adds the text snippets to replicate Chart 2.\n",
        "\n",
        "    # Deterministically select one positive and one negative example.\n",
        "    pos_example = synthetic_df[synthetic_df['sentiment_label'] == 1].iloc[0]\n",
        "    neg_example = synthetic_df[synthetic_df['sentiment_label'] == 0].iloc[0]\n",
        "\n",
        "    # Get their corresponding 2D coordinates.\n",
        "    pos_coords = embedding_2d[pos_example.name]\n",
        "    neg_coords = embedding_2d[neg_example.name]\n",
        "\n",
        "    # Create text snippets.\n",
        "    pos_snippet = ' '.join(pos_example['full_text'].split()[:20]) + '...'\n",
        "    neg_snippet = ' '.join(neg_example['full_text'].split()[:20]) + '...'\n",
        "\n",
        "    # Define styles for the annotation boxes.\n",
        "    bbox_props = dict(boxstyle=\"round,pad=0.5\", fc=\"ivory\", ec=\"gray\", lw=1, alpha=0.9)\n",
        "    arrow_props = dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0.1\", ec=\"black\")\n",
        "\n",
        "    # Add the annotation for the positive example.\n",
        "    ax.annotate(\n",
        "        pos_snippet,\n",
        "        xy=pos_coords,\n",
        "        xytext=(pos_coords[0] - 3, pos_coords[1] + 2), # Manual offset for placement\n",
        "        bbox=bbox_props,\n",
        "        arrowprops=arrow_props,\n",
        "        fontsize=9,\n",
        "        wrap=True,\n",
        "        ha='center'\n",
        "    )\n",
        "\n",
        "    # Add the annotation for the negative example.\n",
        "    ax.annotate(\n",
        "        neg_snippet,\n",
        "        xy=neg_coords,\n",
        "        xytext=(neg_coords[0] + 3, neg_coords[1] - 2), # Manual offset for placement\n",
        "        bbox=bbox_props,\n",
        "        arrowprops=arrow_props,\n",
        "        fontsize=9,\n",
        "        wrap=True,\n",
        "        ha='center'\n",
        "    )\n",
        "\n",
        "    # --- 5. Finalize and Save ---\n",
        "    # Set plot titles and labels for clarity.\n",
        "    ax.set_title(\"UMAP Projection of Synthetic Articles' Embeddings\", fontsize=16, fontweight='bold')\n",
        "    ax.set_xlabel(\"UMAP Dimension 1\", fontsize=12)\n",
        "    ax.set_ylabel(\"UMAP Dimension 2\", fontsize=12)\n",
        "    ax.legend(title='Economic Outlook', title_fontsize='13', fontsize='11')\n",
        "\n",
        "    # Save the figure to the specified path with high resolution.\n",
        "    fig.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"UMAP visualization with annotations saved to: {output_path}\")\n",
        "    # Close the figure to free up memory.\n",
        "    plt.close(fig)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def process_synthetic_embeddings(\n",
        "    synthetic_corpus_path: str,\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    output_directory: str,\n",
        "    real_embedding_diagnostics: Optional[Dict[str, Any]] = None\n",
        ") -> Tuple[np.ndarray, pd.Series, str]:\n",
        "    \"\"\"\n",
        "    Orchestrates the embedding and visualization of the synthetic corpus.\n",
        "\n",
        "    This function manages the workflow of loading the synthetic articles,\n",
        "    generating their embeddings using the same model as the main corpus,\n",
        "    validating their quality, and producing a diagnostic UMAP visualization.\n",
        "\n",
        "    Args:\n",
        "        synthetic_corpus_path (str): Path to the CSV of synthetic articles.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        output_directory (str): Directory to save the UMAP plot.\n",
        "        real_embedding_diagnostics (Optional[Dict]): The diagnostic report from\n",
        "            Task 6, used for comparative validation.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - (np.ndarray): The (256, 1024) matrix of synthetic embeddings (Z).\n",
        "        - (pd.Series): The corresponding sentiment labels (y).\n",
        "        - (str): The path to the saved UMAP visualization plot.\n",
        "    \"\"\"\n",
        "    # --- 1. Load Data ---\n",
        "    # Load the previously generated synthetic articles.\n",
        "    synthetic_df = pd.read_csv(synthetic_corpus_path)\n",
        "\n",
        "    # Extract configuration parameters.\n",
        "    embedding_config = fused_master_input_specification['master_config']['embedding_params']\n",
        "    umap_config = fused_master_input_specification['master_config']['umap_diagnostic_params']\n",
        "\n",
        "    # --- Step 1: Generate Embeddings ---\n",
        "    # Ensure the same model is used as for the main corpus.\n",
        "    synthetic_embeddings = _embed_synthetic_articles(\n",
        "        synthetic_df,\n",
        "        embedding_config['model_name']\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Validate Embedding Quality ---\n",
        "    # Perform a sanity check on the generated embeddings.\n",
        "    _validate_embedding_consistency(\n",
        "        synthetic_embeddings,\n",
        "        real_embedding_diagnostics.get('summary_stats') if real_embedding_diagnostics else None\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Create UMAP Visualization ---\n",
        "    # Define the output path for the plot.\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    umap_plot_path = os.path.join(output_directory, 'chart2_umap_synthetic_embeddings.png')\n",
        "\n",
        "    # Generate and save the plot.\n",
        "    _create_umap_visualization(\n",
        "        synthetic_embeddings=synthetic_embeddings,\n",
        "        labels=synthetic_df['sentiment_label'],\n",
        "        umap_params=umap_config,\n",
        "        output_path=umap_plot_path\n",
        "    )\n",
        "\n",
        "    # --- 4. Prepare and Return Final Outputs ---\n",
        "    # The labels corresponding to the embeddings.\n",
        "    synthetic_labels = synthetic_df['sentiment_label']\n",
        "\n",
        "    return synthetic_embeddings, synthetic_labels, umap_plot_path\n"
      ],
      "metadata": {
        "id": "Z-m80J7MbGfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Train L2-regularized logistic regression on synthetic embeddings via cross-validation\n",
        "\n",
        "# ===============================================================================================\n",
        "# Task 12: Train L2-regularized logistic regression on synthetic embeddings via cross-validation\n",
        "# ===============================================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------\n",
        "# Task 12, Step 2 (Combined with Step 1): Perform cross-validation to select optimal C\n",
        "# -------------------------------------------------------------------------------------\n",
        "\n",
        "def _find_optimal_regularization(\n",
        "    X: np.ndarray,\n",
        "    y: pd.Series,\n",
        "    config: Dict[str, Any],\n",
        "    seed: int\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs stratified K-fold cross-validation to find the optimal C.\n",
        "\n",
        "    Purpose:\n",
        "    In a high-dimensional setting (p > n), regularization is essential to prevent\n",
        "    overfitting. This function systematically searches for the best regularization\n",
        "    strength by evaluating model performance across a grid of `C` values (where\n",
        "    C is the inverse of the regularization strength lambda). It uses stratified\n",
        "    K-fold cross-validation to ensure that each fold maintains the same class\n",
        "    balance as the overall dataset, which is critical for reliable performance\n",
        "    estimation on a small, balanced dataset.\n",
        "\n",
        "    Inputs:\n",
        "        X (np.ndarray): The (256, 1024) matrix of synthetic embeddings.\n",
        "        y (pd.Series): The (256,) series of binary sentiment labels.\n",
        "        config (Dict[str, Any]): The 'classifier_config' sub-dictionary.\n",
        "        seed (int): The global random seed for reproducible shuffling in CV.\n",
        "\n",
        "    Processes:\n",
        "    1.  Initializes a LogisticRegression model with L2 penalty as specified.\n",
        "    2.  Defines the hyperparameter grid for 'C' from the config.\n",
        "    3.  Sets up a StratifiedKFold cross-validator for robust evaluation.\n",
        "    4.  Initializes and fits GridSearchCV to perform the exhaustive search.\n",
        "    5.  Extracts the best hyperparameter and the full cross-validation results.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (float): The optimal value for the hyperparameter C.\n",
        "        - (Dict[str, Any]): A dictionary containing the detailed cross-validation\n",
        "          results from GridSearchCV.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialize the Model (Step 1 logic) ---\n",
        "    # The model is L2-regularized (Ridge) logistic regression.\n",
        "    # The solver 'lbfgs' is efficient and supports L2 penalty.\n",
        "    # The random_state ensures deterministic behavior if the solver uses randomness.\n",
        "    log_reg = LogisticRegression(\n",
        "        penalty=config['penalty'],\n",
        "        solver=config['solver'],\n",
        "        random_state=seed,\n",
        "        max_iter=1000 # Increase max_iter for convergence on high-dim data\n",
        "    )\n",
        "\n",
        "    # --- 2. Define the Cross-Validation Strategy ---\n",
        "    # Stratified K-Fold is essential for maintaining class balance in each fold.\n",
        "    # Shuffling with a random_state ensures the fold splits are reproducible.\n",
        "    cv_strategy = StratifiedKFold(\n",
        "        n_splits=config['cross_validation_folds'],\n",
        "        shuffle=True,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    # --- 3. Set up and Run Grid Search ---\n",
        "    # GridSearchCV automates the process of training and evaluating the model\n",
        "    # for each hyperparameter combination across all CV folds.\n",
        "    print(\"Starting GridSearchCV to find optimal regularization strength 'C'...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=log_reg,\n",
        "        param_grid=config['hyperparameter_grid'],\n",
        "        cv=cv_strategy,\n",
        "        scoring='neg_log_loss', # Log-loss is a proper scoring rule for probabilistic models.\n",
        "        verbose=1,\n",
        "        n_jobs=-1 # Use all available CPU cores.\n",
        "    )\n",
        "\n",
        "    # Fit the grid search to the synthetic data.\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    # --- 4. Extract and Report Results ---\n",
        "    # The best hyperparameter found during the search.\n",
        "    optimal_c = grid_search.best_params_['C']\n",
        "    print(f\"GridSearchCV complete. Optimal C = {optimal_c}\")\n",
        "    print(f\"Best cross-validation score (neg_log_loss): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    return optimal_c, grid_search.cv_results_\n",
        "\n",
        "# ------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Fit the final model on the full synthetic dataset and persist parameters\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def _fit_and_persist_final_model(\n",
        "    X: np.ndarray,\n",
        "    y: pd.Series,\n",
        "    optimal_c: float,\n",
        "    config: Dict[str, Any],\n",
        "    seed: int,\n",
        "    output_path: str\n",
        ") -> LogisticRegression:\n",
        "    \"\"\"\n",
        "    Fits the final logistic regression model and saves it to disk.\n",
        "\n",
        "    Purpose:\n",
        "    After identifying the optimal hyperparameter via cross-validation, this\n",
        "    function trains a new model on the *entire* synthetic dataset. This approach\n",
        "    leverages all available data to obtain the best possible estimates for the\n",
        "    model's coefficients. The final, trained model object is then serialized\n",
        "    to disk for later use in the inference pipeline.\n",
        "\n",
        "    Inputs:\n",
        "        X (np.ndarray): The full (256, 1024) matrix of synthetic embeddings.\n",
        "        y (pd.Series): The full (256,) series of binary sentiment labels.\n",
        "        optimal_c (float): The best value for C found via cross-validation.\n",
        "        config (Dict[str, Any]): The 'classifier_config' sub-dictionary.\n",
        "        seed (int): The global random seed for reproducibility.\n",
        "        output_path (str): The file path to save the trained model.\n",
        "\n",
        "    Outputs:\n",
        "        (LogisticRegression): The trained scikit-learn model object.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialize the Final Model ---\n",
        "    # Instantiate the model with the exact same parameters as before, but now\n",
        "    # with the optimal 'C' value determined by the grid search.\n",
        "    final_model = LogisticRegression(\n",
        "        penalty=config['penalty'],\n",
        "        solver=config['solver'],\n",
        "        C=optimal_c,\n",
        "        random_state=seed,\n",
        "        max_iter=1000\n",
        "    )\n",
        "\n",
        "    # --- 2. Fit on the Full Dataset ---\n",
        "    # Train the model on all 256 synthetic samples.\n",
        "    print(f\"Fitting final model on the full synthetic dataset with C={optimal_c}...\")\n",
        "    final_model.fit(X, y)\n",
        "    print(\"Final model fitting complete.\")\n",
        "\n",
        "    # --- 3. Persist the Model ---\n",
        "    # Save the trained model object to the specified path using joblib.\n",
        "    # This is the standard and most robust way to save scikit-learn models.\n",
        "    joblib.dump(final_model, output_path)\n",
        "    print(f\"Final sentiment classifier model saved to: {output_path}\")\n",
        "\n",
        "    return final_model\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_sentiment_classifier(\n",
        "    synthetic_embeddings: np.ndarray,\n",
        "    synthetic_labels: pd.Series,\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    output_path: str\n",
        ") -> Tuple[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the training of the L2-regularized sentiment classifier.\n",
        "\n",
        "    This function manages the complete workflow for training the sentiment model:\n",
        "    1. It performs a cross-validated grid search to find the optimal\n",
        "       regularization strength.\n",
        "    2. It then trains a final model on the entire synthetic dataset using this\n",
        "       optimal hyperparameter.\n",
        "    3. It saves the final trained model to disk for downstream inference tasks.\n",
        "\n",
        "    Args:\n",
        "        synthetic_embeddings (np.ndarray): The (256, 1024) matrix of embeddings.\n",
        "        synthetic_labels (pd.Series): The (256,) series of sentiment labels.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        output_path (str): The file path to save the final trained model.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - (str): The path to the saved model file.\n",
        "        - (Dict[str, Any]): A results dictionary containing the optimal 'C' and\n",
        "          the detailed cross-validation results.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(synthetic_embeddings, np.ndarray) or synthetic_embeddings.shape != (256, 1024):\n",
        "        raise ValueError(\"`synthetic_embeddings` must be a NumPy array of shape (256, 1024).\")\n",
        "    if not isinstance(synthetic_labels, pd.Series) or len(synthetic_labels) != 256:\n",
        "        raise ValueError(\"`synthetic_labels` must be a pandas Series of length 256.\")\n",
        "\n",
        "    # --- Extract Configuration ---\n",
        "    classifier_config = fused_master_input_specification['master_config']['sentiment_model_params']['classifier_config']\n",
        "    seed = fused_master_input_specification['master_config']['reproducibility']['random_seeds']['global_seed']\n",
        "\n",
        "    # --- Step 1 & 2: Find Optimal Regularization via CV ---\n",
        "    optimal_c, cv_results = _find_optimal_regularization(\n",
        "        X=synthetic_embeddings,\n",
        "        y=synthetic_labels,\n",
        "        config=classifier_config,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Fit and Persist the Final Model ---\n",
        "    _fit_and_persist_final_model(\n",
        "        X=synthetic_embeddings,\n",
        "        y=synthetic_labels,\n",
        "        optimal_c=optimal_c,\n",
        "        config=classifier_config,\n",
        "        seed=seed,\n",
        "        output_path=output_path\n",
        "    )\n",
        "\n",
        "    # --- 4. Package and Return Results ---\n",
        "    # Create a comprehensive results dictionary for logging and analysis.\n",
        "    results = {\n",
        "        \"model_path\": output_path,\n",
        "        \"optimal_c\": optimal_c,\n",
        "        \"cv_results\": {k: v.tolist() for k, v in cv_results.items()} # Convert numpy arrays for serialization\n",
        "    }\n",
        "\n",
        "    return output_path, results\n"
      ],
      "metadata": {
        "id": "_PNU8ws6dR14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Score all relevant real articles with the trained sentiment model\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Score all relevant real articles with the trained sentiment model\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Steps 1 & 2: Load model, retrieve embeddings, and compute probabilities\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_sentiment_inference_in_batches(\n",
        "    model: LogisticRegression,\n",
        "    embeddings_path: str,\n",
        "    relevant_indices: np.ndarray,\n",
        "    inference_batch_size: int = 16384\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Scores relevant article embeddings in batches using the trained sentiment model.\n",
        "\n",
        "    Purpose:\n",
        "    This function applies the trained logistic regression model to the embeddings\n",
        "    of all economics-relevant articles. To handle a potentially large number of\n",
        "    articles (millions) without exhausting system memory, it reads the required\n",
        "    embeddings from the HDF5 file and performs prediction in manageable batches.\n",
        "\n",
        "    Inputs:\n",
        "        model (LogisticRegression): The loaded, trained scikit-learn model object.\n",
        "        embeddings_path (str): The path to the HDF5 file containing the full\n",
        "                               embedding matrix for the entire corpus.\n",
        "        relevant_indices (np.ndarray): A sorted NumPy array of integer row indices\n",
        "                                       specifying which embeddings to score.\n",
        "        inference_batch_size (int): The number of embeddings to process in a\n",
        "                                    single prediction batch.\n",
        "\n",
        "    Processes:\n",
        "    1.  Opens the HDF5 embeddings file.\n",
        "    2.  Iterates through the `relevant_indices` array in chunks of size\n",
        "        `inference_batch_size`.\n",
        "    3.  For each chunk of indices, it reads the corresponding embedding vectors\n",
        "        from the HDF5 file. This is an efficient, non-contiguous read.\n",
        "    4.  Calls `model.predict_proba()` on the batch of embeddings to get the\n",
        "        sentiment probabilities for both classes.\n",
        "    5.  Selects the probability for the positive class (class 1).\n",
        "    6.  Collects the probabilities from all batches.\n",
        "    7.  Concatenates the batch results into a single, final NumPy array.\n",
        "\n",
        "    Outputs:\n",
        "        (np.ndarray): A 1D NumPy array of sentiment probabilities (`p_pos`),\n",
        "                      ordered to correspond to the `relevant_indices` array.\n",
        "    \"\"\"\n",
        "    # This list will store the probability arrays from each processed batch.\n",
        "    all_probabilities: List[np.ndarray] = []\n",
        "\n",
        "    # Open the HDF5 file for reading.\n",
        "    with h5py.File(embeddings_path, 'r') as f:\n",
        "        embeddings_dset = f['embeddings']\n",
        "\n",
        "        # Iterate through the sorted list of relevant indices in batches.\n",
        "        for i in tqdm(range(0, len(relevant_indices), inference_batch_size), desc=\"Scoring article sentiment\"):\n",
        "            # Get the indices for the current batch.\n",
        "            batch_indices = relevant_indices[i:i + inference_batch_size]\n",
        "\n",
        "            # Read the corresponding embedding vectors for this batch.\n",
        "            # HDF5 is highly optimized for this type of indexed slicing.\n",
        "            batch_embeddings = embeddings_dset[batch_indices]\n",
        "\n",
        "            # Use predict_proba to get the probability for each class.\n",
        "            # Equation: p_i = 1 / (1 + exp(-(w^T x_i + b)))\n",
        "            batch_probs_all_classes = model.predict_proba(batch_embeddings)\n",
        "\n",
        "            # We only need the probability of the positive class (class 1).\n",
        "            # The columns are ordered [class_0_prob, class_1_prob].\n",
        "            batch_probs_positive_class = batch_probs_all_classes[:, 1]\n",
        "\n",
        "            # Append the results for this batch to our list.\n",
        "            all_probabilities.append(batch_probs_positive_class)\n",
        "\n",
        "    # Concatenate the list of batch arrays into a single, large NumPy array.\n",
        "    full_probability_vector = np.concatenate(all_probabilities)\n",
        "\n",
        "    return full_probability_vector\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Validate score distribution and persist for downstream aggregation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _assemble_and_persist_scored_articles(\n",
        "    df_relevant: pd.DataFrame,\n",
        "    probabilities: np.ndarray,\n",
        "    output_path: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Assembles the final scored articles DataFrame, validates it, and saves it.\n",
        "\n",
        "    Purpose:\n",
        "    This function combines the metadata of the relevant articles with their newly\n",
        "    computed sentiment scores. It performs critical validation checks on the scores\n",
        "    and persists the final, enriched DataFrame to a performant file format. This\n",
        "    artifact is the direct input for the final indicator aggregation stage.\n",
        "\n",
        "    Inputs:\n",
        "        df_relevant (pd.DataFrame): The DataFrame containing metadata for only\n",
        "                                    the relevant articles.\n",
        "        probabilities (np.ndarray): The 1D array of sentiment scores, ordered to\n",
        "                                    match the rows in `df_relevant`.\n",
        "        output_path (str): The file path to save the final scored articles data.\n",
        "\n",
        "    Processes:\n",
        "    1.  Creates a new DataFrame from `df_relevant` to avoid modifying the original.\n",
        "    2.  Assigns the `probabilities` array to a new column, 'p_pos'.\n",
        "    3.  Validates that all scores are non-null and fall within the [0, 1] range.\n",
        "    4.  Prints descriptive statistics of the score distribution as a diagnostic.\n",
        "    5.  Saves the validated DataFrame to the specified path using the Feather format.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): The final, validated DataFrame of scored articles.\n",
        "    \"\"\"\n",
        "    # --- 1. Assemble the DataFrame ---\n",
        "    # Create a copy to ensure the original df_relevant is not modified.\n",
        "    scored_articles_df = df_relevant.copy()\n",
        "    # Assign the computed probabilities. The order is guaranteed by the orchestrator.\n",
        "    scored_articles_df['p_pos'] = probabilities\n",
        "\n",
        "    # --- 2. Validate the Scores ---\n",
        "    # Check for any null probabilities, which would indicate an error.\n",
        "    if scored_articles_df['p_pos'].isna().any():\n",
        "        raise ValueError(\"Sentiment scores contain unexpected null values.\")\n",
        "    # Check that all probabilities are within the valid [0, 1] range.\n",
        "    if not scored_articles_df['p_pos'].between(0, 1).all():\n",
        "        raise ValueError(\"Sentiment scores contain values outside the valid [0, 1] range.\")\n",
        "\n",
        "    # --- 3. Diagnostic Reporting ---\n",
        "    # Print summary statistics for the sentiment score distribution.\n",
        "    print(\"\\n--- Sentiment Score Distribution Summary ---\")\n",
        "    print(scored_articles_df['p_pos'].describe())\n",
        "    print(\"----------------------------------------\")\n",
        "\n",
        "    # --- 4. Persist the DataFrame ---\n",
        "    # Select key columns for the final artifact.\n",
        "    columns_to_save = [\n",
        "        'article_id', 'publication_datetime_utc', 'language', 'outlet_id', 'p_pos'\n",
        "    ]\n",
        "    # Save to Feather format for fast read/write operations.\n",
        "    scored_articles_df[columns_to_save].to_feather(output_path)\n",
        "    print(f\"Scored articles DataFrame saved to: {output_path}\")\n",
        "\n",
        "    return scored_articles_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def score_relevant_articles(\n",
        "    df_relevant: pd.DataFrame,\n",
        "    sentiment_model_path: str,\n",
        "    embeddings_path: str,\n",
        "    crosswalk_path: str,\n",
        "    output_path: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the sentiment scoring of all relevant articles.\n",
        "\n",
        "    This function manages the end-to-end workflow of applying the trained\n",
        "    sentiment classifier to the embeddings of all economics-relevant articles.\n",
        "    It handles loading the model, retrieving the correct data in batches,\n",
        "    running inference, and saving the final, validated output.\n",
        "\n",
        "    Args:\n",
        "        df_relevant (pd.DataFrame): DataFrame of metadata for relevant articles (from Task 9).\n",
        "        sentiment_model_path (str): Path to the trained sentiment classifier model.\n",
        "        embeddings_path (str): Path to the HDF5 file of all embeddings.\n",
        "        crosswalk_path (str): Path to the CSV crosswalk table.\n",
        "        output_path (str): File path to save the final scored articles data.\n",
        "\n",
        "    Returns:\n",
        "        (str): The path to the saved file of scored articles.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Load Model and Retrieve Embedding Indices ---\n",
        "    # Load the trained scikit-learn model from disk.\n",
        "    print(f\"Loading sentiment classifier from: {sentiment_model_path}\")\n",
        "    model: LogisticRegression = joblib.load(sentiment_model_path)\n",
        "\n",
        "    # Load the crosswalk and filter it to get the HDF5 row indices for our relevant articles.\n",
        "    crosswalk_df = pd.read_csv(crosswalk_path)\n",
        "    relevant_ids = set(df_relevant['article_id'])\n",
        "    relevant_crosswalk = crosswalk_df[crosswalk_df['article_id'].isin(relevant_ids)]\n",
        "\n",
        "    # IMPORTANT: Sort the df_relevant and the indices to ensure perfect alignment.\n",
        "    # This creates a canonical order that will be preserved through scoring.\n",
        "    df_relevant_sorted = df_relevant.set_index('article_id').loc[relevant_crosswalk['article_id']].reset_index()\n",
        "    relevant_indices_sorted = relevant_crosswalk['embedding_row_index'].values\n",
        "\n",
        "    # --- Step 2: Compute Article-Level Sentiment Probabilities in Batches ---\n",
        "    probabilities = _run_sentiment_inference_in_batches(\n",
        "        model=model,\n",
        "        embeddings_path=embeddings_path,\n",
        "        relevant_indices=relevant_indices_sorted\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Assemble, Validate, and Persist the Final DataFrame ---\n",
        "    _assemble_and_persist_scored_articles(\n",
        "        df_relevant=df_relevant_sorted,\n",
        "        probabilities=probabilities,\n",
        "        output_path=output_path\n",
        "    )\n",
        "\n",
        "    return output_path\n"
      ],
      "metadata": {
        "id": "lpjG0SqXe1Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Construct monthly NEOS and early-release variants by temporal aggregation\n",
        "\n",
        "# ===================================================================================\n",
        "# Task 14: Construct monthly NEOS and early-release variants by temporal aggregation\n",
        "# ===================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Helper Function for Aggregation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _aggregate_scores_to_monthly(\n",
        "    df: pd.DataFrame,\n",
        "    suffix: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates article-level scores to a monthly time series.\n",
        "\n",
        "    Purpose:\n",
        "    This is a core helper function that takes a DataFrame of scored articles\n",
        "    and computes the monthly average sentiment score (the indicator value) and\n",
        "    the total number of articles for that month. It handles months with no\n",
        "    articles by producing NaNs, ensuring a continuous time series index.\n",
        "\n",
        "    Inputs:\n",
        "        df (pd.DataFrame): A DataFrame containing 'publication_datetime_utc' and 'p_pos'.\n",
        "        suffix (str): A suffix to append to the output column names (e.g., '_baseline').\n",
        "\n",
        "    Processes:\n",
        "    1.  Sets 'publication_datetime_utc' as the index.\n",
        "    2.  Groups the DataFrame by month-start frequency ('MS').\n",
        "    3.  For each month, it calculates the mean of 'p_pos' and the count of articles.\n",
        "    4.  Renames the columns with the provided suffix for clarity.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A monthly time series DataFrame with columns for the\n",
        "                        indicator value and the article count.\n",
        "    \"\"\"\n",
        "    # Return an empty DataFrame if the input is empty.\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Set the timestamp as the index for time-based grouping.\n",
        "    df_indexed = df.set_index('publication_datetime_utc')\n",
        "\n",
        "    # Group by month-start frequency and apply aggregations.\n",
        "    # 'mean' calculates the NEOS value.\n",
        "    # 'size' calculates the number of articles in the group.\n",
        "    monthly_agg = df_indexed.groupby(pd.Grouper(freq='MS')).agg(\n",
        "        indicator_value=('p_pos', 'mean'),\n",
        "        article_count=('p_pos', 'size')\n",
        "    )\n",
        "\n",
        "    # Rename columns to be descriptive using the provided suffix.\n",
        "    monthly_agg.rename(columns={\n",
        "        'indicator_value': f'NEOS{suffix}',\n",
        "        'article_count': f'count{suffix}'\n",
        "    }, inplace=True)\n",
        "\n",
        "    return monthly_agg\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Steps 1 & 2: Compute baseline and early-release NEOS variants\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_monthly_indicators(\n",
        "    scored_articles_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs the baseline monthly NEOS and all early-release variants.\n",
        "\n",
        "    Purpose:\n",
        "    This function generates the main set of monthly time-series indicators. It\n",
        "    calculates the full-month baseline NEOS and then iteratively calculates the\n",
        "    timelier variants based on articles published within the first 7, 14, and\n",
        "    21 days of each month.\n",
        "\n",
        "    Inputs:\n",
        "        scored_articles_df (pd.DataFrame): The DataFrame of all scored relevant articles.\n",
        "        config (Dict[str, Any]): The 'aggregation_params' sub-dictionary.\n",
        "\n",
        "    Processes:\n",
        "    1.  Derives 'day_of_month' from the publication timestamp.\n",
        "    2.  Calls the aggregation helper on the full dataset to get the baseline NEOS.\n",
        "    3.  Loops through the specified early-release day cutoffs (e.g., [7, 14, 21]).\n",
        "    4.  In each loop, it filters the articles for that window and calls the\n",
        "        aggregation helper.\n",
        "    5.  Joins all resulting time series into a single, comprehensive monthly DataFrame.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A DataFrame indexed by month, with columns for each\n",
        "                        NEOS variant and its corresponding article count.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Data ---\n",
        "    # The 'day_of_month' is needed for filtering early-release windows.\n",
        "    df = scored_articles_df.copy()\n",
        "    df['day_of_month'] = df['publication_datetime_utc'].dt.day\n",
        "\n",
        "    # --- 2. Compute Baseline NEOS (Step 1) ---\n",
        "    # This is the main indicator using all articles in a given month.\n",
        "    # Equation: NEOS_m = (1/|I_m|) * sum_{i in I_m} p_i\n",
        "    baseline_neos = _aggregate_scores_to_monthly(df, suffix='_baseline')\n",
        "\n",
        "    # --- 3. Compute Early-Release Variants (Step 2) ---\n",
        "    # This list will hold the DataFrames for each variant.\n",
        "    all_indicators = [baseline_neos]\n",
        "    early_release_days = config.get('early_release_variants_days', [7, 14, 21])\n",
        "\n",
        "    # Loop through each specified cutoff day.\n",
        "    for k in early_release_days:\n",
        "        # Equation: NEOS_m^(k) = (1/|I_m^(<=k)|) * sum_{i in I_m^(<=k)} p_i\n",
        "        # Filter the DataFrame to include only articles published on or before day k.\n",
        "        df_filtered = df[df['day_of_month'] <= k]\n",
        "\n",
        "        # Call the aggregation helper on the filtered subset.\n",
        "        early_release_neos = _aggregate_scores_to_monthly(df_filtered, suffix=f'_{k}d')\n",
        "\n",
        "        # Add the resulting time series to our list.\n",
        "        all_indicators.append(early_release_neos)\n",
        "\n",
        "    # --- 4. Combine All Indicators ---\n",
        "    # Join all the monthly time series together on their common DatetimeIndex.\n",
        "    # The join='outer' ensures we keep all months from all series.\n",
        "    final_monthly_df = pd.concat(all_indicators, axis=1)\n",
        "\n",
        "    # Fill NaN counts with 0 for clarity. NEOS values remain NaN for empty months.\n",
        "    count_cols = [col for col in final_monthly_df.columns if 'count' in col]\n",
        "    final_monthly_df[count_cols] = final_monthly_df[count_cols].fillna(0).astype(int)\n",
        "\n",
        "    return final_monthly_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: (Optional) Construct daily month-to-date NEOS\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_daily_mtd_indicator(\n",
        "    scored_articles_df: pd.DataFrame\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Constructs a daily, month-to-date (MTD) version of the NEOS indicator.\n",
        "\n",
        "    Purpose:\n",
        "    This function generates the high-frequency diagnostic series required to\n",
        "    replicate Chart 4. For each day, it calculates the average sentiment of all\n",
        "    articles published so far within that month. This provides a real-time view\n",
        "    of how the indicator evolves intra-month.\n",
        "\n",
        "    Inputs:\n",
        "        scored_articles_df (pd.DataFrame): The DataFrame of all scored relevant articles.\n",
        "\n",
        "    Outputs:\n",
        "        (Optional[pd.DataFrame]): A daily time series DataFrame with the MTD NEOS\n",
        "                                  and cumulative article count, or None if the\n",
        "                                  input is empty.\n",
        "    \"\"\"\n",
        "    if scored_articles_df.empty:\n",
        "        return None\n",
        "\n",
        "    # Sort by time to ensure correct cumulative calculations.\n",
        "    df = scored_articles_df.sort_values('publication_datetime_utc').copy()\n",
        "\n",
        "    # Create a 'year_month' column for grouping.\n",
        "    df['year_month'] = df['publication_datetime_utc'].dt.to_period('M')\n",
        "\n",
        "    # --- Calculate Cumulative Values Within Each Month ---\n",
        "    # Group by month and then calculate the cumulative sum of scores and a cumulative count.\n",
        "    df['cumulative_score_sum'] = df.groupby('year_month')['p_pos'].cumsum()\n",
        "    df['cumulative_article_count'] = df.groupby('year_month').cumcount() + 1\n",
        "\n",
        "    # The MTD NEOS at the time of each article is the cumulative sum / cumulative count.\n",
        "    df['NEOS_mtd_at_article_time'] = df['cumulative_score_sum'] / df['cumulative_article_count']\n",
        "\n",
        "    # --- Convert to a Daily Series ---\n",
        "    # Set the timestamp as the index.\n",
        "    df_indexed = df.set_index('publication_datetime_utc')\n",
        "\n",
        "    # Create a daily series by taking the *last* calculated MTD value for each day.\n",
        "    daily_series = df_indexed.resample('D').last()\n",
        "\n",
        "    # Forward-fill the values to carry the last known MTD value over days with no new articles.\n",
        "    daily_series_ffilled = daily_series[['NEOS_mtd_at_article_time', 'cumulative_article_count']].ffill()\n",
        "\n",
        "    # Rename for clarity.\n",
        "    daily_series_ffilled.rename(columns={\n",
        "        'NEOS_mtd_at_article_time': 'NEOS_mtd',\n",
        "        'cumulative_article_count': 'count_mtd'\n",
        "    }, inplace=True)\n",
        "\n",
        "    return daily_series_ffilled\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_neos_indicators(\n",
        "    scored_articles_path: str,\n",
        "    fused_master_input_specification: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the temporal aggregation of article scores into NEOS indicators.\n",
        "\n",
        "    This function loads the article-level sentiment scores and manages the\n",
        "    process of creating all required time-series indicators: the baseline monthly\n",
        "    NEOS, the early-release variants, and the optional daily month-to-date series\n",
        "    for diagnostic purposes.\n",
        "\n",
        "    Args:\n",
        "        scored_articles_path (str): Path to the Feather file of scored articles.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): A DataFrame of all monthly NEOS indicators and counts.\n",
        "        - (Optional[pd.DataFrame]): A DataFrame of the daily MTD indicator, or\n",
        "          None if it was not computed.\n",
        "    \"\"\"\n",
        "    # --- 1. Load Data ---\n",
        "    # Load the output from the previous sentiment scoring task.\n",
        "    print(f\"Loading scored articles from: {scored_articles_path}\")\n",
        "    scored_articles_df = pd.read_feather(scored_articles_path)\n",
        "\n",
        "    # Extract the relevant configuration section.\n",
        "    agg_config = fused_master_input_specification['master_config']['aggregation_params']\n",
        "\n",
        "    # --- 2. Construct Monthly Indicators (Steps 1 & 2) ---\n",
        "    # This function handles both the baseline and all early-release variants.\n",
        "    monthly_indicators_df = _construct_monthly_indicators(scored_articles_df, agg_config)\n",
        "\n",
        "    # --- 3. Construct Daily Diagnostic Indicator (Step 3, Optional) ---\n",
        "    # This is controlled by a flag in the config.\n",
        "    daily_indicator_df = None\n",
        "    if agg_config.get('month_to_date_series', False):\n",
        "        print(\"Constructing daily month-to-date diagnostic series...\")\n",
        "        daily_indicator_df = _construct_daily_mtd_indicator(scored_articles_df)\n",
        "\n",
        "    return monthly_indicators_df, daily_indicator_df\n"
      ],
      "metadata": {
        "id": "ZikGKiy-u6kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Align monthly NEOS to quarterly predictors and prepare comparator indicators\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 15: Align monthly NEOS to quarterly predictors and prepare comparator indicators\n",
        "# ======================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Helper Function for Temporal Alignment\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _align_monthly_to_quarterly(\n",
        "    monthly_df: pd.DataFrame,\n",
        "    month_of_quarter: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aligns a monthly time series to a quarterly frequency by selecting a specific month.\n",
        "\n",
        "    Purpose:\n",
        "    This function implements the core \"information set\" logic. Instead of\n",
        "    averaging, it selects the data from a specific month within each quarter\n",
        "    (e.g., the 2nd month) to represent the entire quarter. This simulates the\n",
        "    data available to a forecaster at a specific point in time.\n",
        "\n",
        "    Inputs:\n",
        "        monthly_df (pd.DataFrame): A DataFrame with a monthly DatetimeIndex ('MS').\n",
        "        month_of_quarter (int): The month to select from each quarter (1, 2, or 3).\n",
        "\n",
        "    Processes:\n",
        "    1.  Adds a 'quarter' column, mapping each month to its quarter's start date.\n",
        "    2.  Adds a 'month_of_quarter' column (1, 2, or 3).\n",
        "    3.  Filters the DataFrame to keep only the rows for the specified month of each quarter.\n",
        "    4.  Sets the 'quarter' column as the new DatetimeIndex, resulting in a\n",
        "        quarterly time series.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A new DataFrame with a quarterly DatetimeIndex ('QS-JAN').\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(monthly_df.index, pd.DatetimeIndex) or monthly_df.index.freqstr != 'MS':\n",
        "        raise ValueError(\"Input `monthly_df` must have a DatetimeIndex with 'MS' frequency.\")\n",
        "    if month_of_quarter not in [1, 2, 3]:\n",
        "        raise ValueError(\"`month_of_quarter` must be 1, 2, or 3.\")\n",
        "\n",
        "    # Create a working copy.\n",
        "    df = monthly_df.copy()\n",
        "\n",
        "    # --- 1. Map Months to Quarters ---\n",
        "    # This creates a timestamp for the first day of the quarter for each monthly observation.\n",
        "    df['quarter'] = df.index.to_period('Q').to_timestamp()\n",
        "\n",
        "    # --- 2. Identify Month within Quarter ---\n",
        "    # The formula (month - 1) % 3 + 1 correctly maps [1..12] to [1,2,3,1,2,3,...].\n",
        "    df['month_of_quarter'] = (df.index.month - 1) % 3 + 1\n",
        "\n",
        "    # --- 3. Filter for the Selected Month ---\n",
        "    # Keep only the rows corresponding to the desired month of the quarter.\n",
        "    quarterly_df = df[df['month_of_quarter'] == month_of_quarter].copy()\n",
        "\n",
        "    # --- 4. Set the New Quarterly Index ---\n",
        "    # Set the 'quarter' column as the index and drop the helper columns.\n",
        "    quarterly_df = quarterly_df.set_index('quarter').drop(\n",
        "        columns=['month_of_quarter']\n",
        "    )\n",
        "    # Ensure the new index has the correct frequency attribute.\n",
        "    quarterly_df.index.freq = 'QS-JAN'\n",
        "\n",
        "    return quarterly_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Steps 1, 2, & 3: Align all series and merge into final dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_forecasting_dataset(\n",
        "    monthly_neos_df: pd.DataFrame,\n",
        "    monthly_indicator_df: pd.DataFrame,\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    fused_master_input_specification: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs the final quarterly dataset for the forecasting exercise.\n",
        "\n",
        "    Purpose:\n",
        "    This orchestrator function is the final data assembly step. It takes all\n",
        "    monthly and quarterly source data and meticulously aligns them according to\n",
        "    the paper's information set policy. It creates the definitive, wide-format\n",
        "    DataFrame that will be the input for all subsequent econometric analysis,\n",
        "    ensuring all variables are correctly timed and aligned.\n",
        "\n",
        "    Inputs:\n",
        "        monthly_neos_df (pd.DataFrame): Monthly NEOS indicators from Task 14.\n",
        "        monthly_indicator_df (pd.DataFrame): Monthly comparator indicators (PMI, KOF).\n",
        "        raw_macro_data_df (pd.DataFrame): Raw quarterly data (GDP, SECO).\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "\n",
        "    Processes:\n",
        "    1.  **Align NEOS Variants:** Applies the information set policy (m=2 for\n",
        "        baseline, m=3 for early-release) to the monthly NEOS series.\n",
        "    2.  **Align Comparators:** Applies the baseline policy (m=2) to the monthly\n",
        "        comparator indicators (PMI, KOF).\n",
        "    3.  **Combine Predictors:** Merges all newly aligned quarterly predictors into\n",
        "        a single DataFrame.\n",
        "    4.  **Final Merge:** Merges the predictor DataFrame with the raw quarterly\n",
        "        data, which contains the dependent variable (GDP).\n",
        "    5.  **Create Lagged Variable:** Creates the lagged dependent variable (y_{t-1}),\n",
        "        which is a required regressor in the forecasting models.\n",
        "    6.  **Finalize:** Returns a clean DataFrame with a complete quarterly index,\n",
        "        ready for econometric modeling.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): The final, fully aligned quarterly dataset for forecasting.\n",
        "    \"\"\"\n",
        "    # --- 1. Align NEOS Indicators (Step 1) ---\n",
        "    # Extract the information set policy from the configuration.\n",
        "    info_policy = fused_master_input_specification['master_config']['econometric_validation_params']['information_set_policy']\n",
        "    m_baseline = info_policy['use_month_m_for_quarter_t']['baseline']\n",
        "    m_early = info_policy['use_month_m_for_quarter_t']['early_neos']\n",
        "\n",
        "    # Align the baseline NEOS using the m=2 rule.\n",
        "    neos_baseline_q = _align_monthly_to_quarterly(\n",
        "        monthly_neos_df[['NEOS_baseline']], m_baseline\n",
        "    )\n",
        "\n",
        "    # Align the early-release variants using the m=3 rule.\n",
        "    early_cols = [col for col in monthly_neos_df.columns if '_d' in col and 'NEOS' in col]\n",
        "    neos_early_q = _align_monthly_to_quarterly(\n",
        "        monthly_neos_df[early_cols], m_early\n",
        "    )\n",
        "\n",
        "    # --- 2. Align Comparator Indicators (Step 2) ---\n",
        "    # Align the monthly comparators using the same baseline m=2 rule.\n",
        "    comparators_q = _align_monthly_to_quarterly(\n",
        "        monthly_indicator_df, m_baseline\n",
        "    )\n",
        "    # Rename columns to the '_m2' convention for clarity.\n",
        "    comparators_q.columns = [f\"{col}_m2\" for col in comparators_q.columns]\n",
        "\n",
        "    # --- 3. Combine All Predictors ---\n",
        "    # Join all the newly created quarterly predictor series.\n",
        "    all_predictors_q = neos_baseline_q.join(\n",
        "        [neos_early_q, comparators_q], how='outer'\n",
        "    )\n",
        "\n",
        "    # --- 4. Final Merge with Dependent Variable (Step 3) ---\n",
        "    # Merge the predictors with the raw macro data (which includes GDP and SECO).\n",
        "    # We select all columns from the raw data to ensure we keep everything.\n",
        "    final_df = raw_macro_data_df.join(all_predictors_q, how='outer')\n",
        "\n",
        "    # --- 5. Create Lagged Dependent Variable ---\n",
        "    # The forecasting equation y_{t+h} = f(y_{t-1}, ...) requires the first lag of GDP.\n",
        "    # The .shift(1) operator correctly creates this lagged series.\n",
        "    dep_var = fused_master_input_specification['master_config']['econometric_validation_params']['dependent_variable']\n",
        "    final_df[f'{dep_var}_lag1'] = final_df[dep_var].shift(1)\n",
        "\n",
        "    # --- 6. Finalize and Return ---\n",
        "    # Sort the index to ensure chronological order.\n",
        "    final_df.sort_index(inplace=True)\n",
        "\n",
        "    print(\"Final forecasting dataset constructed with shape:\", final_df.shape)\n",
        "\n",
        "    return final_df\n"
      ],
      "metadata": {
        "id": "fxFQ4DuPwKKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Execute pseudo-out-of-sample (POOS) expanding-window forecasts for horizons h ∈ {0,1,2}\n",
        "\n",
        "# ==================================================================================================\n",
        "# Task 16: Execute pseudo-out-of-sample (POOS) expanding-window forecasts for horizons h in {0,1,2}\n",
        "# ==================================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Helper Function for a single POOS run\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_expanding_window_regression(\n",
        "    df: pd.DataFrame,\n",
        "    dep_var: str,\n",
        "    dep_var_lag1: str,\n",
        "    indicator_col: Optional[str],\n",
        "    horizon: int,\n",
        "    initial_window_size: int\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes a single pseudo-out-of-sample expanding-window forecast.\n",
        "\n",
        "    Purpose:\n",
        "    This function is the core of the POOS exercise. It simulates a real-time\n",
        "    forecasting process for a single model specification. It iterates through time,\n",
        "    at each step using only past and current data to fit a model and predict a\n",
        "    future value. This rigorously prevents any look-ahead bias.\n",
        "\n",
        "    Inputs:\n",
        "        df (pd.DataFrame): The analysis-ready quarterly DataFrame. It must contain\n",
        "                           the dependent variable, its lag, and the indicator.\n",
        "        dep_var (str): The name of the dependent variable column (e.g., 'yoy_gdp_growth').\n",
        "        dep_var_lag1 (str): The name of the lagged dependent variable column.\n",
        "        indicator_col (Optional[str]): The name of the indicator column. If None,\n",
        "                                       an AR(1) benchmark model is estimated.\n",
        "        horizon (int): The forecast horizon `h` in quarters (0, 1, or 2).\n",
        "        initial_window_size (int): The number of initial quarters for the first estimation.\n",
        "\n",
        "    Processes:\n",
        "    1.  Constructs the appropriate regression formula (AR(1) or AR(1) + indicator).\n",
        "    2.  Creates the shifted dependent variable `y_{t+h}`.\n",
        "    3.  Iterates from the end of the initial window to the end of the sample.\n",
        "    4.  In each iteration `t`:\n",
        "        a. Defines the expanding training window (all data up to `t`).\n",
        "        b. Fits an OLS model on the training window.\n",
        "        c. Predicts the value for `y_{t+h}` using the regressors at time `t`.\n",
        "        d. Calculates the forecast error.\n",
        "        e. Stores the actual value, forecast, error, and timestamp.\n",
        "\n",
        "    Outputs:\n",
        "        (List[Dict[str, Any]]): A list of dictionaries, each containing the\n",
        "                                detailed results for a single forecast origin.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Data and Formula ---\n",
        "    # The target variable y_{t+h} is the future value of the dependent variable.\n",
        "    # We create it by shifting the original series backwards by `h` periods.\n",
        "    target_col = f\"{dep_var}_h{horizon}\"\n",
        "    df[target_col] = df[dep_var].shift(-horizon)\n",
        "\n",
        "    # Construct the regression formula using statsmodels' formula syntax.\n",
        "    if indicator_col:\n",
        "        # Equation (1): y_{t+h} = alpha + beta*y_{t-1} + gamma*x_t + epsilon_t\n",
        "        formula = f\"{target_col} ~ {dep_var_lag1} + `{indicator_col}`\"\n",
        "    else:\n",
        "        # AR(1) Benchmark: y_{t+h} = alpha + beta*y_{t-1} + epsilon_t\n",
        "        formula = f\"{target_col} ~ {dep_var_lag1}\"\n",
        "\n",
        "    # --- 2. POOS Loop ---\n",
        "    # This list will store the results from each forecast origin.\n",
        "    results = []\n",
        "\n",
        "    # The loop starts after the initial estimation window and must end `h`\n",
        "    # periods before the end to have an actual value to compare against.\n",
        "    end_point = len(df) - horizon\n",
        "    start_point = initial_window_size\n",
        "\n",
        "    # Iterate through each possible forecast origin date `t`.\n",
        "    for t in range(start_point, end_point):\n",
        "        # The training data is all data from the beginning up to the current time `t`.\n",
        "        # This implements the \"expanding window\".\n",
        "        train_window = df.iloc[:t+1]\n",
        "\n",
        "        # The information set for prediction is the very last row of the window.\n",
        "        predict_data = train_window.iloc[-1:]\n",
        "\n",
        "        # Check if we have enough data to fit the model (no NaNs in the window).\n",
        "        # This handles missing indicator values at the start of the sample.\n",
        "        if train_window[[dep_var_lag1, indicator_col] if indicator_col else [dep_var_lag1]].dropna().empty:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Fit the OLS model on the current training window.\n",
        "            model = smf.ols(formula, data=train_window).fit()\n",
        "\n",
        "            # Generate the out-of-sample forecast for y_{t+h} using data at time `t`.\n",
        "            forecast = model.predict(predict_data).iloc[0]\n",
        "\n",
        "            # Get the actual realized value of y_{t+h}.\n",
        "            actual = predict_data[target_col].iloc[0]\n",
        "\n",
        "            # Store the results if the forecast is valid.\n",
        "            if pd.notna(forecast) and pd.notna(actual):\n",
        "                results.append({\n",
        "                    'forecast_origin': predict_data.index[0],\n",
        "                    'horizon': horizon,\n",
        "                    'model': indicator_col if indicator_col else 'AR(1)',\n",
        "                    'actual': actual,\n",
        "                    'forecast': forecast,\n",
        "                    'error': actual - forecast\n",
        "                })\n",
        "        except Exception as e:\n",
        "            # Catch potential estimation errors (e.g., perfect multicollinearity).\n",
        "            print(f\"Warning: Could not fit model for origin {predict_data.index[0]} and indicator {indicator_col}. Error: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def execute_poos_forecasts(\n",
        "    forecasting_df: pd.DataFrame,\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    evaluation_windows_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire pseudo-out-of-sample (POOS) forecasting exercise.\n",
        "\n",
        "    This function iterates through all specified indicators and forecast horizons,\n",
        "    running the expanding-window regression for each. It correctly handles\n",
        "    indicators with limited historical availability by restricting the evaluation\n",
        "    period, ensuring a fair and methodologically sound comparison.\n",
        "\n",
        "    Args:\n",
        "        forecasting_df (pd.DataFrame): The final quarterly dataset from Task 15.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        evaluation_windows_df (pd.DataFrame): Defines valid evaluation periods.\n",
        "\n",
        "    Returns:\n",
        "        (pd.DataFrame): A comprehensive DataFrame containing the forecast errors\n",
        "                        for every model, horizon, and time point.\n",
        "    \"\"\"\n",
        "    # --- 1. Extract Configuration ---\n",
        "    econ_params = fused_master_input_specification['master_config']['econometric_validation_params']\n",
        "    dep_var = econ_params['dependent_variable']\n",
        "    dep_var_lag1 = f\"{dep_var}_lag1\"\n",
        "    horizons = econ_params['forecast_horizons_quarters']\n",
        "    initial_window = econ_params['poos_initial_window_quarters']\n",
        "\n",
        "    # --- 2. Define Models to Evaluate ---\n",
        "    # Identify all potential predictor columns in the dataset.\n",
        "    all_cols = forecasting_df.columns\n",
        "    indicator_cols = [\n",
        "        col for col in all_cols if 'NEOS' in col or '_m2' in col or 'seco' in col\n",
        "    ]\n",
        "    # The benchmark model is represented by `None`.\n",
        "    models_to_run = [None] + indicator_cols\n",
        "\n",
        "    # --- 3. Main Evaluation Loop ---\n",
        "    all_results = []\n",
        "\n",
        "    # Use tqdm for a master progress bar over all models and horizons.\n",
        "    pbar = tqdm(total=len(models_to_run) * len(horizons), desc=\"Executing POOS Forecasts\")\n",
        "\n",
        "    # Loop over each model specification (AR(1) and each indicator).\n",
        "    for indicator in models_to_run:\n",
        "        # --- Step 3 (logic): Handle limited-availability indicators ---\n",
        "        # Default to the full DataFrame.\n",
        "        df_for_run = forecasting_df.copy()\n",
        "\n",
        "        # Check if the current indicator has a special evaluation window.\n",
        "        indicator_name_for_window = indicator\n",
        "        if indicator and '_m2' in indicator: # Map e.g. 'service_pmi_m2' to 'service_pmi'\n",
        "            indicator_name_for_window = indicator.replace('_m2', '')\n",
        "\n",
        "        window_spec = evaluation_windows_df[evaluation_windows_df['series_name'] == indicator_name_for_window]\n",
        "\n",
        "        if not window_spec.empty:\n",
        "            # If a window is defined, slice the DataFrame to that period.\n",
        "            start_q = window_spec['start_quarter'].iloc[0]\n",
        "            print(f\"Applying limited evaluation window for '{indicator}': starting from {start_q.date()}\")\n",
        "            df_for_run = df_for_run.loc[start_q:]\n",
        "\n",
        "        # Loop over each forecast horizon (h=0, 1, 2).\n",
        "        for h in horizons:\n",
        "            # --- Step 2 (logic): Run the expanding-window regression ---\n",
        "            # This helper function performs the core POOS loop for one model.\n",
        "            model_results = _run_expanding_window_regression(\n",
        "                df=df_for_run,\n",
        "                dep_var=dep_var,\n",
        "                dep_var_lag1=dep_var_lag1,\n",
        "                indicator_col=indicator,\n",
        "                horizon=h,\n",
        "                initial_window_size=initial_window\n",
        "            )\n",
        "            all_results.extend(model_results)\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    # --- 4. Finalize and Return ---\n",
        "    # Convert the list of result dictionaries into a final DataFrame.\n",
        "    if not all_results:\n",
        "        print(\"Warning: No forecast results were generated. Check data availability and window sizes.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "rYdhJZqFxd8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Compute RMSE ratios for all indicators and horizons\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Compute RMSE ratios for all indicators and horizons\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Steps 1, 2, & 3: Compute RMSEs, Ratios, and Tabulate\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_rmse_ratios(\n",
        "    forecast_errors_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes RMSE ratios for all indicators against the AR(1) benchmark.\n",
        "\n",
        "    Purpose:\n",
        "    This function is the primary evaluation engine of the forecasting exercise.\n",
        "    It takes the raw forecast errors and calculates the main performance metric:\n",
        "    the ratio of the indicator model's Root Mean Squared Error (RMSE) to the\n",
        "    benchmark AR(1) model's RMSE. A ratio below 1 signifies that the indicator\n",
        "    improves forecast accuracy. The function is meticulously designed to ensure\n",
        "    a fair comparison by using the exact same set of forecast origins for each\n",
        "    indicator-benchmark pair.\n",
        "\n",
        "    Inputs:\n",
        "        forecast_errors_df (pd.DataFrame): The long-format DataFrame of forecast\n",
        "                                          errors from Task 16. Must contain\n",
        "                                          'model', 'horizon', 'forecast_origin',\n",
        "                                          and 'error' columns.\n",
        "\n",
        "    Processes:\n",
        "    1.  Pivots the input DataFrame to align errors from all models by their\n",
        "        'forecast_origin' date for each horizon.\n",
        "    2.  Separates the AR(1) benchmark errors.\n",
        "    3.  Iterates through each indicator model and each forecast horizon.\n",
        "    4.  For each combination:\n",
        "        a. **[Step 1]** Identifies the common sample of forecast origins where\n",
        "           *both* the indicator model and the AR(1) model produced a valid forecast.\n",
        "        b. Calculates the squared errors for both models on this common sample.\n",
        "        c. Computes the RMSE for both models using the formula:\n",
        "           RMSE = sqrt(mean(squared_errors)).\n",
        "        d. **[Step 2]** Computes the RMSE ratio: RMSE_indicator / RMSE_AR(1).\n",
        "        e. Stores the indicator name, horizon, ratio, and the common sample size (T).\n",
        "    5.  **[Step 3]** Assembles the collected results into a final, wide-format\n",
        "        DataFrame that mirrors the structure of Table 1 in the paper.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A DataFrame with indicators as the index, horizons as\n",
        "                        columns, and RMSE ratios as values. It also includes\n",
        "                        columns for the sample size used in each calculation.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if forecast_errors_df.empty:\n",
        "        print(\"Warning: Input `forecast_errors_df` is empty. Cannot compute RMSE ratios.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # This list will store the summary results for each model/horizon pair.\n",
        "    summary_results: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Get the unique horizons and model names from the data.\n",
        "    horizons = sorted(forecast_errors_df['horizon'].unique())\n",
        "    # Exclude the benchmark from the list of indicator models to iterate over.\n",
        "    indicator_models = sorted([m for m in forecast_errors_df['model'].unique() if m != 'AR(1)'])\n",
        "\n",
        "    # --- Main Loop: Iterate through each indicator and horizon ---\n",
        "    for horizon in horizons:\n",
        "        # Filter the errors for the current horizon.\n",
        "        horizon_errors = forecast_errors_df[forecast_errors_df['horizon'] == horizon]\n",
        "\n",
        "        # Pivot the table to align errors by date.\n",
        "        # This makes finding the common sample trivial.\n",
        "        pivoted_errors = horizon_errors.pivot_table(\n",
        "            index='forecast_origin', columns='model', values='error'\n",
        "        )\n",
        "\n",
        "        # Extract the benchmark AR(1) errors.\n",
        "        ar1_errors = pivoted_errors['AR(1)']\n",
        "\n",
        "        for indicator in indicator_models:\n",
        "            # Skip if the indicator column doesn't exist for this horizon (can happen with limited windows).\n",
        "            if indicator not in pivoted_errors.columns:\n",
        "                continue\n",
        "\n",
        "            # --- Step 1 (logic): Find common sample and compute RMSEs ---\n",
        "            # Create a temporary DataFrame with just the indicator and benchmark errors.\n",
        "            comparison_df = pivoted_errors[[indicator, 'AR(1)']].copy()\n",
        "\n",
        "            # Drop all rows with any NaNs to get the common sample of forecast origins.\n",
        "            common_sample_errors = comparison_df.dropna()\n",
        "\n",
        "            # Get the size of the common sample, T.\n",
        "            sample_size = len(common_sample_errors)\n",
        "\n",
        "            if sample_size == 0:\n",
        "                continue # Cannot compute ratio if there's no overlapping data.\n",
        "\n",
        "            # Calculate the Mean Squared Error for both models on the common sample.\n",
        "            mse_indicator = np.mean(common_sample_errors[indicator] ** 2)\n",
        "            mse_ar1 = np.mean(common_sample_errors['AR(1)'] ** 2)\n",
        "\n",
        "            # Calculate the Root Mean Squared Error.\n",
        "            # Equation: RMSE = sqrt(mean(error^2))\n",
        "            rmse_indicator = np.sqrt(mse_indicator)\n",
        "            rmse_ar1 = np.sqrt(mse_ar1)\n",
        "\n",
        "            # --- Step 2 (logic): Compute RMSE Ratio ---\n",
        "            # Equation: R_h = RMSE_indicator,h / RMSE_AR(1),h\n",
        "            # Handle the edge case of division by zero.\n",
        "            rmse_ratio = rmse_indicator / rmse_ar1 if rmse_ar1 > 0 else np.nan\n",
        "\n",
        "            # --- Store the results ---\n",
        "            summary_results.append({\n",
        "                'Indicator': indicator,\n",
        "                'Horizon': f'h={horizon}',\n",
        "                'RMSE_Ratio': rmse_ratio,\n",
        "                'N_obs': sample_size\n",
        "            })\n",
        "\n",
        "    # --- Step 3: Tabulate the Final Results ---\n",
        "    # Convert the list of results into a DataFrame.\n",
        "    if not summary_results:\n",
        "        print(\"Warning: No valid RMSE ratios could be computed.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results_long_df = pd.DataFrame(summary_results)\n",
        "\n",
        "    # Pivot the table to create the final wide-format output.\n",
        "    # This creates a table with Indicators as rows and Horizons as columns.\n",
        "    rmse_ratio_table = results_long_df.pivot(\n",
        "        index='Indicator', columns='Horizon', values='RMSE_Ratio'\n",
        "    )\n",
        "\n",
        "    # Create a parallel table for the sample sizes.\n",
        "    sample_size_table = results_long_df.pivot(\n",
        "        index='Indicator', columns='Horizon', values='N_obs'\n",
        "    )\n",
        "    sample_size_table.columns = [f\"N_{col}\" for col in sample_size_table.columns]\n",
        "\n",
        "    # Join the two tables to have ratios and sample sizes side-by-side.\n",
        "    final_table = rmse_ratio_table.join(sample_size_table)\n",
        "\n",
        "    # Reorder columns for clarity.\n",
        "    ordered_cols = []\n",
        "    for h in horizons:\n",
        "        ordered_cols.append(f'h={h}')\n",
        "        ordered_cols.append(f'N_h={h}')\n",
        "    final_table = final_table[ordered_cols]\n",
        "\n",
        "    print(\"\\n--- RMSE Ratio Results (Indicator vs. AR(1) Benchmark) ---\")\n",
        "    print(final_table)\n",
        "\n",
        "    return final_table\n"
      ],
      "metadata": {
        "id": "FMQBDaLAzUUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Perform the modified Diebold–Mariano test with HAC robust standard errors\n",
        "\n",
        "# ===================================================================================\n",
        "# Task 18: Perform the modified Diebold–Mariano test with HAC robust standard errors\n",
        "# ===================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Helper Function for a single DM Test\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_dm_test(\n",
        "    errors_model1: pd.Series,\n",
        "    errors_model2: pd.Series,\n",
        "    horizon: int\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Computes a single Diebold-Mariano test with HAC robust standard errors.\n",
        "\n",
        "    Purpose:\n",
        "    This function performs a statistical test of the null hypothesis that two\n",
        "    sets of forecasts have equal predictive accuracy. It is designed for\n",
        "    out-of-sample forecast evaluation and is robust to the serial correlation\n",
        "    that is naturally present in multi-step-ahead forecast errors.\n",
        "\n",
        "    Inputs:\n",
        "        errors_model1 (pd.Series): Forecast errors from the first model (e.g., indicator).\n",
        "        errors_model2 (pd.Series): Forecast errors from the second model (e.g., benchmark).\n",
        "                                   Must be indexed identically to errors_model1.\n",
        "        horizon (int): The forecast horizon `h`. Used to determine the HAC bandwidth.\n",
        "\n",
        "    Processes:\n",
        "    1.  **[Step 1]** Computes the squared errors for both models and calculates the\n",
        "        loss differential series: d_t = error1_t^2 - error2_t^2.\n",
        "    2.  **[Step 2]** Estimates the HAC-robust long-run variance of the mean of the\n",
        "        loss differential series using the Newey-West estimator with a Bartlett\n",
        "        kernel. The bandwidth `q` is set to `h-1` for h>0, a standard choice for\n",
        "        h-step-ahead forecasts.\n",
        "    3.  **[Step 3]** Computes the Diebold-Mariano test statistic and its two-sided\n",
        "        p-value under the standard normal asymptotic distribution.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (float): The Diebold-Mariano test statistic.\n",
        "        - (float): The corresponding p-value.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Compute Loss Differential ---\n",
        "    # The loss is the squared forecast error.\n",
        "    loss1 = errors_model1**2\n",
        "    loss2 = errors_model2**2\n",
        "    # The loss differential series, d_t.\n",
        "    loss_diff = loss1 - loss2\n",
        "\n",
        "    # The mean of the loss differential, d-bar.\n",
        "    d_bar = loss_diff.mean()\n",
        "\n",
        "    # --- Step 2: Estimate HAC-Robust Variance (Newey-West) ---\n",
        "    # The bandwidth `q` for the HAC estimator. For h-step-ahead forecasts,\n",
        "    # the errors can be serially correlated up to lag h-1.\n",
        "    # For h=0 and h=1, q=0 (no autocorrelation correction needed).\n",
        "    bandwidth = max(0, horizon - 1)\n",
        "\n",
        "    # Get the number of observations, T.\n",
        "    T = len(loss_diff)\n",
        "\n",
        "    # Calculate the autocovariances of the loss differential series.\n",
        "    # `acovf` computes gamma_0, gamma_1, ..., gamma_q.\n",
        "    gamma = tsa.acovf(loss_diff, nlag=bandwidth, fft=False)\n",
        "\n",
        "    # Equation: Var_hat(d_bar) = (1/T) * [gamma_0 + 2 * sum_{l=1 to q} w_l * gamma_l]\n",
        "    # where w_l = 1 - l / (q + 1) is the Bartlett kernel weight.\n",
        "    variance_sum = gamma[0] # Start with gamma_0\n",
        "    for l in range(1, bandwidth + 1):\n",
        "        # Bartlett kernel weight.\n",
        "        weight = 1 - (l / (bandwidth + 1))\n",
        "        variance_sum += 2 * weight * gamma[l]\n",
        "\n",
        "    # The final HAC-robust variance of the mean.\n",
        "    hac_variance = variance_sum / T\n",
        "\n",
        "    # --- Step 3: Compute DM Statistic and p-value ---\n",
        "    # Handle the case where variance is zero or negative (numerical instability).\n",
        "    if hac_variance <= 0:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Equation: DM = d_bar / sqrt(Var_hat(d_bar))\n",
        "    dm_statistic = d_bar / np.sqrt(hac_variance)\n",
        "\n",
        "    # The p-value is calculated from the standard normal distribution (asymptotic).\n",
        "    # Equation: p = 2 * (1 - Phi(|DM|))\n",
        "    # `norm.sf` is the survival function (1 - CDF), which is more accurate for large values.\n",
        "    p_value = 2 * norm.sf(np.abs(dm_statistic))\n",
        "\n",
        "    return dm_statistic, p_value\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def perform_diebold_mariano_tests(\n",
        "    forecast_errors_df: pd.DataFrame,\n",
        "    rmse_ratios_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the Diebold-Mariano testing for all indicators and horizons.\n",
        "\n",
        "    This function systematically compares each indicator model's forecast\n",
        "    accuracy against the AR(1) benchmark. It calculates the DM test statistic\n",
        "    and p-value for each comparison and appends these results to the existing\n",
        "    RMSE ratio table, creating a comprehensive final evaluation summary.\n",
        "\n",
        "    Args:\n",
        "        forecast_errors_df (pd.DataFrame): The long-format DataFrame of forecast\n",
        "                                          errors from Task 16.\n",
        "        rmse_ratios_df (pd.DataFrame): The summary table of RMSE ratios from Task 17.\n",
        "\n",
        "    Returns:\n",
        "        (pd.DataFrame): The `rmse_ratios_df` augmented with columns for the\n",
        "                        DM test p-values for each horizon.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if forecast_errors_df.empty:\n",
        "        print(\"Warning: Input `forecast_errors_df` is empty. Cannot perform DM tests.\")\n",
        "        return rmse_ratios_df.copy()\n",
        "\n",
        "    # This list will store the p-values to be added to the results table.\n",
        "    dm_results: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Get the unique horizons and indicator models from the data.\n",
        "    horizons = sorted(forecast_errors_df['horizon'].unique())\n",
        "    indicator_models = sorted([m for m in forecast_errors_df['model'].unique() if m != 'AR(1)'])\n",
        "\n",
        "    # --- Main Loop: Iterate through each indicator and horizon ---\n",
        "    for horizon in horizons:\n",
        "        # Filter the errors for the current horizon.\n",
        "        horizon_errors = forecast_errors_df[forecast_errors_df['horizon'] == horizon]\n",
        "\n",
        "        # Pivot to align errors by date.\n",
        "        pivoted_errors = horizon_errors.pivot_table(\n",
        "            index='forecast_origin', columns='model', values='error'\n",
        "        )\n",
        "\n",
        "        # Extract the benchmark AR(1) errors.\n",
        "        ar1_errors = pivoted_errors['AR(1)']\n",
        "\n",
        "        for indicator in indicator_models:\n",
        "            if indicator not in pivoted_errors.columns:\n",
        "                continue\n",
        "\n",
        "            # Find the common sample where both models have valid forecasts.\n",
        "            comparison_df = pivoted_errors[[indicator, 'AR(1)']].dropna()\n",
        "\n",
        "            if comparison_df.empty:\n",
        "                continue\n",
        "\n",
        "            # --- Call the DM test helper function ---\n",
        "            # This performs all three steps for the current comparison.\n",
        "            dm_stat, p_val = _compute_dm_test(\n",
        "                errors_model1=comparison_df[indicator],\n",
        "                errors_model2=comparison_df['AR(1)'],\n",
        "                horizon=horizon\n",
        "            )\n",
        "\n",
        "            # Store the resulting p-value.\n",
        "            dm_results.append({\n",
        "                'Indicator': indicator,\n",
        "                'Horizon': f'h={horizon}',\n",
        "                'p_value': p_val\n",
        "            })\n",
        "\n",
        "    # --- Assemble Final Table ---\n",
        "    # Convert the list of results into a DataFrame.\n",
        "    if not dm_results:\n",
        "        print(\"Warning: No DM test results were generated.\")\n",
        "        return rmse_ratios_df.copy()\n",
        "\n",
        "    p_values_long_df = pd.DataFrame(dm_results)\n",
        "\n",
        "    # Pivot the p-values into a wide-format table.\n",
        "    p_values_table = p_values_long_df.pivot(\n",
        "        index='Indicator', columns='Horizon', values='p_value'\n",
        "    )\n",
        "    # Rename columns for clarity.\n",
        "    p_values_table.columns = [f\"p_val_{col}\" for col in p_values_table.columns]\n",
        "\n",
        "    # Join the p-values with the original RMSE ratio table.\n",
        "    final_table = rmse_ratios_df.join(p_values_table)\n",
        "\n",
        "    # Reorder columns to group ratios and p-values by horizon.\n",
        "    ordered_cols = []\n",
        "    for h in horizons:\n",
        "        h_str = f'h={h}'\n",
        "        if h_str in final_table.columns:\n",
        "            ordered_cols.append(h_str)\n",
        "            ordered_cols.append(f'p_val_{h_str}')\n",
        "        # Also keep the sample size columns.\n",
        "        if f'N_{h_str}' in final_table.columns:\n",
        "            ordered_cols.append(f'N_{h_str}')\n",
        "\n",
        "    final_table = final_table[ordered_cols]\n",
        "\n",
        "    print(\"\\n--- Final Evaluation Table (RMSE Ratios and DM p-values) ---\")\n",
        "    print(final_table)\n",
        "\n",
        "    return final_table\n"
      ],
      "metadata": {
        "id": "oVWi_qvS0ssf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Construct the lexicon-based baseline indicator (German articles only)\n",
        "\n",
        "# ===============================================================================\n",
        "# Task 19: Construct the lexicon-based baseline indicator (German articles only)\n",
        "# ===============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Step 1: Prepare the Barbaglia et al. (2025) lexicon\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _load_and_prepare_lexicon(\n",
        "    lexicon_path: str\n",
        ") -> Tuple[Set[str], Set[str]]:\n",
        "    \"\"\"\n",
        "    Loads and prepares the translated German sentiment lexicon.\n",
        "\n",
        "    Purpose:\n",
        "    This function reads the pre-translated lexicon file, validates its structure,\n",
        "    and processes it into a data structure optimized for fast lookups during the\n",
        "    scoring phase. It separates the words into distinct sets for positive and\n",
        "    negative sentiment.\n",
        "\n",
        "    Inputs:\n",
        "        lexicon_path (str): The file path to the CSV containing the translated\n",
        "                            lexicon. The CSV must have 'word_de' and 'sentiment'\n",
        "                            columns.\n",
        "\n",
        "    Processes:\n",
        "    1.  Loads the lexicon from the specified CSV path.\n",
        "    2.  Validates the presence of the required 'word_de' and 'sentiment' columns.\n",
        "    3.  Converts all words in the 'word_de' column to lowercase for case-insensitive matching.\n",
        "    4.  Filters the lexicon to create a set of unique positive words.\n",
        "    5.  Filters the lexicon to create a set of unique negative words.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (Set[str]): A set of lowercase positive German words.\n",
        "        - (Set[str]): A set of lowercase negative German words.\n",
        "    \"\"\"\n",
        "    # --- 1. Load and Validate Lexicon ---\n",
        "    try:\n",
        "        lexicon_df = pd.read_csv(lexicon_path)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Lexicon file not found at: {lexicon_path}\")\n",
        "\n",
        "    if 'word_de' not in lexicon_df.columns or 'sentiment' not in lexicon_df.columns:\n",
        "        raise ValueError(\"Lexicon CSV must contain 'word_de' and 'sentiment' columns.\")\n",
        "\n",
        "    # --- 2. Prepare Word Sets ---\n",
        "    # Convert words to lowercase for consistent matching.\n",
        "    lexicon_df['word_de'] = lexicon_df['word_de'].str.lower()\n",
        "\n",
        "    # Create a set of positive words for efficient lookup.\n",
        "    positive_words = set(lexicon_df[lexicon_df['sentiment'] == 'positive']['word_de'].dropna())\n",
        "\n",
        "    # Create a set of negative words for efficient lookup.\n",
        "    negative_words = set(lexicon_df[lexicon_df['sentiment'] == 'negative']['word_de'].dropna())\n",
        "\n",
        "    print(f\"Loaded lexicon: {len(positive_words)} positive words, {len(negative_words)} negative words.\")\n",
        "\n",
        "    return positive_words, negative_words\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Step 2: Score German articles and compute monthly aggregates\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _score_articles_and_aggregate(\n",
        "    german_articles_df: pd.DataFrame,\n",
        "    positive_words: Set[str],\n",
        "    negative_words: Set[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scores German articles based on lexicon word counts and aggregates to monthly.\n",
        "\n",
        "    Purpose:\n",
        "    This function implements the core logic of the lexicon-based approach. It\n",
        "    tokenizes each article, counts the occurrences of positive and negative\n",
        "    words, calculates a sentiment score for each article, and then computes the\n",
        "    average score for each month to create the final time-series indicator.\n",
        "\n",
        "    Inputs:\n",
        "        german_articles_df (pd.DataFrame): A DataFrame of relevant German articles.\n",
        "        positive_words (Set[str]): A set of positive sentiment words.\n",
        "        negative_words (Set[str]): A set of negative sentiment words.\n",
        "\n",
        "    Processes:\n",
        "    1.  Defines a scoring function that:\n",
        "        a. Takes a text, tokenizes it by splitting on whitespace.\n",
        "        b. Counts positive and negative words.\n",
        "        c. Calculates a polarity score: (Pos - Neg) / (Pos + Neg).\n",
        "    2.  Applies this scoring function to every article's 'full_text'.\n",
        "    3.  Calls a helper to aggregate the resulting article-level scores into a\n",
        "        monthly time series.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A monthly time series DataFrame for the lexicon indicator.\n",
        "    \"\"\"\n",
        "    # --- 1. Define the Article Scoring Function ---\n",
        "    def calculate_lexicon_score(text: str) -> float:\n",
        "        \"\"\"Calculates the polarity score for a single piece of text.\"\"\"\n",
        "        # Simple whitespace tokenization and lowercasing.\n",
        "        tokens = text.lower().split()\n",
        "\n",
        "        # Count positive and negative words.\n",
        "        pos_count = sum(1 for token in tokens if token in positive_words)\n",
        "        neg_count = sum(1 for token in tokens if token in negative_words)\n",
        "\n",
        "        # Calculate the polarity score. Add a small epsilon to avoid division by zero.\n",
        "        # Formula: (Positive - Negative) / (Positive + Negative)\n",
        "        total_sentiment_words = pos_count + neg_count\n",
        "        if total_sentiment_words == 0:\n",
        "            return 0.0 # Neutral if no sentiment words are found.\n",
        "\n",
        "        return (pos_count - neg_count) / total_sentiment_words\n",
        "\n",
        "    # --- 2. Apply Scoring to All Articles ---\n",
        "    # Initialize tqdm for pandas .apply() to show a progress bar.\n",
        "    tqdm.pandas(desc=\"Scoring articles with lexicon\")\n",
        "\n",
        "    # Create a new column with the calculated score for each article.\n",
        "    # This is the most computationally intensive step.\n",
        "    df = german_articles_df.copy()\n",
        "    df['lexicon_score'] = df['full_text'].progress_apply(calculate_lexicon_score)\n",
        "\n",
        "    # --- 3. Aggregate to Monthly Frequency ---\n",
        "    # Rename the score column to match the input expected by the aggregation helper.\n",
        "    df.rename(columns={'lexicon_score': 'p_pos'}, inplace=True)\n",
        "\n",
        "    # Use the same aggregation helper from Task 14 for consistency.\n",
        "    monthly_lexicon_indicator = _aggregate_scores_to_monthly(df, suffix='_lexicon')\n",
        "\n",
        "    return monthly_lexicon_indicator\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_lexicon_baseline_indicator(\n",
        "    df_relevant: pd.DataFrame,\n",
        "    lexicon_path: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the lexicon-based sentiment indicator.\n",
        "\n",
        "    This function serves as a self-contained pipeline to create the baseline\n",
        "    sentiment indicator described in Appendix A.3. It loads a translated lexicon,\n",
        "    applies it to score all relevant German-language articles, and aggregates\n",
        "    these scores into a final monthly time series.\n",
        "\n",
        "    Args:\n",
        "        df_relevant (pd.DataFrame): The DataFrame of all economics-relevant articles.\n",
        "        lexicon_path (str): The file path to the translated German lexicon CSV.\n",
        "\n",
        "    Returns:\n",
        "        (pd.DataFrame): A monthly time series DataFrame containing the lexicon-based\n",
        "                        indicator and the corresponding monthly article counts. This\n",
        "                        DataFrame is ready to be merged with other indicators for\n",
        "                        the forecasting exercise.\n",
        "    \"\"\"\n",
        "    # --- 1. Load and Prepare the Lexicon ---\n",
        "    positive_words, negative_words = _load_and_prepare_lexicon(lexicon_path)\n",
        "\n",
        "    # --- 2. Filter for German Articles ---\n",
        "    # The paper specifies that this baseline is constructed on German articles only.\n",
        "    german_articles_df = df_relevant[df_relevant['language'] == 'de'].copy()\n",
        "\n",
        "    if german_articles_df.empty:\n",
        "        print(\"Warning: No German articles found in the relevant set. Cannot construct lexicon baseline.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- 3. Score Articles and Aggregate ---\n",
        "    # This function handles the core computation.\n",
        "    monthly_indicator = _score_articles_and_aggregate(\n",
        "        german_articles_df,\n",
        "        positive_words,\n",
        "        negative_words\n",
        "    )\n",
        "\n",
        "    print(\"Successfully constructed the lexicon-based baseline indicator.\")\n",
        "\n",
        "    return monthly_indicator\n"
      ],
      "metadata": {
        "id": "2ATi1sVW2pTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 — Compute correlation benchmarks (Appendix A.2 replication)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Compute correlation benchmarks (Appendix A.2 replication)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Step 1: Transform monthly indicators to quarterly via three-month averaging\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _average_monthly_to_quarterly(\n",
        "    monthly_indicators: Dict[str, pd.DataFrame]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transforms monthly indicators to quarterly by simple three-month averaging.\n",
        "\n",
        "    Purpose:\n",
        "    This function prepares the indicator data specifically for the correlation\n",
        "    analysis in Appendix A.2. Unlike the forecasting task which uses a selection\n",
        "    rule, this analysis requires a simple average of the three monthly values\n",
        "    within each quarter.\n",
        "\n",
        "    Inputs:\n",
        "        monthly_indicators (Dict[str, pd.DataFrame]): A dictionary where keys are\n",
        "            indicator names and values are the corresponding monthly time-series\n",
        "            DataFrames.\n",
        "\n",
        "    Processes:\n",
        "    1.  Initializes an empty list to store the quarterly-averaged series.\n",
        "    2.  Iterates through each monthly indicator DataFrame provided.\n",
        "    3.  Uses the `resample('QS-JAN').mean()` method to perform the quarterly averaging.\n",
        "    4.  Appends the resulting quarterly series to the list.\n",
        "    5.  Concatenates all quarterly series into a single, aligned DataFrame.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A single DataFrame containing all indicators at a\n",
        "                        quarterly frequency, created by averaging.\n",
        "    \"\"\"\n",
        "    quarterly_series_list = []\n",
        "\n",
        "    # Iterate through each monthly indicator provided in the input dictionary.\n",
        "    for name, df in monthly_indicators.items():\n",
        "        # Use resample('QS-JAN') to group by calendar quarter, then take the mean.\n",
        "        # This correctly handles quarters with missing months (e.g., averaging over 2 instead of 3 values).\n",
        "        quarterly_series = df.resample('QS-JAN').mean()\n",
        "        # Rename the column to reflect the indicator name.\n",
        "        quarterly_series.columns = [name]\n",
        "        quarterly_series_list.append(quarterly_series)\n",
        "\n",
        "    # Join all the quarterly series into a single DataFrame.\n",
        "    # The 'outer' join ensures that the full time range is preserved.\n",
        "    all_quarterly_indicators = pd.concat(quarterly_series_list, axis=1, join='outer')\n",
        "\n",
        "    return all_quarterly_indicators\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Steps 2 & 3: Compute lagged correlations and tabulate results\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_and_tabulate_correlations(\n",
        "    quarterly_indicators: pd.DataFrame,\n",
        "    gdp_series: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Computes and tabulates lagged correlations between indicators and GDP growth.\n",
        "\n",
        "    Purpose:\n",
        "    This function replicates the analysis in Table 2 of the paper. It calculates\n",
        "    the Pearson correlation between various indicators (at lags 0 to 4) and two\n",
        "    measures of GDP growth (year-on-year and quarter-on-quarter). It then\n",
        "    assembles these results into two publication-ready tables.\n",
        "\n",
        "    Inputs:\n",
        "        quarterly_indicators (pd.DataFrame): The quarterly-averaged indicators.\n",
        "        gdp_series (pd.DataFrame): A DataFrame with 'yoy_gdp' and 'qoq_gdp' columns.\n",
        "\n",
        "    Processes:\n",
        "    1.  Defines the lags to be computed (0 to 4).\n",
        "    2.  Iterates through each indicator and each lag.\n",
        "    3.  For each combination, it creates a lagged version of the indicator.\n",
        "    4.  It then computes the correlation between the lagged indicator and both\n",
        "        yoy and qoq GDP growth, ensuring that the calculation for each pair\n",
        "        only uses the common set of available data points.\n",
        "    5.  Stores the results in a long-format list.\n",
        "    6.  Converts the list to a DataFrame and pivots it to create the final\n",
        "        wide-format tables, adding the summary average columns.\n",
        "\n",
        "    Outputs:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): The correlation table for year-on-year (yoy) GDP growth.\n",
        "        - (pd.DataFrame): The correlation table for quarter-on-quarter (qoq) GDP growth.\n",
        "    \"\"\"\n",
        "    # This list will store the raw correlation results in a long format.\n",
        "    correlation_results = []\n",
        "    lags = range(5) # Lags 0, 1, 2, 3, 4\n",
        "\n",
        "    # Iterate through each indicator column.\n",
        "    for indicator_col in quarterly_indicators.columns:\n",
        "        # Iterate through each lag.\n",
        "        for lag in lags:\n",
        "            # Create the lagged indicator series.\n",
        "            lagged_indicator = quarterly_indicators[indicator_col].shift(lag)\n",
        "\n",
        "            # --- Correlate with YoY GDP ---\n",
        "            # Combine the two series and drop NaNs to find the common sample.\n",
        "            yoy_aligned = pd.concat([gdp_series['yoy_gdp'], lagged_indicator], axis=1).dropna()\n",
        "            # Compute correlation on the aligned data.\n",
        "            yoy_corr = yoy_aligned.corr().iloc[0, 1] if len(yoy_aligned) > 1 else np.nan\n",
        "\n",
        "            correlation_results.append({\n",
        "                'Indicator': indicator_col,\n",
        "                'GDP_Type': 'yoy',\n",
        "                'Lag': lag,\n",
        "                'Correlation': yoy_corr\n",
        "            })\n",
        "\n",
        "            # --- Correlate with QoQ GDP ---\n",
        "            # Repeat the process for quarter-on-quarter GDP.\n",
        "            qoq_aligned = pd.concat([gdp_series['qoq_gdp'], lagged_indicator], axis=1).dropna()\n",
        "            qoq_corr = qoq_aligned.corr().iloc[0, 1] if len(qoq_aligned) > 1 else np.nan\n",
        "\n",
        "            correlation_results.append({\n",
        "                'Indicator': indicator_col,\n",
        "                'GDP_Type': 'qoq',\n",
        "                'Lag': lag,\n",
        "                'Correlation': qoq_corr\n",
        "            })\n",
        "\n",
        "    # --- Assemble the Final Tables (Step 3) ---\n",
        "    results_df = pd.DataFrame(correlation_results)\n",
        "\n",
        "    # Create a helper function to build the final table for a given GDP type.\n",
        "    def build_table(gdp_type: str) -> pd.DataFrame:\n",
        "        # Filter for the specific GDP type.\n",
        "        df_subset = results_df[results_df['GDP_Type'] == gdp_type]\n",
        "\n",
        "        # Pivot from long to wide format.\n",
        "        table = df_subset.pivot(index='Indicator', columns='Lag', values='Correlation')\n",
        "\n",
        "        # Calculate summary average columns.\n",
        "        if 0 in table.columns and 1 in table.columns:\n",
        "            table['AVG(0:1)'] = table[[0, 1]].mean(axis=1)\n",
        "        if all(l in table.columns for l in lags):\n",
        "            table['AVG(0:4)'] = table[list(lags)].mean(axis=1)\n",
        "\n",
        "        return table\n",
        "\n",
        "    # Build the two final tables.\n",
        "    yoy_table = build_table('yoy')\n",
        "    qoq_table = build_table('qoq')\n",
        "\n",
        "    return yoy_table, qoq_table\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_correlation_benchmarks(\n",
        "    monthly_neos_df: pd.DataFrame,\n",
        "    monthly_indicator_df: pd.DataFrame,\n",
        "    lexicon_baseline_df: pd.DataFrame,\n",
        "    raw_macro_data_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full correlation analysis to replicate Appendix A.2.\n",
        "\n",
        "    This function manages the entire workflow for the descriptive correlation\n",
        "    analysis. It first transforms all monthly indicators to a quarterly frequency\n",
        "    using simple averaging, then computes their lagged correlations with both\n",
        "    year-on-year and quarter-on-quarter GDP growth, and finally formats the\n",
        "    results into two publication-ready tables.\n",
        "\n",
        "    Args:\n",
        "        monthly_neos_df (pd.DataFrame): Monthly NEOS indicators.\n",
        "        monthly_indicator_df (pd.DataFrame): Monthly comparator indicators (PMI, KOF).\n",
        "        lexicon_baseline_df (pd.DataFrame): Monthly lexicon-based indicator.\n",
        "        raw_macro_data_df (pd.DataFrame): Raw quarterly data containing GDP series.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - (pd.DataFrame): The correlation table for year-on-year (yoy) GDP growth.\n",
        "        - (pd.DataFrame): The correlation table for quarter-on-quarter (qoq) GDP growth.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Monthly Indicators for Averaging ---\n",
        "    # Combine all monthly series into a dictionary for processing.\n",
        "    # We only need the main indicator columns, not the counts.\n",
        "    monthly_indicators_to_process = {\n",
        "        'NEOS': monthly_neos_df[['NEOS_baseline']],\n",
        "        'Manufacturing_PMI': monthly_indicator_df[['manufacturing_pmi']],\n",
        "        'Service_PMI': monthly_indicator_df[['service_pmi']],\n",
        "        'KOF_Business_Situation': monthly_indicator_df[['kof_biz_situation']],\n",
        "        'Lexicon_Baseline': lexicon_baseline_df[['NEOS_lexicon']]\n",
        "    }\n",
        "    # Add early NEOS variants as well.\n",
        "    for col in monthly_neos_df.columns:\n",
        "        if '_d' in col and 'NEOS' in col:\n",
        "            monthly_indicators_to_process[col] = monthly_neos_df[[col]]\n",
        "\n",
        "    # --- Step 1: Transform to Quarterly via Averaging ---\n",
        "    quarterly_indicators = _average_monthly_to_quarterly(monthly_indicators_to_process)\n",
        "\n",
        "    # --- 2. Prepare GDP Series ---\n",
        "    # Assume yoy_gdp is present. Calculate qoq_gdp if not.\n",
        "    gdp_df = raw_macro_data_df[['yoy_gdp_growth_sports_adj']].copy()\n",
        "    gdp_df.rename(columns={'yoy_gdp_growth_sports_adj': 'yoy_gdp'}, inplace=True)\n",
        "\n",
        "    # For qoq, we need an underlying GDP level series, which is not provided.\n",
        "    # For this implementation, we will assume a placeholder 'qoq_gdp' column exists\n",
        "    # in raw_macro_data_df or create it as NaNs if not.\n",
        "    if 'qoq_gdp_growth' in raw_macro_data_df.columns:\n",
        "         gdp_df['qoq_gdp'] = raw_macro_data_df['qoq_gdp_growth']\n",
        "    else:\n",
        "         print(\"Warning: Quarter-on-quarter GDP growth not found. QoQ correlations will be NaN.\")\n",
        "         gdp_df['qoq_gdp'] = np.nan\n",
        "\n",
        "    # Add the quarterly SECO indicator to the set of indicators to test.\n",
        "    if 'seco_consumer_sentiment_q' in raw_macro_data_df.columns:\n",
        "        quarterly_indicators['SECO_Consumer_Sentiment'] = raw_macro_data_df['seco_consumer_sentiment_q']\n",
        "\n",
        "    # --- Steps 2 & 3: Compute and Tabulate Correlations ---\n",
        "    yoy_table, qoq_table = _calculate_and_tabulate_correlations(\n",
        "        quarterly_indicators, gdp_df\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Correlation Table with YoY GDP Growth ---\")\n",
        "    print(yoy_table.round(2))\n",
        "    print(\"\\n--- Correlation Table with QoQ GDP Growth ---\")\n",
        "    print(qoq_table.round(2))\n",
        "\n",
        "    return yoy_table, qoq_table\n"
      ],
      "metadata": {
        "id": "wIFDaUE54PFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 — Generate diagnostics and figures (Charts 2, 3, 4, 5)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Generate diagnostics and figures (Charts 2, 3, 4, 5)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 2: Chart 3 — Time-series comparison\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_chart3_timeseries_comparison(\n",
        "    monthly_neos_df: pd.DataFrame,\n",
        "    monthly_indicator_df: pd.DataFrame,\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    output_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates a time-series comparison plot (replication of Chart 3).\n",
        "\n",
        "    This function plots the standardized NEOS indicator against other key\n",
        "    monthly indicators (PMI, KOF) and overlays the quarterly year-on-year\n",
        "    GDP growth as bars on a secondary axis.\n",
        "\n",
        "    Args:\n",
        "        monthly_neos_df: DataFrame with the monthly NEOS_baseline.\n",
        "        monthly_indicator_df: DataFrame with monthly comparator indicators.\n",
        "        raw_macro_data_df: DataFrame with quarterly GDP growth.\n",
        "        output_path: File path to save the generated plot.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare and Combine Data ---\n",
        "    # Select the series for plotting.\n",
        "    indicators_to_plot = {\n",
        "        'NEOS': monthly_neos_df['NEOS_baseline'],\n",
        "        'Manufacturing PMI': monthly_indicator_df['manufacturing_pmi'],\n",
        "        'KOF Business Situation': monthly_indicator_df['kof_biz_situation'],\n",
        "    }\n",
        "    plot_df = pd.concat(indicators_to_plot, axis=1).dropna(how='all')\n",
        "\n",
        "    # --- 2. Standardize (Z-score) the Indicators ---\n",
        "    # Standardization is necessary to plot them on the same scale.\n",
        "    scaler = StandardScaler()\n",
        "    plot_df_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(plot_df),\n",
        "        index=plot_df.index,\n",
        "        columns=plot_df.columns\n",
        "    )\n",
        "\n",
        "    # --- 3. Create the Plot ---\n",
        "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    # Plot the standardized indicators on the primary y-axis.\n",
        "    plot_df_scaled.plot(ax=ax1, lw=1.5)\n",
        "    ax1.set_ylabel('Index, standardised', fontsize=12)\n",
        "    ax1.set_xlabel('')\n",
        "    ax1.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
        "\n",
        "    # --- 4. Create and Plot on Secondary Axis ---\n",
        "    # Create a secondary y-axis for the GDP growth bars.\n",
        "    ax2 = ax1.twinx()\n",
        "    gdp_series = raw_macro_data_df['yoy_gdp_growth_sports_adj']\n",
        "    # Plot GDP growth as bars. Width is set to ~90 days to fill quarters.\n",
        "    ax2.bar(gdp_series.index, gdp_series.values, width=90, alpha=0.3, color='gray', label='GDP, yoy (rhs)')\n",
        "    ax2.set_ylabel('%', fontsize=12, rotation=0, labelpad=15)\n",
        "    ax2.grid(False) # Turn off grid for the secondary axis.\n",
        "\n",
        "    # --- 5. Finalize Aesthetics ---\n",
        "    fig.suptitle('News-based Economic Outlook for Switzerland (NEOS)', fontsize=16, fontweight='bold')\n",
        "    ax1.set_title('Comparison with survey-based indicators and real GDP growth', fontsize=12)\n",
        "\n",
        "    # Combine legends from both axes.\n",
        "    lines, labels = ax1.get_legend_handles_labels()\n",
        "    bars, bar_labels = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines + bars, labels + bar_labels, loc='upper left')\n",
        "\n",
        "    # Set x-axis limits and format.\n",
        "    ax1.set_xlim(plot_df_scaled.index.min(), plot_df_scaled.index.max())\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Save the figure.\n",
        "    fig.savefig(output_path, dpi=300)\n",
        "    print(f\"Chart 3 saved to: {output_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Charts 4 and 5 — Timeliness and crisis diagnostics\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_chart4_timeliness_diagnostic(\n",
        "    daily_indicator_df: pd.DataFrame,\n",
        "    output_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates the daily month-to-date NEOS evolution plot (Chart 4).\n",
        "\n",
        "    This function visualizes the high-frequency nature of the NEOS indicator\n",
        "    by plotting its daily evolution within a specific crisis period and\n",
        "    annotating it with key real-world events.\n",
        "\n",
        "    Args:\n",
        "        daily_indicator_df: The daily MTD NEOS series from Task 14.\n",
        "        output_path: File path to save the generated plot.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Data ---\n",
        "    # The paper uses a hypothetical Feb-Apr 2025 period for illustration.\n",
        "    plot_df = daily_indicator_df.loc['2025-02-01':'2025-04-30'].copy()\n",
        "\n",
        "    # --- 2. Create the Plot ---\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    # Plot the daily MTD series.\n",
        "    ax.plot(plot_df.index, plot_df['NEOS_mtd'], label='NEOS (month-to-date)')\n",
        "\n",
        "    # Find month-end values to mark with diamonds.\n",
        "    month_ends = plot_df.resample('M').last()\n",
        "    ax.plot(month_ends.index, month_ends['NEOS_mtd'], 'D', ms=8, label='NEOS (monthly indicator)')\n",
        "\n",
        "    # --- 3. Add Event Annotations ---\n",
        "    # These are the hypothetical events from the paper's chart.\n",
        "    events = {\n",
        "        '2025-02-03': '[Feb 03] Tariffs paused',\n",
        "        '2025-03-04': '[Mar 04] Tariffs go into effect',\n",
        "        '2025-04-09': '[Apr 09] 90-day pause on \"reciprocal\" tariffs',\n",
        "    }\n",
        "    for date_str, label in events.items():\n",
        "        date = pd.to_datetime(date_str)\n",
        "        ax.axvline(x=date, color='r', linestyle='--', linewidth=1)\n",
        "        ax.text(date + pd.Timedelta(days=1), ax.get_ylim()[1]*0.95, label,\n",
        "                rotation=0, verticalalignment='top', color='r')\n",
        "\n",
        "    # --- 4. Finalize Aesthetics ---\n",
        "    ax.set_title('NEWS-BASED ECONOMIC OUTLOOK FOR SWITZERLAND (NEOS)', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Index, standardised')\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%B %Y'))\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # Save the figure.\n",
        "    fig.savefig(output_path, dpi=300)\n",
        "    print(f\"Chart 4 saved to: {output_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "def _generate_chart5_crisis_performance(\n",
        "    forecast_errors_df: pd.DataFrame,\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    output_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates the cumulative squared error difference plot (Chart 5).\n",
        "\n",
        "    This function visualizes when the NEOS model provides the most value. It\n",
        "    plots the cumulative sum of the difference in squared forecast errors\n",
        "    between the NEOS model and the AR(1) benchmark. A downward-sloping line\n",
        "    indicates periods where NEOS is outperforming the benchmark.\n",
        "\n",
        "    Args:\n",
        "        forecast_errors_df: The long-format DataFrame of all forecast errors.\n",
        "        raw_macro_data_df: DataFrame with quarterly GDP growth for context.\n",
        "        output_path: File path to save the generated plot.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Data ---\n",
        "    # Filter for the specific comparison: NEOS baseline vs. AR(1) at h=0.\n",
        "    errors_h0 = forecast_errors_df[forecast_errors_df['horizon'] == 0]\n",
        "    pivoted_errors = errors_h0.pivot_table(\n",
        "        index='forecast_origin', columns='model', values='error'\n",
        "    )\n",
        "\n",
        "    # Select the relevant models and find the common sample.\n",
        "    comparison_df = pivoted_errors[['NEOS_baseline', 'AR(1)']].dropna()\n",
        "\n",
        "    # --- 2. Calculate Cumulative Squared Error Difference ---\n",
        "    # Equation: Delta_t = e_NEOS^2 - e_AR(1)^2\n",
        "    comparison_df['sq_error_diff'] = comparison_df['NEOS_baseline']**2 - comparison_df['AR(1)']**2\n",
        "    # Equation: C_t = sum_{s<=t} Delta_s\n",
        "    comparison_df['cumulative_sq_error_diff'] = comparison_df['sq_error_diff'].cumsum()\n",
        "\n",
        "    # --- 3. Create the Plot ---\n",
        "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    # Plot the cumulative difference on the primary y-axis.\n",
        "    ax1.plot(comparison_df.index, comparison_df['cumulative_sq_error_diff'],\n",
        "             label='Cumulative squared error differences', color='tab:blue')\n",
        "    ax1.set_ylabel('Cumulative squared error differences', fontsize=12)\n",
        "    ax1.set_xlabel('')\n",
        "\n",
        "    # --- 4. Add GDP Context on Secondary Axis ---\n",
        "    ax2 = ax1.twinx()\n",
        "    gdp_series = raw_macro_data_df['yoy_gdp_growth_sports_adj']\n",
        "    ax2.plot(gdp_series.index, gdp_series, color='black', lw=1.5, alpha=0.7,\n",
        "             label='Real GDP, yoy (rhs)')\n",
        "    ax2.set_ylabel('%', fontsize=12, rotation=0, labelpad=15)\n",
        "    ax2.axhline(0, color='gray', linestyle='--', lw=1) # Add zero line for GDP\n",
        "\n",
        "    # --- 5. Finalize Aesthetics ---\n",
        "    ax1.set_title('NEOS is a valuable predictor for Swiss GDP in times of crises', fontsize=16, fontweight='bold')\n",
        "    fig.legend(loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=2)\n",
        "    ax1.set_xlim(comparison_df.index.min(), comparison_df.index.max())\n",
        "    fig.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
        "\n",
        "    # Save the figure.\n",
        "    fig.savefig(output_path, dpi=300)\n",
        "    print(f\"Chart 5 saved to: {output_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_all_charts(\n",
        "    # Inputs for Chart 2\n",
        "    synthetic_corpus_path: str,\n",
        "    synthetic_embeddings: np.ndarray,\n",
        "    # Inputs for Chart 3\n",
        "    monthly_neos_df: pd.DataFrame,\n",
        "    monthly_indicator_df: pd.DataFrame,\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    # Inputs for Chart 4\n",
        "    daily_indicator_df: Optional[pd.DataFrame],\n",
        "    # Inputs for Chart 5\n",
        "    forecast_errors_df: pd.DataFrame,\n",
        "    # General\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    output_directory: str\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of all diagnostic and result figures.\n",
        "\n",
        "    This function calls the dedicated plotting function for each of the four\n",
        "    charts specified in the paper, saving each to a file.\n",
        "\n",
        "    Args:\n",
        "        synthetic_corpus_path: Path to the synthetic articles CSV.\n",
        "        synthetic_embeddings: The (256, 1024) array of synthetic embeddings.\n",
        "        monthly_neos_df: DataFrame with monthly NEOS indicators.\n",
        "        monthly_indicator_df: DataFrame with monthly comparator indicators.\n",
        "        raw_macro_data_df: DataFrame with quarterly GDP.\n",
        "        daily_indicator_df: Optional DataFrame with daily MTD NEOS.\n",
        "        forecast_errors_df: DataFrame with all POOS forecast errors.\n",
        "        fused_master_input_specification: The master config.\n",
        "        output_directory: The directory where all chart images will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping chart names to their output file paths.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    chart_paths = {}\n",
        "\n",
        "    # --- Generate Chart 2 (UMAP) ---\n",
        "    # Note: This calls the remediated version of _create_umap_visualization\n",
        "    # which is assumed to be available in the execution context.\n",
        "    print(\"\\n--- Generating Chart 2: UMAP Visualization ---\")\n",
        "    chart2_path = os.path.join(output_directory, 'chart2_umap_synthetic_embeddings.png')\n",
        "    _create_umap_visualization(\n",
        "        synthetic_embeddings=synthetic_embeddings,\n",
        "        synthetic_df=pd.read_csv(synthetic_corpus_path),\n",
        "        umap_params=fused_master_input_specification['master_config']['umap_diagnostic_params'],\n",
        "        output_path=chart2_path\n",
        "    )\n",
        "    chart_paths['chart2'] = chart2_path\n",
        "\n",
        "    # --- Generate Chart 3 (Time Series Comparison) ---\n",
        "    print(\"\\n--- Generating Chart 3: Time Series Comparison ---\")\n",
        "    chart3_path = os.path.join(output_directory, 'chart3_timeseries_comparison.png')\n",
        "    _generate_chart3_timeseries_comparison(\n",
        "        monthly_neos_df, monthly_indicator_df, raw_macro_data_df, chart3_path\n",
        "    )\n",
        "    chart_paths['chart3'] = chart3_path\n",
        "\n",
        "    # --- Generate Chart 4 (Timeliness) ---\n",
        "    if daily_indicator_df is not None:\n",
        "        print(\"\\n--- Generating Chart 4: Timeliness Diagnostic ---\")\n",
        "        chart4_path = os.path.join(output_directory, 'chart4_timeliness_diagnostic.png')\n",
        "        _generate_chart4_timeliness_diagnostic(daily_indicator_df, chart4_path)\n",
        "        chart_paths['chart4'] = chart4_path\n",
        "    else:\n",
        "        print(\"\\nSkipping Chart 4: Daily MTD indicator data not provided.\")\n",
        "\n",
        "    # --- Generate Chart 5 (Crisis Performance) ---\n",
        "    print(\"\\n--- Generating Chart 5: Crisis Performance ---\")\n",
        "    chart5_path = os.path.join(output_directory, 'chart5_crisis_performance.png')\n",
        "    _generate_chart5_crisis_performance(\n",
        "        forecast_errors_df, raw_macro_data_df, chart5_path\n",
        "    )\n",
        "    chart_paths['chart5'] = chart5_path\n",
        "\n",
        "    return chart_paths\n"
      ],
      "metadata": {
        "id": "NGf8SfXv5prB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 — Build the end-to-end orchestrator function for the full NEOS pipeline\n",
        "\n",
        "# ===============================================================================\n",
        "# Task 22: Build the end-to-end orchestrator function for the full NEOS pipeline\n",
        "# ===============================================================================\n",
        "\n",
        "def _setup_artifact_paths(output_directory: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Creates a structured dictionary of all output file paths for the pipeline.\n",
        "\n",
        "    Purpose:\n",
        "    This helper function centralizes the management of all file paths for\n",
        "    artifacts generated by the pipeline. It creates a nested directory structure\n",
        "    (e.g., for data, models, results) within the main output directory. This\n",
        "    approach avoids hardcoding paths throughout the orchestrator, making the\n",
        "    pipeline more maintainable, configurable, and robust.\n",
        "\n",
        "    Inputs:\n",
        "        output_directory (str): The root directory where all artifacts will be saved.\n",
        "\n",
        "    Processes:\n",
        "    1.  Defines a dictionary mapping logical artifact names (e.g., \"relevance_model\")\n",
        "        to their full, absolute file paths within a standardized subdirectory\n",
        "        structure (`models/`, `data/`, `results/`).\n",
        "    2.  Iterates through the defined paths and uses `os.makedirs` to create the\n",
        "        necessary subdirectories, ensuring they exist before any files are written.\n",
        "\n",
        "    Outputs:\n",
        "        (Dict[str, Any]): A dictionary where keys are artifact names and values\n",
        "                          are their corresponding file paths.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the output directory is a valid string.\n",
        "    if not isinstance(output_directory, str) or not output_directory:\n",
        "        raise ValueError(\"`output_directory` must be a non-empty string.\")\n",
        "\n",
        "    # --- Define Canonical Paths for All Artifacts ---\n",
        "    # This dictionary serves as the single source of truth for file locations.\n",
        "    paths: Dict[str, str] = {\n",
        "        \"relevance_model\": os.path.join(output_directory, 'models', 'relevance_classifier.keras'),\n",
        "        \"sentiment_model\": os.path.join(output_directory, 'models', 'sentiment_classifier.joblib'),\n",
        "        \"embeddings_h5\": os.path.join(output_directory, 'data', 'document_embeddings.h5'),\n",
        "        \"embeddings_crosswalk\": os.path.join(output_directory, 'data', 'embeddings_crosswalk.csv'),\n",
        "        \"synthetic_corpus\": os.path.join(output_directory, 'data', 'synthetic_corpus.csv'),\n",
        "        \"relevance_scores\": os.path.join(output_directory, 'data', 'relevance_scores.feather'),\n",
        "        \"scored_articles\": os.path.join(output_directory, 'data', 'scored_relevant_articles.feather'),\n",
        "        \"final_evaluation_table\": os.path.join(output_directory, 'results', 'final_evaluation_results.csv'),\n",
        "        \"correlation_yoy\": os.path.join(output_directory, 'results', 'correlation_yoy.csv'),\n",
        "        \"correlation_qoq\": os.path.join(output_directory, 'results', 'correlation_qoq.csv'),\n",
        "        \"charts_dir\": os.path.join(output_directory, 'results', 'charts'),\n",
        "        \"reproducibility_manifest\": os.path.join(output_directory, 'reproducibility_manifest.json')\n",
        "    }\n",
        "\n",
        "    # --- Create Directory Structure ---\n",
        "    # Iterate through the path values and ensure the parent directory for each file exists.\n",
        "    for path in paths.values():\n",
        "        # Extract the directory part of the path.\n",
        "        dir_name = os.path.dirname(path)\n",
        "        # Create the directory if it doesn't already exist. `exist_ok=True` prevents errors on re-runs.\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "    # Return the completed dictionary of paths.\n",
        "    return paths\n",
        "\n",
        "\n",
        "def _create_reproducibility_manifest(config: Dict[str, Any], paths: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Generates a JSON file documenting the run's configuration, artifacts, and environment.\n",
        "\n",
        "    Purpose:\n",
        "    This function creates a critical final artifact: a self-contained manifest\n",
        "    that captures all the necessary information to understand and reproduce a\n",
        "    pipeline run. It records a snapshot of the configuration, the versions of all\n",
        "    key libraries, and the locations of all generated data and result files.\n",
        "\n",
        "    Inputs:\n",
        "        config (Dict[str, Any]): The `fused_master_input_specification` dictionary\n",
        "                                 used for the run.\n",
        "        paths (Dict[str, Any]): The dictionary of artifact paths generated by\n",
        "                                `_setup_artifact_paths`.\n",
        "\n",
        "    Processes:\n",
        "    1.  Defines a list of key scientific and ML libraries.\n",
        "    2.  Iterates through the list, dynamically importing each library and\n",
        "        retrieving its `__version__` attribute. It handles cases where a library\n",
        "        might not be installed.\n",
        "    3.  Constructs a final manifest dictionary containing the configuration\n",
        "        snapshot, the library versions, and the artifact paths.\n",
        "    4.  Serializes this dictionary to a formatted JSON file.\n",
        "    \"\"\"\n",
        "    # --- 1. Collect Library Versions ---\n",
        "    # This provides a snapshot of the software environment for reproducibility.\n",
        "    versions: Dict[str, str] = {}\n",
        "    # Define the list of critical libraries to version track.\n",
        "    libs_to_track = [\n",
        "        'pandas', 'numpy', 'tensorflow', 'sklearn', 'statsmodels',\n",
        "        'h5py', 'joblib', 'anthropic', 'umap', 'matplotlib', 'seaborn'\n",
        "    ]\n",
        "    # Iterate and safely retrieve the version for each library.\n",
        "    for lib in libs_to_track:\n",
        "        try:\n",
        "            # Dynamically import the library.\n",
        "            module = __import__(lib)\n",
        "            # Get its version attribute.\n",
        "            versions[lib] = module.__version__\n",
        "        except (ImportError, AttributeError):\n",
        "            # If the library is not found or has no version, record it as \"unknown\".\n",
        "            versions[lib] = \"unknown\"\n",
        "\n",
        "    # --- 2. Assemble the Manifest ---\n",
        "    # The manifest is a dictionary containing all key provenance information.\n",
        "    manifest = {\n",
        "        \"config_snapshot\": config,\n",
        "        \"output_artifacts\": paths,\n",
        "        \"library_versions\": versions\n",
        "    }\n",
        "\n",
        "    # --- 3. Serialize to JSON ---\n",
        "    # Write the manifest dictionary to a human-readable, indented JSON file.\n",
        "    try:\n",
        "        with open(paths['reproducibility_manifest'], 'w') as f:\n",
        "            json.dump(manifest, f, indent=4)\n",
        "    except IOError as e:\n",
        "        # Handle potential file writing errors.\n",
        "        logging.error(f\"Failed to write reproducibility manifest: {e}\")\n",
        "\n",
        "\n",
        "def run_neos_pipeline(\n",
        "    # --- Raw Data Inputs ---\n",
        "    raw_news_data_df: pd.DataFrame,\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    monthly_indicator_data_df: pd.DataFrame,\n",
        "    release_calendar_df: pd.DataFrame,\n",
        "    evaluation_windows_df: pd.DataFrame,\n",
        "    # --- Configuration and Paths ---\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    output_directory: str,\n",
        "    lexicon_path: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end execution of the full NEOS pipeline.\n",
        "\n",
        "    Purpose:\n",
        "    This function is the master controller for the entire project. It sequences\n",
        "    all tasks from initial data validation to final figure generation. It manages\n",
        "    the flow of data artifacts between tasks and implements checkpointing to allow\n",
        "    for resumability, saving significant computation time on re-runs. It produces\n",
        "    a comprehensive set of output artifacts and a reproducibility manifest.\n",
        "\n",
        "    Inputs:\n",
        "        raw_news_data_df (pd.DataFrame): The raw, unprocessed news corpus.\n",
        "        raw_macro_data_df (pd.DataFrame): Raw quarterly macro data (GDP, etc.).\n",
        "        monthly_indicator_data_df (pd.DataFrame): Raw monthly comparator indicators.\n",
        "        release_calendar_df (pd.DataFrame): Metadata on indicator release timing.\n",
        "        evaluation_windows_df (pd.DataFrame): Metadata on valid evaluation periods.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        output_directory (str): A root directory to save all generated artifacts.\n",
        "        lexicon_path (str): Path to the translated German sentiment lexicon CSV.\n",
        "\n",
        "    Outputs:\n",
        "        (Dict[str, Any]): A dictionary containing a 'status' ('Success' or 'Failure'),\n",
        "                          an 'error' message if applicable, and a dictionary of\n",
        "                          'artifacts' mapping names to their file paths.\n",
        "    \"\"\"\n",
        "    # --- Phase 0: Setup Logging and Path Management ---\n",
        "    # Create the output directory if it doesn't exist.\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    # Configure a logger to write to both a file and the console.\n",
        "    log_file = os.path.join(output_directory, 'pipeline_run.log')\n",
        "    # Remove existing handlers to avoid duplication in interactive environments (e.g., notebooks).\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "    # Set up the new configuration.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s [%(levelname)s] - %(message)s',\n",
        "        handlers=[logging.FileHandler(log_file, mode='w'), logging.StreamHandler(sys.stdout)]\n",
        "    )\n",
        "\n",
        "    logging.info(\"Initializing NEOS pipeline orchestration.\")\n",
        "    # Generate and create the directory structure for all pipeline artifacts.\n",
        "    artifact_paths = _setup_artifact_paths(output_directory)\n",
        "\n",
        "    try:\n",
        "        # --- Phase 1: Pre-flight Validation (Tasks 1, 2, 3) ---\n",
        "        logging.info(\"--- [Phase 1/8] Running Pre-flight Validations ---\")\n",
        "        # Validate all configuration and data inputs before starting computation.\n",
        "        config_val = validate_master_config(fused_master_input_specification)\n",
        "        news_val = validate_raw_news_corpus(raw_news_data_df, fused_master_input_specification)\n",
        "        macro_val = validate_macro_data(raw_macro_data_df, monthly_indicator_data_df, release_calendar_df, evaluation_windows_df)\n",
        "\n",
        "        # Aggregate all validation issues.\n",
        "        all_issues = config_val.get('issues', []) + news_val.get('errors', []) + news_val.get('warnings', []) + macro_val.get('issues', [])\n",
        "        # Check if any issue is a critical error and halt execution if so.\n",
        "        if any(\"Error\" in str(issue) for issue in all_issues):\n",
        "            logging.error(\"Pre-flight validation failed. Halting execution.\")\n",
        "            for issue in all_issues: logging.error(f\"  - {issue}\")\n",
        "            raise ValueError(\"Input data or configuration failed validation.\")\n",
        "        logging.info(\"All inputs validated successfully.\")\n",
        "\n",
        "        # --- Phase 2: Data Cleansing and Preparation (Tasks 4, 5) ---\n",
        "        logging.info(\"--- [Phase 2/8] Cleansing and Preparing Corpus ---\")\n",
        "        df_clean, audit_log = cleanse_news_corpus(raw_news_data_df, fused_master_input_specification)\n",
        "        logging.info(f\"Cleansing complete. Final count: {audit_log.get('final_article_count', 'N/A')}. Full audit log generated.\")\n",
        "        df_prepared, _ = prepare_corpus_for_embedding(df_clean, raw_news_data_df)\n",
        "        logging.info(\"Temporal features added to corpus.\")\n",
        "\n",
        "        # --- Phase 3: Feature Engineering: Embeddings (Task 6) ---\n",
        "        logging.info(\"--- [Phase 3/8] Generating Document Embeddings ---\")\n",
        "        # Checkpointing: Skip this expensive step if outputs already exist.\n",
        "        if not os.path.exists(artifact_paths['embeddings_h5']) or not os.path.exists(artifact_paths['embeddings_crosswalk']):\n",
        "            df_prepared, _, _, _ = generate_document_embeddings(\n",
        "                df_prepared, fused_master_input_specification, os.path.dirname(artifact_paths['embeddings_h5'])\n",
        "            )\n",
        "            logging.info(\"Embedding generation complete.\")\n",
        "        else:\n",
        "            logging.info(\"Embeddings and crosswalk files already exist. Skipping generation.\")\n",
        "\n",
        "        # --- Phase 4: Relevance Model (Tasks 7, 8, 9) ---\n",
        "        logging.info(\"--- [Phase 4/8] Training and Applying Relevance Model ---\")\n",
        "        # Checkpointing: Skip model training if the model file already exists.\n",
        "        if not os.path.exists(artifact_paths['relevance_model']):\n",
        "            X_train, y_train, X_val, y_val = prepare_relevance_training_data(\n",
        "                df_prepared, artifact_paths['embeddings_crosswalk'], artifact_paths['embeddings_h5'], fused_master_input_specification\n",
        "            )\n",
        "            _, train_results = train_relevance_classifier(\n",
        "                X_train, y_train, X_val, y_val, fused_master_input_specification, artifact_paths['relevance_model']\n",
        "            )\n",
        "            logging.info(f\"Relevance model training complete. Validation AUC: {train_results['validation_metrics'].get('auc', 'N/A'):.4f}\")\n",
        "        else:\n",
        "            logging.info(\"Relevance model already exists. Skipping training.\")\n",
        "\n",
        "        # Apply the classifier to filter the corpus. This step always runs.\n",
        "        df_relevant, _, relevance_audit = filter_corpus_by_relevance(\n",
        "            df_prepared, artifact_paths['relevance_model'], artifact_paths['embeddings_h5'],\n",
        "            artifact_paths['embeddings_crosswalk'], fused_master_input_specification, os.path.dirname(artifact_paths['relevance_scores'])\n",
        "        )\n",
        "        logging.info(f\"Corpus filtered for relevance. {relevance_audit.get('total_relevant_articles', 'N/A')} articles remain.\")\n",
        "\n",
        "        # --- Phase 5: Sentiment Model (Tasks 10, 11, 12) ---\n",
        "        logging.info(\"--- [Phase 5/8] Training Sentiment Model ---\")\n",
        "        # Checkpointing: Skip synthetic data generation if it exists.\n",
        "        if not os.path.exists(artifact_paths['synthetic_corpus']):\n",
        "            generate_synthetic_articles(fused_master_input_specification, artifact_paths['synthetic_corpus'])\n",
        "        else:\n",
        "            logging.info(\"Synthetic corpus already exists. Skipping generation.\")\n",
        "\n",
        "        # Embed the synthetic data. This is fast and always runs.\n",
        "        synthetic_embeddings, synthetic_labels, _ = process_synthetic_embeddings(\n",
        "            artifact_paths['synthetic_corpus'], fused_master_input_specification, artifact_paths['charts_dir']\n",
        "        )\n",
        "\n",
        "        # Checkpointing: Skip sentiment model training if it exists.\n",
        "        if not os.path.exists(artifact_paths['sentiment_model']):\n",
        "            _, sent_train_results = train_sentiment_classifier(\n",
        "                synthetic_embeddings, synthetic_labels, fused_master_input_specification, artifact_paths['sentiment_model']\n",
        "            )\n",
        "            logging.info(f\"Sentiment model training complete. Optimal C: {sent_train_results.get('optimal_c', 'N/A')}\")\n",
        "        else:\n",
        "            logging.info(\"Sentiment model already exists. Skipping training.\")\n",
        "\n",
        "        # --- Phase 6: Indicator Construction (Tasks 13, 14) ---\n",
        "        logging.info(\"--- [Phase 6/8] Scoring Articles and Constructing Indicators ---\")\n",
        "        score_relevant_articles(\n",
        "            df_relevant, artifact_paths['sentiment_model'], artifact_paths['embeddings_h5'],\n",
        "            artifact_paths['embeddings_crosswalk'], artifact_paths['scored_articles']\n",
        "        )\n",
        "        monthly_neos_df, daily_indicator_df = construct_neos_indicators(\n",
        "            artifact_paths['scored_articles'], fused_master_input_specification\n",
        "        )\n",
        "        logging.info(\"Monthly and daily NEOS indicators constructed.\")\n",
        "\n",
        "        # --- Phase 7: Econometric Evaluation (Tasks 15-20) ---\n",
        "        logging.info(\"--- [Phase 7/8] Performing Econometric Evaluation ---\")\n",
        "        lexicon_baseline_df = construct_lexicon_baseline_indicator(df_relevant, lexicon_path)\n",
        "        forecasting_df = prepare_forecasting_dataset(\n",
        "            monthly_neos_df, monthly_indicator_data_df.join(lexicon_baseline_df, how='outer'), raw_macro_data_df, fused_master_input_specification\n",
        "        )\n",
        "        forecast_errors_df = execute_poos_forecasts(forecasting_df, fused_master_input_specification, evaluation_windows_df)\n",
        "        rmse_ratios_df = compute_rmse_ratios(forecast_errors_df)\n",
        "        final_eval_table = perform_diebold_mariano_tests(forecast_errors_df, rmse_ratios_df)\n",
        "        final_eval_table.to_csv(artifact_paths['final_evaluation_table'])\n",
        "        logging.info(f\"Final evaluation table saved to {artifact_paths['final_evaluation_table']}\")\n",
        "\n",
        "        yoy_corr, qoq_corr = compute_correlation_benchmarks(\n",
        "            monthly_neos_df, monthly_indicator_data_df, lexicon_baseline_df, raw_macro_data_df\n",
        "        )\n",
        "        yoy_corr.to_csv(artifact_paths['correlation_yoy'])\n",
        "        qoq_corr.to_csv(artifact_paths['correlation_qoq'])\n",
        "        logging.info(\"Correlation tables generated.\")\n",
        "\n",
        "        # --- Phase 8: Final Diagnostics and Figures (Task 21) ---\n",
        "        logging.info(\"--- [Phase 8/8] Generating Final Figures ---\")\n",
        "        generate_all_charts(\n",
        "            synthetic_corpus_path=artifact_paths['synthetic_corpus'], synthetic_embeddings=synthetic_embeddings,\n",
        "            monthly_neos_df=monthly_neos_df, monthly_indicator_df=monthly_indicator_data_df,\n",
        "            raw_macro_data_df=raw_macro_data_df, daily_indicator_df=daily_indicator_df,\n",
        "            forecast_errors_df=forecast_errors_df, fused_master_input_specification=fused_master_input_specification,\n",
        "            output_directory=artifact_paths['charts_dir']\n",
        "        )\n",
        "        logging.info(f\"All charts saved to {artifact_paths['charts_dir']}\")\n",
        "\n",
        "        # --- Final Step: Create Reproducibility Manifest ---\n",
        "        _create_reproducibility_manifest(fused_master_input_specification, artifact_paths)\n",
        "        logging.info(f\"Reproducibility manifest saved to {artifact_paths['reproducibility_manifest']}\")\n",
        "\n",
        "        logging.info(\"NEOS pipeline completed successfully.\")\n",
        "        # Return a dictionary indicating success and providing the artifact locations.\n",
        "        return {\"status\": \"Success\", \"artifacts\": artifact_paths}\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception that occurs during the pipeline execution.\n",
        "        logging.error(f\"Pipeline failed with a critical error: {e}\", exc_info=True)\n",
        "        # Return a dictionary indicating failure and the error message.\n",
        "        return {\"status\": \"Failure\", \"error\": str(e), \"artifacts\": artifact_paths}\n"
      ],
      "metadata": {
        "id": "syZWp6h5Cxtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23: Conduct robustness analyses using the orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Conduct robustness analyses using the orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Helper for Aggregating Results\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _aggregate_sensitivity_results(\n",
        "    base_output_dir: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates final evaluation results from multiple sensitivity run directories.\n",
        "\n",
        "    Purpose:\n",
        "    After the main orchestrator has been run multiple times with different\n",
        "    parameters, this function systematically scans the output directories,\n",
        "    reads the `final_evaluation_results.csv` from each, and combines them into a\n",
        "    single master DataFrame. This allows for easy comparison of the key\n",
        "    performance metrics (RMSE ratios, p-values) across all experimental runs.\n",
        "\n",
        "    Inputs:\n",
        "        base_output_dir (str): The root directory containing all the individual\n",
        "                               sensitivity run subdirectories.\n",
        "\n",
        "    Outputs:\n",
        "        (pd.DataFrame): A single DataFrame containing the aggregated results from\n",
        "                        all sensitivity analyses, with additional columns\n",
        "                        identifying the parameters of each run.\n",
        "    \"\"\"\n",
        "    # This list will hold the DataFrames from each sensitivity run.\n",
        "    all_results: List[pd.DataFrame] = []\n",
        "\n",
        "    # Walk through the subdirectories of the base output directory.\n",
        "    for root, dirs, files in os.walk(base_output_dir):\n",
        "        # Define the expected results file.\n",
        "        results_filename = 'final_evaluation_results.csv'\n",
        "        # Check if the results file exists in the current directory.\n",
        "        if results_filename in files:\n",
        "            try:\n",
        "                # Construct the full path to the results file.\n",
        "                results_path = os.path.join(root, results_filename)\n",
        "                # Read the evaluation table from the CSV.\n",
        "                results_df = pd.read_csv(results_path)\n",
        "                # Extract the unique run name from the directory path.\n",
        "                run_name = os.path.basename(root)\n",
        "                # Add a column to identify which sensitivity run these results belong to.\n",
        "                results_df['sensitivity_run'] = run_name\n",
        "                all_results.append(results_df)\n",
        "            except Exception as e:\n",
        "                # Log a warning if a results file is found but cannot be processed.\n",
        "                logging.warning(f\"Could not read or process results from {root}: {e}\")\n",
        "\n",
        "    # Return an empty DataFrame if no results were found.\n",
        "    if not all_results:\n",
        "        logging.warning(\"No sensitivity results found to aggregate.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate all the collected DataFrames into a single master table.\n",
        "    aggregated_df = pd.concat(all_results, ignore_index=True)\n",
        "    # Set a multi-index for easier slicing and analysis of the results.\n",
        "    if 'Indicator' in aggregated_df.columns:\n",
        "        aggregated_df.set_index(['sensitivity_run', 'Indicator'], inplace=True)\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Helper for a Single Parameter-based Run\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_single_parameter_sensitivity(\n",
        "    param_path: Tuple[str, ...],\n",
        "    param_value: Any,\n",
        "    run_name: str,\n",
        "    base_output_dir: str,\n",
        "    base_config: Dict[str, Any],\n",
        "    **kwargs: Any\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Executes a single, isolated run of the NEOS pipeline with one modified parameter.\n",
        "\n",
        "    Purpose:\n",
        "    This helper function is the core engine for the parameter sensitivity analysis.\n",
        "    It provides a robust and reproducible way to test the impact of changing a\n",
        "    single hyperparameter. It ensures complete isolation between runs by creating\n",
        "    a unique output directory and working on a deep copy of the master\n",
        "    configuration, thus preventing any side effects or data contamination.\n",
        "\n",
        "    Inputs:\n",
        "        param_path (Tuple[str, ...]): A tuple of strings representing the nested\n",
        "            keys to access the parameter to be modified within the master config.\n",
        "            For example: ('master_config', 'relevance_model_params', 'classification_threshold').\n",
        "        param_value (Any): The new value to assign to the specified parameter.\n",
        "        run_name (str): A unique name for this sensitivity run, used to create\n",
        "                        the output subdirectory (e.g., \"sensitivity_tau_0.4\").\n",
        "        base_output_dir (str): The root directory where the subdirectory for this\n",
        "                               run will be created.\n",
        "        base_config (Dict[str, Any]): The baseline `fused_master_input_specification`\n",
        "                                      dictionary, which will be deep-copied.\n",
        "        **kwargs (Any): All other keyword arguments required by the main\n",
        "                        `run_neos_pipeline` function (e.g., the raw DataFrames\n",
        "                        and lexicon path).\n",
        "\n",
        "    Processes:\n",
        "    1.  Logs the start of the specific sensitivity run.\n",
        "    2.  Creates a unique, isolated output directory for this run's artifacts.\n",
        "    3.  Performs a deep copy of the base configuration to ensure the original\n",
        "        config remains unmodified.\n",
        "    4.  Programmatically navigates the nested configuration dictionary using the\n",
        "        `param_path` and updates the target parameter with `param_value`.\n",
        "    5.  Calls the main `run_neos_pipeline` orchestrator, passing the modified\n",
        "        configuration, the unique output directory, and all other necessary data.\n",
        "\n",
        "    Outputs:\n",
        "        None: This function does not return a value. Its primary effect is the\n",
        "              execution of the full pipeline, which generates a complete set of\n",
        "              artifacts within the specified `run_output_dir`.\n",
        "\n",
        "    Error Handling:\n",
        "        - Raises KeyError if the `param_path` is invalid for the given `base_config`.\n",
        "        - Propagates any exceptions raised by the underlying `run_neos_pipeline`.\n",
        "    \"\"\"\n",
        "    # --- Log the start of this specific experimental run ---\n",
        "    logging.info(f\"\\n--- Running Sensitivity Analysis For: {run_name} ---\")\n",
        "\n",
        "    # --- 1. Create an Isolated Environment for the Run ---\n",
        "    # Define a unique output directory for this run to prevent artifact collision.\n",
        "    run_output_dir = os.path.join(base_output_dir, run_name)\n",
        "    # A deep copy is essential to prevent modifications in this run from affecting subsequent runs.\n",
        "    run_config = copy.deepcopy(base_config)\n",
        "\n",
        "    # --- 2. Modify the Configuration Parameter ---\n",
        "    # Navigate through the nested dictionary structure to reach the target parameter.\n",
        "    try:\n",
        "        # Start at the top level of the configuration dictionary.\n",
        "        config_level = run_config\n",
        "        # Traverse the path until the second-to-last key.\n",
        "        for key in param_path[:-1]:\n",
        "            config_level = config_level[key]\n",
        "        # Use the final key to set the new parameter value.\n",
        "        config_level[param_path[-1]] = param_value\n",
        "        logging.info(f\"Modified parameter '{'.'.join(param_path)}' to '{param_value}'.\")\n",
        "    except KeyError as e:\n",
        "        # If the path is invalid, log a critical error and raise it.\n",
        "        logging.error(f\"Invalid parameter path provided: {param_path}. Key not found: {e}\")\n",
        "        raise e\n",
        "\n",
        "    # --- 3. Execute the Full Pipeline with the Modified Config ---\n",
        "    # Call the main orchestrator with the modified config and unique output directory.\n",
        "    # All other data inputs are passed through via **kwargs.\n",
        "    run_neos_pipeline(\n",
        "        fused_master_input_specification=run_config,\n",
        "        output_directory=run_output_dir,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator for Robustness Analyses\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_robustness_analyses(\n",
        "    # --- Base Data Inputs (same as main pipeline) ---\n",
        "    raw_news_data_df: pd.DataFrame,\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    monthly_indicator_data_df: pd.DataFrame,\n",
        "    release_calendar_df: pd.DataFrame,\n",
        "    evaluation_windows_df: pd.DataFrame,\n",
        "    # --- Base Configuration and Paths ---\n",
        "    base_config: Dict[str, Any],\n",
        "    base_output_dir: str,\n",
        "    lexicon_path: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates a series of sensitivity analyses by re-running the main pipeline.\n",
        "\n",
        "    Purpose:\n",
        "    This \"meta-orchestrator\" systematically tests the robustness of the main\n",
        "    findings to changes in key methodological choices. It does this by creating\n",
        "    modified versions of the configuration or input data and executing the\n",
        "    entire `run_neos_pipeline` for each variation. Finally, it aggregates the\n",
        "    results for comparison.\n",
        "\n",
        "    Args:\n",
        "        (various): All the standard inputs required by `run_neos_pipeline`.\n",
        "        base_config (Dict[str, Any]): The baseline `fused_master_input_specification`.\n",
        "        base_output_dir (str): A root directory where subdirectories for each\n",
        "                               sensitivity run will be created.\n",
        "\n",
        "    Returns:\n",
        "        (pd.DataFrame): A summary DataFrame comparing the final evaluation\n",
        "                        metrics across all sensitivity runs.\n",
        "    \"\"\"\n",
        "    logging.info(\"====== STARTING ROBUSTNESS ANALYSIS PIPELINE ======\")\n",
        "\n",
        "    # Package the base data inputs for repeated calls to the main pipeline.\n",
        "    base_kwargs = {\n",
        "        \"raw_news_data_df\": raw_news_data_df,\n",
        "        \"raw_macro_data_df\": raw_macro_data_df,\n",
        "        \"monthly_indicator_data_df\": monthly_indicator_data_df,\n",
        "        \"release_calendar_df\": release_calendar_df,\n",
        "        \"evaluation_windows_df\": evaluation_windows_df,\n",
        "        \"lexicon_path\": lexicon_path\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Parameter Sensitivity ---\n",
        "    logging.info(\"\\n--- [Robustness 1/3] Testing Parameter Sensitivity ---\")\n",
        "\n",
        "    # 1a: Varying the relevance classification threshold (tau)\n",
        "    for threshold in [0.4, 0.5, 0.6]:\n",
        "        _run_single_parameter_sensitivity(\n",
        "            param_path=('master_config', 'relevance_model_params', 'classification_threshold'),\n",
        "            param_value=threshold,\n",
        "            run_name=f\"sensitivity_tau_{threshold}\",\n",
        "            base_output_dir=base_output_dir,\n",
        "            base_config=base_config,\n",
        "            **base_kwargs\n",
        "        )\n",
        "\n",
        "    # 1b: Varying the POOS initial window size (covers Step 3)\n",
        "    for window_size in [6, 8, 10, 12]:\n",
        "        _run_single_parameter_sensitivity(\n",
        "            param_path=('master_config', 'econometric_validation_params', 'poos_initial_window_quarters'),\n",
        "            param_value=window_size,\n",
        "            run_name=f\"sensitivity_poos_window_{window_size}\",\n",
        "            base_output_dir=base_output_dir,\n",
        "            base_config=base_config,\n",
        "            **base_kwargs\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Data-Scope Sensitivity ---\n",
        "    logging.info(\"\\n--- [Robustness 2/3] Testing Data-Scope Sensitivity ---\")\n",
        "\n",
        "    # 2a: German-only vs. French-only NEOS\n",
        "    for lang in ['de', 'fr']:\n",
        "        run_name = f\"sensitivity_lang_{lang}_only\"\n",
        "        logging.info(f\"\\n--- Running analysis for: {run_name} ---\")\n",
        "        run_output_dir = os.path.join(base_output_dir, run_name)\n",
        "        lang_only_df = raw_news_data_df[raw_news_data_df['language'] == lang].copy()\n",
        "        run_kwargs = base_kwargs.copy()\n",
        "        run_kwargs['raw_news_data_df'] = lang_only_df\n",
        "        run_neos_pipeline(\n",
        "            fused_master_input_specification=base_config,\n",
        "            output_directory=run_output_dir,\n",
        "            **run_kwargs\n",
        "        )\n",
        "\n",
        "    # 2b: Truncation Sensitivity Analysis\n",
        "    logging.info(\"\\n--- Running analysis for: Truncation Sensitivity ---\")\n",
        "    run_name = \"sensitivity_truncation_filtered\"\n",
        "    run_output_dir = os.path.join(base_output_dir, run_name)\n",
        "    # Stage A: Generate metadata to identify high-truncation months.\n",
        "    logging.info(\"Truncation analysis: Stage A - Generating metadata...\")\n",
        "    temp_output_dir = os.path.join(base_output_dir, \"temp_truncation_meta\")\n",
        "    df_clean_temp, _ = cleanse_news_corpus(raw_news_data_df, base_config)\n",
        "    df_prepared_temp, _, _, _ = generate_document_embeddings(\n",
        "        df_clean_temp, base_config, temp_output_dir\n",
        "    )\n",
        "    # Stage B: Identify months with truncation rate > 10%.\n",
        "    logging.info(\"Truncation analysis: Stage B - Identifying high-truncation months...\")\n",
        "    monthly_truncation_rate = df_prepared_temp.groupby('year_month')['truncation_flag'].mean()\n",
        "    high_truncation_months = monthly_truncation_rate[monthly_truncation_rate > 0.10].index.tolist()\n",
        "    logging.info(f\"Found {len(high_truncation_months)} months with >10% truncation rate to exclude.\")\n",
        "\n",
        "    # Stage C: Filter the original raw data.\n",
        "    logging.info(\"Truncation analysis: Stage C - Filtering raw data...\")\n",
        "    raw_df_copy = raw_news_data_df.copy()\n",
        "    raw_df_copy['year_month'] = raw_df_copy['publication_datetime_utc'].dt.strftime('%Y-%m')\n",
        "    filtered_raw_df = raw_df_copy[~raw_df_copy['year_month'].isin(high_truncation_months)].drop(columns=['year_month'])\n",
        "\n",
        "    # Stage D: Execute the full pipeline on the filtered data.\n",
        "    logging.info(\"Truncation analysis: Stage D - Executing full pipeline on filtered data...\")\n",
        "    run_kwargs = base_kwargs.copy()\n",
        "    run_kwargs['raw_news_data_df'] = filtered_raw_df\n",
        "    run_neos_pipeline(\n",
        "        fused_master_input_specification=base_config,\n",
        "        output_directory=run_output_dir,\n",
        "        **run_kwargs\n",
        "    )\n",
        "\n",
        "    # --- Final Step: Aggregate All Results ---\n",
        "    logging.info(\"\\n--- Aggregating all sensitivity analysis results ---\")\n",
        "    summary_df = _aggregate_sensitivity_results(base_output_dir)\n",
        "\n",
        "    # Save the final summary table.\n",
        "    summary_path = os.path.join(base_output_dir, 'robustness_summary_results.csv')\n",
        "    summary_df.to_csv(summary_path)\n",
        "\n",
        "    logging.info(f\"====== ROBUSTNESS ANALYSIS PIPELINE COMPLETE ======\")\n",
        "    logging.info(f\"Final summary of all runs saved to: {summary_path}\")\n",
        "\n",
        "    return summary_df\n"
      ],
      "metadata": {
        "id": "gur0xyqyZVas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "def run_complete_neos_study(\n",
        "    # --- Raw Data Inputs ---\n",
        "    raw_news_data_df: pd.DataFrame,\n",
        "    raw_macro_data_df: pd.DataFrame,\n",
        "    monthly_indicator_data_df: pd.DataFrame,\n",
        "    release_calendar_df: pd.DataFrame,\n",
        "    evaluation_windows_df: pd.DataFrame,\n",
        "    # --- Configuration and Paths ---\n",
        "    fused_master_input_specification: Dict[str, Any],\n",
        "    root_output_directory: str,\n",
        "    lexicon_path: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete NEOS study, including the baseline analysis and all robustness checks.\n",
        "\n",
        "    Purpose:\n",
        "    This top-level orchestrator serves as the single entry point for the entire\n",
        "    research project. It first runs the main NEOS pipeline with the baseline\n",
        "    configuration to generate the primary results. It then systematically\n",
        "    launches a series of subsequent pipeline runs to perform the sensitivity\n",
        "    and robustness analyses specified in Task 23.\n",
        "\n",
        "    Inputs:\n",
        "        raw_news_data_df (pd.DataFrame): The raw, unprocessed news corpus.\n",
        "        raw_macro_data_df (pd.DataFrame): Raw quarterly macro data (GDP, etc.).\n",
        "        monthly_indicator_data_df (pd.DataFrame): Raw monthly comparator indicators.\n",
        "        release_calendar_df (pd.DataFrame): Metadata on indicator release timing.\n",
        "        evaluation_windows_df (pd.DataFrame): Metadata on valid evaluation periods.\n",
        "        fused_master_input_specification (Dict[str, Any]): The master config.\n",
        "        root_output_directory (str): A root directory where all outputs, including\n",
        "                                     subdirectories for baseline and sensitivity\n",
        "                                     runs, will be saved.\n",
        "        lexicon_path (str): Path to the translated German sentiment lexicon CSV.\n",
        "\n",
        "    Processes:\n",
        "    1.  Sets up a clear directory structure for the baseline and robustness runs.\n",
        "    2.  Calls `run_neos_pipeline` to execute the main analysis based on the\n",
        "        provided configuration, saving results to a 'baseline' subdirectory.\n",
        "    3.  Calls `run_robustness_analyses` which then orchestrates multiple re-runs\n",
        "        of the main pipeline with modified configurations or data, saving results\n",
        "        to a 'robustness_checks' subdirectory.\n",
        "    4.  Collects the primary outputs from both stages.\n",
        "\n",
        "    Outputs:\n",
        "        (Dict[str, Any]): A dictionary containing the results of the orchestration:\n",
        "                          - 'baseline_run_results': The results dictionary from the\n",
        "                            main pipeline run.\n",
        "                          - 'robustness_summary_df': The aggregated DataFrame\n",
        "                            comparing all robustness checks.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup Directory Structure ---\n",
        "    # Define distinct subdirectories for the baseline run and the sensitivity analyses.\n",
        "    baseline_output_dir = os.path.join(root_output_directory, 'baseline_run')\n",
        "    robustness_output_dir = os.path.join(root_output_directory, 'robustness_checks')\n",
        "\n",
        "    # Create the directories.\n",
        "    os.makedirs(baseline_output_dir, exist_ok=True)\n",
        "    os.makedirs(robustness_output_dir, exist_ok=True)\n",
        "\n",
        "    # --- 2. Execute the Baseline Pipeline Run ---\n",
        "    logging.info(\"=\"*80)\n",
        "    logging.info(\">>> STARTING BASELINE NEOS PIPELINE RUN <<<\")\n",
        "    logging.info(\"=\"*80)\n",
        "\n",
        "    # Execute the main end-to-end pipeline with the provided configuration.\n",
        "    baseline_run_results = run_neos_pipeline(\n",
        "        raw_news_data_df=raw_news_data_df,\n",
        "        raw_macro_data_df=raw_macro_data_df,\n",
        "        monthly_indicator_data_df=monthly_indicator_data_df,\n",
        "        release_calendar_df=release_calendar_df,\n",
        "        evaluation_windows_df=evaluation_windows_df,\n",
        "        fused_master_input_specification=fused_master_input_specification,\n",
        "        output_directory=baseline_output_dir,\n",
        "        lexicon_path=lexicon_path\n",
        "    )\n",
        "\n",
        "    # Check for failure in the baseline run before proceeding.\n",
        "    if baseline_run_results.get('status') == 'Failure':\n",
        "        logging.error(\"Baseline pipeline run failed. Halting execution of robustness analyses.\")\n",
        "        return {\n",
        "            \"baseline_run_results\": baseline_run_results,\n",
        "            \"robustness_summary_df\": pd.DataFrame()\n",
        "        }\n",
        "\n",
        "    logging.info(\">>> BASELINE NEOS PIPELINE RUN COMPLETED SUCCESSFULLY <<<\")\n",
        "\n",
        "    # --- 3. Execute the Robustness Analyses ---\n",
        "    logging.info(\"=\"*80)\n",
        "    logging.info(\">>> STARTING ROBUSTNESS ANALYSIS RUNS <<<\")\n",
        "    logging.info(\"=\"*80)\n",
        "\n",
        "    # Execute the meta-orchestrator for all sensitivity checks.\n",
        "    # This function will internally call `run_neos_pipeline` multiple times.\n",
        "    robustness_summary_df = run_robustness_analyses(\n",
        "        raw_news_data_df=raw_news_data_df,\n",
        "        raw_macro_data_df=raw_macro_data_df,\n",
        "        monthly_indicator_data_df=monthly_indicator_data_df,\n",
        "        release_calendar_df=release_calendar_df,\n",
        "        evaluation_windows_df=evaluation_windows_df,\n",
        "        base_config=fused_master_input_specification,\n",
        "        base_output_dir=robustness_output_dir,\n",
        "        lexicon_path=lexicon_path\n",
        "    )\n",
        "\n",
        "    logging.info(\">>> ROBUSTNESS ANALYSIS RUNS COMPLETED SUCCESSFULLY <<<\")\n",
        "\n",
        "    # --- 4. Package and Return Final Outputs ---\n",
        "    # The final output is a dictionary containing the key results from both major stages.\n",
        "    final_results = {\n",
        "        \"baseline_run_results\": baseline_run_results,\n",
        "        \"robustness_summary_df\": robustness_summary_df\n",
        "    }\n",
        "\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "rmIiagxdc12e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}